{'target': {'mean': array([-0.47906604], dtype=float32), 'std': array([0.6003092], dtype=float32), 'min': array([-5.7370567], dtype=float32), 'max': array([4.8414645], dtype=float32)}, 'input': {'mean': array([-0.39840052], dtype=float32), 'std': array([0.27414447], dtype=float32), 'min': array([-0.5], dtype=float32), 'max': array([0.5], dtype=float32)}}
Dataset: 60000, 45000
initialize network with normal
initialize network with normal
===> Epoch[1](0/704): Loss_D: 0.5730 Loss_G: 0.3855 MSE 135810.7188
===> Epoch[1](30/704): Loss_D: 0.2937 Loss_G: 0.5634 MSE 82343.3125
===> Epoch[1](60/704): Loss_D: 0.2810 Loss_G: 0.6921 MSE 101362.3828
===> Epoch[1](90/704): Loss_D: 0.2340 Loss_G: 0.5545 MSE 92836.0156
===> Epoch[1](120/704): Loss_D: 0.2555 Loss_G: 0.4465 MSE 91613.0859
===> Epoch[1](150/704): Loss_D: 0.2607 Loss_G: 0.6301 MSE 94007.2578
===> Epoch[1](180/704): Loss_D: 0.2776 Loss_G: 0.5277 MSE 87738.0234
===> Epoch[1](210/704): Loss_D: 0.2271 Loss_G: 0.5464 MSE 89158.5469
===> Epoch[1](240/704): Loss_D: 0.2285 Loss_G: 0.5311 MSE 92025.2344
===> Epoch[1](270/704): Loss_D: 0.2772 Loss_G: 1.7074 MSE 96510.4219
===> Epoch[1](300/704): Loss_D: 0.1563 Loss_G: 1.1956 MSE 95522.8750
===> Epoch[1](330/704): Loss_D: 0.1998 Loss_G: 0.7207 MSE 77533.6406
===> Epoch[1](360/704): Loss_D: 0.2918 Loss_G: 0.6509 MSE 85367.0000
===> Epoch[1](390/704): Loss_D: 0.2255 Loss_G: 0.9672 MSE 88119.9453
===> Epoch[1](420/704): Loss_D: 0.1930 Loss_G: 1.5337 MSE 88026.8672
===> Epoch[1](450/704): Loss_D: 0.0944 Loss_G: 0.8926 MSE 93142.8906
===> Epoch[1](480/704): Loss_D: 0.2093 Loss_G: 1.0728 MSE 89474.9375
===> Epoch[1](510/704): Loss_D: 0.1293 Loss_G: 1.0340 MSE 94172.5078
===> Epoch[1](540/704): Loss_D: 0.3119 Loss_G: 1.0677 MSE 91152.9531
===> Epoch[1](570/704): Loss_D: 0.1787 Loss_G: 1.0602 MSE 93131.6719
===> Epoch[1](600/704): Loss_D: 0.1513 Loss_G: 0.8596 MSE 89521.3750
===> Epoch[1](630/704): Loss_D: 0.1795 Loss_G: 0.9242 MSE 96744.8906
===> Epoch[1](660/704): Loss_D: 0.1873 Loss_G: 1.1639 MSE 92152.2891
===> Epoch[1](690/704): Loss_D: 0.2112 Loss_G: 0.8780 MSE 87610.5938
learning rate = 0.0010000
learning rate = 0.0010000
Saving
===> Epoch[2](0/704): Loss_D: 0.2703 Loss_G: 0.6684 MSE 88484.8516
===> Epoch[2](30/704): Loss_D: 0.1575 Loss_G: 0.8878 MSE 86096.0078
===> Epoch[2](60/704): Loss_D: 0.2603 Loss_G: 0.6456 MSE 88539.9062
===> Epoch[2](90/704): Loss_D: 0.2098 Loss_G: 0.6407 MSE 88066.0156
===> Epoch[2](120/704): Loss_D: 0.3996 Loss_G: 0.8561 MSE 89981.3672
===> Epoch[2](150/704): Loss_D: 0.1871 Loss_G: 1.0265 MSE 89675.0547
===> Epoch[2](180/704): Loss_D: 0.2537 Loss_G: 0.7270 MSE 94313.7969
===> Epoch[2](210/704): Loss_D: 0.2078 Loss_G: 0.7203 MSE 98369.1406
===> Epoch[2](240/704): Loss_D: 0.1988 Loss_G: 0.8514 MSE 97686.8438
===> Epoch[2](270/704): Loss_D: 0.2197 Loss_G: 0.8608 MSE 90089.7812
===> Epoch[2](300/704): Loss_D: 0.2736 Loss_G: 0.6879 MSE 91786.0625
===> Epoch[2](330/704): Loss_D: 0.1979 Loss_G: 0.6824 MSE 91126.3281
===> Epoch[2](360/704): Loss_D: 0.2487 Loss_G: 1.2474 MSE 93533.1250
===> Epoch[2](390/704): Loss_D: 0.1458 Loss_G: 1.5293 MSE 94029.0781
===> Epoch[2](420/704): Loss_D: 0.2022 Loss_G: 0.8049 MSE 94071.7500
===> Epoch[2](450/704): Loss_D: 0.1954 Loss_G: 0.8545 MSE 90667.1094
===> Epoch[2](480/704): Loss_D: 0.2511 Loss_G: 0.7526 MSE 99283.1094
===> Epoch[2](510/704): Loss_D: 0.1407 Loss_G: 0.9498 MSE 102101.7734
===> Epoch[2](540/704): Loss_D: 0.2322 Loss_G: 1.1705 MSE 99327.8047
===> Epoch[2](570/704): Loss_D: 0.2239 Loss_G: 0.9056 MSE 102560.5156
===> Epoch[2](600/704): Loss_D: 0.1721 Loss_G: 0.6153 MSE 94749.2812
===> Epoch[2](630/704): Loss_D: 0.2565 Loss_G: 0.4800 MSE 92125.6875
===> Epoch[2](660/704): Loss_D: 0.2703 Loss_G: 1.0971 MSE 99086.1328
===> Epoch[2](690/704): Loss_D: 0.1904 Loss_G: 0.8533 MSE 93524.3438
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[3](0/704): Loss_D: 0.1660 Loss_G: 0.6947 MSE 96839.0156
===> Epoch[3](30/704): Loss_D: 0.1981 Loss_G: 0.7293 MSE 89171.5156
===> Epoch[3](60/704): Loss_D: 0.1764 Loss_G: 0.8992 MSE 94826.3750
===> Epoch[3](90/704): Loss_D: 0.1430 Loss_G: 0.8302 MSE 96328.8281
===> Epoch[3](120/704): Loss_D: 0.2229 Loss_G: 0.7579 MSE 89768.7500
===> Epoch[3](150/704): Loss_D: 0.1896 Loss_G: 0.7356 MSE 91966.3984
===> Epoch[3](180/704): Loss_D: 0.1573 Loss_G: 0.7095 MSE 96799.3750
===> Epoch[3](210/704): Loss_D: 0.1940 Loss_G: 0.9263 MSE 90597.7969
===> Epoch[3](240/704): Loss_D: 0.2186 Loss_G: 0.7482 MSE 92588.3906
===> Epoch[3](270/704): Loss_D: 0.1844 Loss_G: 0.6312 MSE 93223.2734
===> Epoch[3](300/704): Loss_D: 0.1703 Loss_G: 0.9111 MSE 90239.4531
===> Epoch[3](330/704): Loss_D: 0.1467 Loss_G: 0.6481 MSE 91098.6094
===> Epoch[3](360/704): Loss_D: 0.2645 Loss_G: 0.6520 MSE 87983.3516
===> Epoch[3](390/704): Loss_D: 0.1760 Loss_G: 0.8869 MSE 92250.7188
===> Epoch[3](420/704): Loss_D: 0.1196 Loss_G: 0.7124 MSE 88709.7344
===> Epoch[3](450/704): Loss_D: 0.2525 Loss_G: 0.6767 MSE 89510.8594
===> Epoch[3](480/704): Loss_D: 0.2251 Loss_G: 0.8906 MSE 98617.2969
===> Epoch[3](510/704): Loss_D: 0.2164 Loss_G: 0.5351 MSE 97365.7891
===> Epoch[3](540/704): Loss_D: 0.1859 Loss_G: 0.7138 MSE 93390.5781
===> Epoch[3](570/704): Loss_D: 0.2216 Loss_G: 0.6094 MSE 90184.9844
===> Epoch[3](600/704): Loss_D: 0.1467 Loss_G: 0.7381 MSE 90237.7344
===> Epoch[3](630/704): Loss_D: 0.2512 Loss_G: 0.7649 MSE 95064.7656
===> Epoch[3](660/704): Loss_D: 0.1316 Loss_G: 0.8631 MSE 99576.6250
===> Epoch[3](690/704): Loss_D: 0.2527 Loss_G: 0.9042 MSE 89664.4219
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[4](0/704): Loss_D: 0.1705 Loss_G: 0.7182 MSE 92847.6797
===> Epoch[4](30/704): Loss_D: 0.1951 Loss_G: 0.6168 MSE 88538.6484
===> Epoch[4](60/704): Loss_D: 0.2119 Loss_G: 0.7828 MSE 88801.7891
===> Epoch[4](90/704): Loss_D: 0.2021 Loss_G: 0.7675 MSE 84984.3438
===> Epoch[4](120/704): Loss_D: 0.1625 Loss_G: 0.9620 MSE 87538.8359
===> Epoch[4](150/704): Loss_D: 0.1452 Loss_G: 0.7403 MSE 85968.1406
===> Epoch[4](180/704): Loss_D: 0.2204 Loss_G: 0.6141 MSE 84512.6875
===> Epoch[4](210/704): Loss_D: 0.1942 Loss_G: 1.0446 MSE 92887.5156
===> Epoch[4](240/704): Loss_D: 0.1790 Loss_G: 0.7223 MSE 88971.6875
===> Epoch[4](270/704): Loss_D: 0.2075 Loss_G: 0.6460 MSE 88750.0625
===> Epoch[4](300/704): Loss_D: 0.1927 Loss_G: 0.6035 MSE 89173.7266
===> Epoch[4](330/704): Loss_D: 0.2049 Loss_G: 0.7810 MSE 90306.0000
===> Epoch[4](360/704): Loss_D: 0.2085 Loss_G: 0.6899 MSE 98301.4062
===> Epoch[4](390/704): Loss_D: 0.2211 Loss_G: 0.6671 MSE 93357.2656
===> Epoch[4](420/704): Loss_D: 0.2341 Loss_G: 0.9905 MSE 98022.5156
===> Epoch[4](450/704): Loss_D: 0.2554 Loss_G: 0.8704 MSE 97136.6484
===> Epoch[4](480/704): Loss_D: 0.2004 Loss_G: 0.7657 MSE 91634.7109
===> Epoch[4](510/704): Loss_D: 0.1905 Loss_G: 0.6687 MSE 98530.7344
===> Epoch[4](540/704): Loss_D: 0.1796 Loss_G: 0.9549 MSE 97243.9531
===> Epoch[4](570/704): Loss_D: 0.1863 Loss_G: 0.7134 MSE 93636.4531
===> Epoch[4](600/704): Loss_D: 0.1767 Loss_G: 0.8416 MSE 101078.3125
===> Epoch[4](630/704): Loss_D: 0.1869 Loss_G: 0.6233 MSE 93030.1094
===> Epoch[4](660/704): Loss_D: 0.2405 Loss_G: 0.7567 MSE 96671.9453
===> Epoch[4](690/704): Loss_D: 0.1782 Loss_G: 0.7766 MSE 88308.3047
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[5](0/704): Loss_D: 0.1420 Loss_G: 0.8758 MSE 90318.7188
===> Epoch[5](30/704): Loss_D: 0.1582 Loss_G: 0.7653 MSE 93039.9531
===> Epoch[5](60/704): Loss_D: 0.2301 Loss_G: 0.4980 MSE 94095.9453
===> Epoch[5](90/704): Loss_D: 0.1834 Loss_G: 0.7789 MSE 90502.3047
===> Epoch[5](120/704): Loss_D: 0.1625 Loss_G: 0.6680 MSE 92413.0312
===> Epoch[5](150/704): Loss_D: 0.1841 Loss_G: 0.6712 MSE 86304.3828
===> Epoch[5](180/704): Loss_D: 0.1850 Loss_G: 0.8023 MSE 90022.6875
===> Epoch[5](210/704): Loss_D: 0.1951 Loss_G: 0.6877 MSE 90665.8828
===> Epoch[5](240/704): Loss_D: 0.1623 Loss_G: 0.7357 MSE 87807.1797
===> Epoch[5](270/704): Loss_D: 0.1924 Loss_G: 0.6511 MSE 88844.4609
===> Epoch[5](300/704): Loss_D: 0.1584 Loss_G: 1.0442 MSE 91986.3047
===> Epoch[5](330/704): Loss_D: 0.1696 Loss_G: 0.8451 MSE 99468.4531
===> Epoch[5](360/704): Loss_D: 0.2189 Loss_G: 0.6541 MSE 96071.4766
===> Epoch[5](390/704): Loss_D: 0.1955 Loss_G: 0.6466 MSE 91162.0312
===> Epoch[5](420/704): Loss_D: 0.1775 Loss_G: 0.5293 MSE 96129.3750
===> Epoch[5](450/704): Loss_D: 0.2022 Loss_G: 0.5970 MSE 91196.4062
===> Epoch[5](480/704): Loss_D: 0.1981 Loss_G: 0.6551 MSE 95383.9922
===> Epoch[5](510/704): Loss_D: 0.1800 Loss_G: 0.7263 MSE 93820.0781
===> Epoch[5](540/704): Loss_D: 0.1418 Loss_G: 0.7059 MSE 99945.2422
===> Epoch[5](570/704): Loss_D: 0.1703 Loss_G: 0.7415 MSE 93036.1719
===> Epoch[5](600/704): Loss_D: 0.1911 Loss_G: 0.8489 MSE 95641.1562
===> Epoch[5](630/704): Loss_D: 0.1234 Loss_G: 1.1360 MSE 88702.8438
===> Epoch[5](660/704): Loss_D: 0.1504 Loss_G: 0.7881 MSE 86683.1875
===> Epoch[5](690/704): Loss_D: 0.1418 Loss_G: 0.8678 MSE 83362.0625
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[6](0/704): Loss_D: 0.1749 Loss_G: 1.5947 MSE 90628.1250
===> Epoch[6](30/704): Loss_D: 0.1422 Loss_G: 0.7559 MSE 96150.4844
===> Epoch[6](60/704): Loss_D: 0.1621 Loss_G: 0.7957 MSE 94231.5234
===> Epoch[6](90/704): Loss_D: 0.3393 Loss_G: 0.5295 MSE 93212.9062
===> Epoch[6](120/704): Loss_D: 0.1984 Loss_G: 0.8793 MSE 94630.8516
===> Epoch[6](150/704): Loss_D: 0.2139 Loss_G: 0.7318 MSE 91855.9766
===> Epoch[6](180/704): Loss_D: 0.1981 Loss_G: 0.7692 MSE 90084.7344
===> Epoch[6](210/704): Loss_D: 0.1508 Loss_G: 0.7496 MSE 96083.4766
===> Epoch[6](240/704): Loss_D: 0.2064 Loss_G: 0.8444 MSE 88693.8438
===> Epoch[6](270/704): Loss_D: 0.1329 Loss_G: 0.8603 MSE 88191.7891
===> Epoch[6](300/704): Loss_D: 0.1631 Loss_G: 1.0207 MSE 94529.3906
===> Epoch[6](330/704): Loss_D: 0.1914 Loss_G: 0.6366 MSE 89717.3047
===> Epoch[6](360/704): Loss_D: 0.2126 Loss_G: 0.6992 MSE 93829.4766
===> Epoch[6](390/704): Loss_D: 0.2472 Loss_G: 0.6578 MSE 89365.7031
===> Epoch[6](420/704): Loss_D: 0.1253 Loss_G: 0.8561 MSE 94485.9844
===> Epoch[6](450/704): Loss_D: 0.1926 Loss_G: 0.6277 MSE 85720.6719
===> Epoch[6](480/704): Loss_D: 0.2548 Loss_G: 0.7552 MSE 96944.7188
===> Epoch[6](510/704): Loss_D: 0.1373 Loss_G: 0.9696 MSE 97486.2344
===> Epoch[6](540/704): Loss_D: 0.1853 Loss_G: 0.5388 MSE 99279.0156
===> Epoch[6](570/704): Loss_D: 0.2030 Loss_G: 0.5677 MSE 95496.0000
===> Epoch[6](600/704): Loss_D: 0.1671 Loss_G: 0.6209 MSE 100615.4688
===> Epoch[6](630/704): Loss_D: 0.2244 Loss_G: 0.8516 MSE 98308.1641
===> Epoch[6](660/704): Loss_D: 0.2220 Loss_G: 0.5516 MSE 97211.2812
===> Epoch[6](690/704): Loss_D: 0.2723 Loss_G: 0.7920 MSE 96584.1094
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[7](0/704): Loss_D: 0.2163 Loss_G: 0.8762 MSE 103171.4922
===> Epoch[7](30/704): Loss_D: 0.1743 Loss_G: 0.6030 MSE 102760.2500
===> Epoch[7](60/704): Loss_D: 0.2065 Loss_G: 0.5942 MSE 103146.4688
===> Epoch[7](90/704): Loss_D: 0.2323 Loss_G: 0.6524 MSE 104001.7969
===> Epoch[7](120/704): Loss_D: 0.1583 Loss_G: 0.7383 MSE 105391.1719
===> Epoch[7](150/704): Loss_D: 0.2134 Loss_G: 0.5539 MSE 102899.7812
===> Epoch[7](180/704): Loss_D: 0.1424 Loss_G: 0.7260 MSE 102891.7500
===> Epoch[7](210/704): Loss_D: 0.2174 Loss_G: 0.7027 MSE 106437.7500
===> Epoch[7](240/704): Loss_D: 0.1511 Loss_G: 0.8381 MSE 104162.1016
===> Epoch[7](270/704): Loss_D: 0.2093 Loss_G: 0.5494 MSE 100096.9375
===> Epoch[7](300/704): Loss_D: 0.2999 Loss_G: 0.9133 MSE 94552.3984
===> Epoch[7](330/704): Loss_D: 0.2173 Loss_G: 0.5764 MSE 95759.0859
===> Epoch[7](360/704): Loss_D: 0.1485 Loss_G: 0.8467 MSE 98305.7891
===> Epoch[7](390/704): Loss_D: 0.2674 Loss_G: 1.0996 MSE 99095.0625
===> Epoch[7](420/704): Loss_D: 0.1940 Loss_G: 0.4627 MSE 103160.9219
===> Epoch[7](450/704): Loss_D: 0.1637 Loss_G: 0.6330 MSE 99608.2031
===> Epoch[7](480/704): Loss_D: 0.1932 Loss_G: 0.5379 MSE 101381.5547
===> Epoch[7](510/704): Loss_D: 0.1591 Loss_G: 0.7338 MSE 104488.6094
===> Epoch[7](540/704): Loss_D: 0.1509 Loss_G: 0.9004 MSE 98294.8750
===> Epoch[7](570/704): Loss_D: 0.2331 Loss_G: 0.7528 MSE 101182.0078
===> Epoch[7](600/704): Loss_D: 0.1412 Loss_G: 0.8958 MSE 96548.1875
===> Epoch[7](630/704): Loss_D: 0.2083 Loss_G: 0.5179 MSE 93943.2031
===> Epoch[7](660/704): Loss_D: 0.1308 Loss_G: 0.7257 MSE 102410.5469
===> Epoch[7](690/704): Loss_D: 0.1415 Loss_G: 0.7119 MSE 95232.2344
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[8](0/704): Loss_D: 0.4266 Loss_G: 1.0312 MSE 99963.7031
===> Epoch[8](30/704): Loss_D: 0.1063 Loss_G: 0.8153 MSE 93351.1406
===> Epoch[8](60/704): Loss_D: 0.1849 Loss_G: 0.7629 MSE 96766.3672
===> Epoch[8](90/704): Loss_D: 0.2779 Loss_G: 0.8355 MSE 94994.4375
===> Epoch[8](120/704): Loss_D: 0.2557 Loss_G: 0.6145 MSE 91868.4141
===> Epoch[8](150/704): Loss_D: 0.1690 Loss_G: 0.7432 MSE 93800.7422
===> Epoch[8](180/704): Loss_D: 0.1992 Loss_G: 0.6971 MSE 95757.9062
===> Epoch[8](210/704): Loss_D: 0.2186 Loss_G: 0.4947 MSE 94798.0391
===> Epoch[8](240/704): Loss_D: 0.1732 Loss_G: 0.8630 MSE 99495.0312
===> Epoch[8](270/704): Loss_D: 0.1468 Loss_G: 0.8640 MSE 103784.8125
===> Epoch[8](300/704): Loss_D: 0.1712 Loss_G: 0.7188 MSE 103375.1250
===> Epoch[8](330/704): Loss_D: 0.2040 Loss_G: 0.5934 MSE 92478.3906
===> Epoch[8](360/704): Loss_D: 0.1334 Loss_G: 0.9622 MSE 95633.5391
===> Epoch[8](390/704): Loss_D: 0.1805 Loss_G: 0.6587 MSE 96717.7812
===> Epoch[8](420/704): Loss_D: 0.1854 Loss_G: 0.6363 MSE 99131.7578
===> Epoch[8](450/704): Loss_D: 0.1779 Loss_G: 0.6181 MSE 96667.4688
===> Epoch[8](480/704): Loss_D: 0.0849 Loss_G: 0.6913 MSE 91308.4375
===> Epoch[8](510/704): Loss_D: 0.2305 Loss_G: 0.5421 MSE 97681.4375
===> Epoch[8](540/704): Loss_D: 0.1541 Loss_G: 1.0009 MSE 100721.8672
===> Epoch[8](570/704): Loss_D: 0.1899 Loss_G: 0.7847 MSE 98004.9844
===> Epoch[8](600/704): Loss_D: 0.1540 Loss_G: 0.7195 MSE 88266.5938
===> Epoch[8](630/704): Loss_D: 0.1411 Loss_G: 0.8758 MSE 96564.7031
===> Epoch[8](660/704): Loss_D: 0.2033 Loss_G: 0.5435 MSE 100340.3750
===> Epoch[8](690/704): Loss_D: 0.1317 Loss_G: 0.7546 MSE 96296.9375
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[9](0/704): Loss_D: 0.4577 Loss_G: 0.8287 MSE 95135.0000
===> Epoch[9](30/704): Loss_D: 0.1362 Loss_G: 0.6569 MSE 99433.0000
===> Epoch[9](60/704): Loss_D: 0.1258 Loss_G: 0.7072 MSE 98185.0156
===> Epoch[9](90/704): Loss_D: 0.2395 Loss_G: 0.6424 MSE 96341.1172
===> Epoch[9](120/704): Loss_D: 0.2216 Loss_G: 0.9551 MSE 95045.7812
===> Epoch[9](150/704): Loss_D: 0.1741 Loss_G: 0.7211 MSE 95441.8203
===> Epoch[9](180/704): Loss_D: 0.1888 Loss_G: 0.7746 MSE 93593.3516
===> Epoch[9](210/704): Loss_D: 0.1535 Loss_G: 0.8620 MSE 100416.6641
===> Epoch[9](240/704): Loss_D: 0.1722 Loss_G: 0.6436 MSE 100209.5312
===> Epoch[9](270/704): Loss_D: 0.1666 Loss_G: 0.6635 MSE 94882.9688
===> Epoch[9](300/704): Loss_D: 0.1653 Loss_G: 0.6437 MSE 96186.8984
===> Epoch[9](330/704): Loss_D: 0.2209 Loss_G: 0.5564 MSE 93284.1562
===> Epoch[9](360/704): Loss_D: 0.1177 Loss_G: 1.0320 MSE 94238.9531
===> Epoch[9](390/704): Loss_D: 0.1083 Loss_G: 0.7672 MSE 98564.8203
===> Epoch[9](420/704): Loss_D: 0.1577 Loss_G: 0.7350 MSE 94449.7969
===> Epoch[9](450/704): Loss_D: 0.2021 Loss_G: 0.7221 MSE 99202.4922
===> Epoch[9](480/704): Loss_D: 0.1900 Loss_G: 0.5889 MSE 93955.0547
===> Epoch[9](510/704): Loss_D: 0.2630 Loss_G: 1.2144 MSE 92896.2188
===> Epoch[9](540/704): Loss_D: 0.1163 Loss_G: 0.7301 MSE 94176.1406
===> Epoch[9](570/704): Loss_D: 0.1970 Loss_G: 0.5340 MSE 97264.1641
===> Epoch[9](600/704): Loss_D: 0.1949 Loss_G: 0.6678 MSE 91119.3047
===> Epoch[9](630/704): Loss_D: 0.2119 Loss_G: 0.8171 MSE 94822.6406
===> Epoch[9](660/704): Loss_D: 0.1770 Loss_G: 0.6508 MSE 93584.6797
===> Epoch[9](690/704): Loss_D: 0.1550 Loss_G: 0.6674 MSE 90099.4297
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[10](0/704): Loss_D: 0.0954 Loss_G: 0.7808 MSE 97459.3750
===> Epoch[10](30/704): Loss_D: 0.1584 Loss_G: 0.8304 MSE 92430.6016
===> Epoch[10](60/704): Loss_D: 0.1378 Loss_G: 0.7773 MSE 91742.9062
===> Epoch[10](90/704): Loss_D: 0.1942 Loss_G: 0.5900 MSE 92157.7344
===> Epoch[10](120/704): Loss_D: 0.1834 Loss_G: 0.6668 MSE 93470.8281
===> Epoch[10](150/704): Loss_D: 0.1125 Loss_G: 0.7377 MSE 95358.8125
===> Epoch[10](180/704): Loss_D: 0.1419 Loss_G: 0.9838 MSE 91355.6641
===> Epoch[10](210/704): Loss_D: 0.1856 Loss_G: 0.5225 MSE 98858.6562
===> Epoch[10](240/704): Loss_D: 0.1365 Loss_G: 0.8789 MSE 99990.9531
===> Epoch[10](270/704): Loss_D: 0.3050 Loss_G: 0.8154 MSE 97546.0156
===> Epoch[10](300/704): Loss_D: 0.2454 Loss_G: 1.2694 MSE 101176.9453
===> Epoch[10](330/704): Loss_D: 0.2013 Loss_G: 0.7478 MSE 94762.4922
===> Epoch[10](360/704): Loss_D: 0.1473 Loss_G: 0.6179 MSE 95418.9766
===> Epoch[10](390/704): Loss_D: 0.2534 Loss_G: 0.6418 MSE 108100.8125
===> Epoch[10](420/704): Loss_D: 0.1187 Loss_G: 0.9254 MSE 101707.7656
===> Epoch[10](450/704): Loss_D: 0.1926 Loss_G: 0.7290 MSE 95513.6094
===> Epoch[10](480/704): Loss_D: 0.1758 Loss_G: 0.6090 MSE 96330.2266
===> Epoch[10](510/704): Loss_D: 0.1681 Loss_G: 0.5932 MSE 92690.7500
===> Epoch[10](540/704): Loss_D: 0.1794 Loss_G: 0.6846 MSE 95427.6250
===> Epoch[10](570/704): Loss_D: 0.1988 Loss_G: 0.8492 MSE 93443.3438
===> Epoch[10](600/704): Loss_D: 0.2160 Loss_G: 0.4834 MSE 93258.7969
===> Epoch[10](630/704): Loss_D: 0.1319 Loss_G: 0.7882 MSE 89758.0156
===> Epoch[10](660/704): Loss_D: 0.1653 Loss_G: 0.6400 MSE 90395.3281
===> Epoch[10](690/704): Loss_D: 0.1970 Loss_G: 0.8048 MSE 88468.7500
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[11](0/704): Loss_D: 0.2904 Loss_G: 0.7067 MSE 92611.1875
===> Epoch[11](30/704): Loss_D: 0.2842 Loss_G: 0.7014 MSE 99762.3281
===> Epoch[11](60/704): Loss_D: 0.1979 Loss_G: 0.7624 MSE 94278.4375
===> Epoch[11](90/704): Loss_D: 0.1844 Loss_G: 0.5394 MSE 96243.0000
===> Epoch[11](120/704): Loss_D: 0.2340 Loss_G: 0.5214 MSE 96470.2344
===> Epoch[11](150/704): Loss_D: 0.1964 Loss_G: 0.7363 MSE 98361.4531
===> Epoch[11](180/704): Loss_D: 0.1466 Loss_G: 0.9702 MSE 89563.7656
===> Epoch[11](210/704): Loss_D: 0.1202 Loss_G: 0.8060 MSE 100592.7422
===> Epoch[11](240/704): Loss_D: 0.2125 Loss_G: 0.4525 MSE 101934.4062
===> Epoch[11](270/704): Loss_D: 0.1157 Loss_G: 0.6010 MSE 100728.4922
===> Epoch[11](300/704): Loss_D: 0.1955 Loss_G: 0.7213 MSE 95804.9062
===> Epoch[11](330/704): Loss_D: 0.2482 Loss_G: 0.6416 MSE 92888.4766
===> Epoch[11](360/704): Loss_D: 0.1567 Loss_G: 0.5816 MSE 101757.5859
===> Epoch[11](390/704): Loss_D: 0.2756 Loss_G: 0.6014 MSE 98416.4531
===> Epoch[11](420/704): Loss_D: 0.1562 Loss_G: 0.6459 MSE 99535.9844
===> Epoch[11](450/704): Loss_D: 0.1647 Loss_G: 1.0177 MSE 94033.9844
===> Epoch[11](480/704): Loss_D: 0.1772 Loss_G: 0.6018 MSE 92643.4219
===> Epoch[11](510/704): Loss_D: 0.1982 Loss_G: 0.5150 MSE 101662.5469
===> Epoch[11](540/704): Loss_D: 0.2068 Loss_G: 0.6849 MSE 98299.4375
===> Epoch[11](570/704): Loss_D: 0.2251 Loss_G: 0.6908 MSE 95329.6406
===> Epoch[11](600/704): Loss_D: 0.1113 Loss_G: 0.5568 MSE 91543.7734
===> Epoch[11](630/704): Loss_D: 0.1583 Loss_G: 0.7886 MSE 94522.4844
===> Epoch[11](660/704): Loss_D: 0.1731 Loss_G: 0.6879 MSE 94007.8359
===> Epoch[11](690/704): Loss_D: 0.1574 Loss_G: 0.6982 MSE 93445.3047
learning rate = 0.0010000
learning rate = 0.0010000
Saving
===> Epoch[12](0/704): Loss_D: 0.1429 Loss_G: 1.1403 MSE 95485.7109
===> Epoch[12](30/704): Loss_D: 0.1537 Loss_G: 0.7141 MSE 92648.2031
===> Epoch[12](60/704): Loss_D: 0.1029 Loss_G: 0.7693 MSE 100197.9531
===> Epoch[12](90/704): Loss_D: 0.1833 Loss_G: 0.6483 MSE 93083.9688
===> Epoch[12](120/704): Loss_D: 0.1819 Loss_G: 0.5693 MSE 97538.5156
===> Epoch[12](150/704): Loss_D: 0.2312 Loss_G: 1.0420 MSE 91362.9844
===> Epoch[12](180/704): Loss_D: 0.1469 Loss_G: 0.8474 MSE 95163.7656
===> Epoch[12](210/704): Loss_D: 0.1702 Loss_G: 0.6946 MSE 100653.0312
===> Epoch[12](240/704): Loss_D: 0.1912 Loss_G: 0.6060 MSE 95300.6328
===> Epoch[12](270/704): Loss_D: 0.2264 Loss_G: 0.5671 MSE 99492.2969
===> Epoch[12](300/704): Loss_D: 0.1851 Loss_G: 0.6470 MSE 95390.4531
===> Epoch[12](330/704): Loss_D: 0.2007 Loss_G: 0.4948 MSE 100348.5078
===> Epoch[12](360/704): Loss_D: 0.1907 Loss_G: 0.6701 MSE 98552.3047
===> Epoch[12](390/704): Loss_D: 0.2593 Loss_G: 1.0199 MSE 93699.4688
===> Epoch[12](420/704): Loss_D: 0.2626 Loss_G: 0.6009 MSE 100087.5234
===> Epoch[12](450/704): Loss_D: 0.1880 Loss_G: 0.5805 MSE 91386.9375
===> Epoch[12](480/704): Loss_D: 0.1544 Loss_G: 0.5765 MSE 99260.3281
===> Epoch[12](510/704): Loss_D: 0.1548 Loss_G: 0.7578 MSE 99003.5625
===> Epoch[12](540/704): Loss_D: 0.1356 Loss_G: 1.2001 MSE 102639.6953
===> Epoch[12](570/704): Loss_D: 0.1676 Loss_G: 0.7135 MSE 95297.8047
===> Epoch[12](600/704): Loss_D: 0.1536 Loss_G: 0.7822 MSE 101596.2578
===> Epoch[12](630/704): Loss_D: 0.1677 Loss_G: 0.7403 MSE 100320.9531
===> Epoch[12](660/704): Loss_D: 0.1529 Loss_G: 0.6492 MSE 94139.9531
===> Epoch[12](690/704): Loss_D: 0.2164 Loss_G: 0.5355 MSE 94020.5781
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[13](0/704): Loss_D: 0.2252 Loss_G: 0.7513 MSE 93224.5469
===> Epoch[13](30/704): Loss_D: 0.1537 Loss_G: 0.6905 MSE 96345.0938
===> Epoch[13](60/704): Loss_D: 0.1729 Loss_G: 0.5055 MSE 96583.5469
===> Epoch[13](90/704): Loss_D: 0.2378 Loss_G: 0.5171 MSE 96627.3281
===> Epoch[13](120/704): Loss_D: 0.2093 Loss_G: 0.4765 MSE 98047.9688
===> Epoch[13](150/704): Loss_D: 0.2381 Loss_G: 0.7004 MSE 101397.1406
===> Epoch[13](180/704): Loss_D: 0.1934 Loss_G: 0.8666 MSE 104115.4453
===> Epoch[13](210/704): Loss_D: 0.2144 Loss_G: 0.5946 MSE 96797.4609
===> Epoch[13](240/704): Loss_D: 0.1821 Loss_G: 0.6714 MSE 98798.0625
===> Epoch[13](270/704): Loss_D: 0.2069 Loss_G: 0.5595 MSE 101554.2812
===> Epoch[13](300/704): Loss_D: 0.1339 Loss_G: 0.7264 MSE 102067.2969
===> Epoch[13](330/704): Loss_D: 0.2416 Loss_G: 0.8185 MSE 100358.7656
===> Epoch[13](360/704): Loss_D: 0.1000 Loss_G: 0.6496 MSE 94741.4609
===> Epoch[13](390/704): Loss_D: 0.1575 Loss_G: 0.8303 MSE 91845.7656
===> Epoch[13](420/704): Loss_D: 0.1879 Loss_G: 0.8796 MSE 93220.2500
===> Epoch[13](450/704): Loss_D: 0.1940 Loss_G: 0.5410 MSE 94585.1562
===> Epoch[13](480/704): Loss_D: 0.1700 Loss_G: 0.5069 MSE 96807.1172
===> Epoch[13](510/704): Loss_D: 0.2350 Loss_G: 0.4993 MSE 98074.5234
===> Epoch[13](540/704): Loss_D: 0.2600 Loss_G: 0.5300 MSE 103853.1719
===> Epoch[13](570/704): Loss_D: 0.2921 Loss_G: 0.4149 MSE 101026.5625
===> Epoch[13](600/704): Loss_D: 0.1474 Loss_G: 0.8558 MSE 98009.1562
===> Epoch[13](630/704): Loss_D: 0.1338 Loss_G: 0.9123 MSE 101031.6406
===> Epoch[13](660/704): Loss_D: 0.1518 Loss_G: 0.7962 MSE 104876.8984
===> Epoch[13](690/704): Loss_D: 0.1787 Loss_G: 0.6024 MSE 94018.9375
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[14](0/704): Loss_D: 0.2757 Loss_G: 0.8905 MSE 97224.9922
===> Epoch[14](30/704): Loss_D: 0.1622 Loss_G: 0.7100 MSE 99776.6406
===> Epoch[14](60/704): Loss_D: 0.1478 Loss_G: 0.7428 MSE 98713.2812
===> Epoch[14](90/704): Loss_D: 0.1944 Loss_G: 0.7748 MSE 98378.1875
===> Epoch[14](120/704): Loss_D: 0.2228 Loss_G: 0.6240 MSE 99283.1250
===> Epoch[14](150/704): Loss_D: 0.1218 Loss_G: 0.8148 MSE 101523.6094
===> Epoch[14](180/704): Loss_D: 0.1511 Loss_G: 0.8193 MSE 103322.1328
===> Epoch[14](210/704): Loss_D: 0.1699 Loss_G: 0.7118 MSE 101394.9531
===> Epoch[14](240/704): Loss_D: 0.2023 Loss_G: 0.5881 MSE 102063.2031
===> Epoch[14](270/704): Loss_D: 0.1931 Loss_G: 0.9264 MSE 95645.3203
===> Epoch[14](300/704): Loss_D: 0.1428 Loss_G: 0.8699 MSE 93795.2969
===> Epoch[14](330/704): Loss_D: 0.2042 Loss_G: 0.6944 MSE 101299.0156
===> Epoch[14](360/704): Loss_D: 0.1407 Loss_G: 0.7822 MSE 107537.4062
===> Epoch[14](390/704): Loss_D: 0.2507 Loss_G: 0.4714 MSE 94524.6953
===> Epoch[14](420/704): Loss_D: 0.2091 Loss_G: 0.5372 MSE 99194.7188
===> Epoch[14](450/704): Loss_D: 0.1556 Loss_G: 0.6403 MSE 95100.0859
===> Epoch[14](480/704): Loss_D: 0.1629 Loss_G: 0.6350 MSE 102028.9766
===> Epoch[14](510/704): Loss_D: 0.2209 Loss_G: 0.8678 MSE 100333.5391
===> Epoch[14](540/704): Loss_D: 0.3143 Loss_G: 0.4963 MSE 91623.4531
===> Epoch[14](570/704): Loss_D: 0.1776 Loss_G: 0.7066 MSE 96132.1094
===> Epoch[14](600/704): Loss_D: 0.1645 Loss_G: 0.6208 MSE 97015.8984
===> Epoch[14](630/704): Loss_D: 0.1869 Loss_G: 0.6996 MSE 98091.3125
===> Epoch[14](660/704): Loss_D: 0.2347 Loss_G: 0.5209 MSE 96117.9531
===> Epoch[14](690/704): Loss_D: 0.1811 Loss_G: 0.5276 MSE 106646.6094
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[15](0/704): Loss_D: 0.2669 Loss_G: 0.7218 MSE 96216.2344
===> Epoch[15](30/704): Loss_D: 0.1699 Loss_G: 0.6237 MSE 100629.1484
===> Epoch[15](60/704): Loss_D: 0.1837 Loss_G: 0.6894 MSE 98133.1875
===> Epoch[15](90/704): Loss_D: 0.2433 Loss_G: 0.5567 MSE 98134.2422
===> Epoch[15](120/704): Loss_D: 0.3237 Loss_G: 1.0102 MSE 94470.8281
===> Epoch[15](150/704): Loss_D: 0.1238 Loss_G: 0.7599 MSE 101231.7266
===> Epoch[15](180/704): Loss_D: 0.1642 Loss_G: 0.8644 MSE 102235.6406
===> Epoch[15](210/704): Loss_D: 0.1442 Loss_G: 0.9110 MSE 104982.6797
===> Epoch[15](240/704): Loss_D: 0.1990 Loss_G: 0.6799 MSE 101220.8047
===> Epoch[15](270/704): Loss_D: 0.1474 Loss_G: 0.8112 MSE 103651.7656
===> Epoch[15](300/704): Loss_D: 0.1622 Loss_G: 0.7438 MSE 102581.5625
===> Epoch[15](330/704): Loss_D: 0.1727 Loss_G: 0.6707 MSE 95684.7656
===> Epoch[15](360/704): Loss_D: 0.2103 Loss_G: 0.6139 MSE 99802.0000
===> Epoch[15](390/704): Loss_D: 0.2041 Loss_G: 0.5497 MSE 99958.1250
===> Epoch[15](420/704): Loss_D: 0.2426 Loss_G: 0.5311 MSE 104576.7969
===> Epoch[15](450/704): Loss_D: 0.1427 Loss_G: 0.8709 MSE 96329.3359
===> Epoch[15](480/704): Loss_D: 0.1752 Loss_G: 0.6436 MSE 98317.6484
===> Epoch[15](510/704): Loss_D: 0.2425 Loss_G: 0.4794 MSE 99114.2500
===> Epoch[15](540/704): Loss_D: 0.1589 Loss_G: 0.4731 MSE 103893.7266
===> Epoch[15](570/704): Loss_D: 0.1240 Loss_G: 0.6878 MSE 106025.9531
===> Epoch[15](600/704): Loss_D: 0.1958 Loss_G: 0.7869 MSE 106005.8203
===> Epoch[15](630/704): Loss_D: 0.1620 Loss_G: 0.6522 MSE 99423.3203
===> Epoch[15](660/704): Loss_D: 0.2151 Loss_G: 0.7774 MSE 103303.1875
===> Epoch[15](690/704): Loss_D: 0.2045 Loss_G: 0.5638 MSE 103804.5469
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[16](0/704): Loss_D: 0.3744 Loss_G: 0.7182 MSE 100351.1562
===> Epoch[16](30/704): Loss_D: 0.2058 Loss_G: 0.6208 MSE 99182.1953
===> Epoch[16](60/704): Loss_D: 0.1703 Loss_G: 0.5819 MSE 100966.3516
===> Epoch[16](90/704): Loss_D: 0.1931 Loss_G: 0.8242 MSE 101524.7812
===> Epoch[16](120/704): Loss_D: 0.1181 Loss_G: 0.7557 MSE 97716.9062
===> Epoch[16](150/704): Loss_D: 0.1791 Loss_G: 0.6483 MSE 97929.7656
===> Epoch[16](180/704): Loss_D: 0.1948 Loss_G: 0.5912 MSE 99503.1328
===> Epoch[16](210/704): Loss_D: 0.2017 Loss_G: 0.7231 MSE 98286.3047
===> Epoch[16](240/704): Loss_D: 0.1009 Loss_G: 0.9300 MSE 95145.8750
===> Epoch[16](270/704): Loss_D: 0.3037 Loss_G: 0.5309 MSE 101371.1328
===> Epoch[16](300/704): Loss_D: 0.2050 Loss_G: 0.7333 MSE 98689.8906
===> Epoch[16](330/704): Loss_D: 0.1682 Loss_G: 0.7996 MSE 99682.7500
===> Epoch[16](360/704): Loss_D: 0.1914 Loss_G: 0.5257 MSE 97525.5547
===> Epoch[16](390/704): Loss_D: 0.1748 Loss_G: 0.6634 MSE 97583.4531
===> Epoch[16](420/704): Loss_D: 0.1647 Loss_G: 0.6992 MSE 95432.4844
===> Epoch[16](450/704): Loss_D: 0.1685 Loss_G: 1.0331 MSE 97296.7812
===> Epoch[16](480/704): Loss_D: 0.3045 Loss_G: 0.7380 MSE 97327.9531
===> Epoch[16](510/704): Loss_D: 0.1791 Loss_G: 0.8338 MSE 100463.5625
===> Epoch[16](540/704): Loss_D: 0.1701 Loss_G: 0.9320 MSE 97915.9141
===> Epoch[16](570/704): Loss_D: 0.3490 Loss_G: 0.5861 MSE 97720.8750
===> Epoch[16](600/704): Loss_D: 0.1693 Loss_G: 0.8334 MSE 94651.6875
===> Epoch[16](630/704): Loss_D: 0.0824 Loss_G: 0.7099 MSE 101839.0938
===> Epoch[16](660/704): Loss_D: 0.1502 Loss_G: 0.9371 MSE 98001.4297
===> Epoch[16](690/704): Loss_D: 0.1666 Loss_G: 0.7768 MSE 104066.5000
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[17](0/704): Loss_D: 0.2995 Loss_G: 0.7530 MSE 98354.0938
===> Epoch[17](30/704): Loss_D: 0.1310 Loss_G: 0.7507 MSE 104391.2812
===> Epoch[17](60/704): Loss_D: 0.1865 Loss_G: 0.4656 MSE 99998.8594
===> Epoch[17](90/704): Loss_D: 0.3249 Loss_G: 0.4183 MSE 98736.7266
===> Epoch[17](120/704): Loss_D: 0.1684 Loss_G: 0.6645 MSE 94323.3906
===> Epoch[17](150/704): Loss_D: 0.1741 Loss_G: 0.5535 MSE 96072.1562
===> Epoch[17](180/704): Loss_D: 0.1409 Loss_G: 0.6544 MSE 97520.8281
===> Epoch[17](210/704): Loss_D: 0.1740 Loss_G: 0.6017 MSE 99089.3125
===> Epoch[17](240/704): Loss_D: 0.1722 Loss_G: 0.6053 MSE 99993.4531
===> Epoch[17](270/704): Loss_D: 0.2181 Loss_G: 0.6228 MSE 97982.4531
===> Epoch[17](300/704): Loss_D: 0.2742 Loss_G: 0.3939 MSE 97956.7891
===> Epoch[17](330/704): Loss_D: 0.1816 Loss_G: 0.6532 MSE 94325.1406
===> Epoch[17](360/704): Loss_D: 0.2142 Loss_G: 0.7184 MSE 97827.5156
===> Epoch[17](390/704): Loss_D: 0.1170 Loss_G: 0.6640 MSE 101142.7734
===> Epoch[17](420/704): Loss_D: 0.2158 Loss_G: 0.6278 MSE 97769.8125
===> Epoch[17](450/704): Loss_D: 0.1675 Loss_G: 0.7036 MSE 91568.2969
===> Epoch[17](480/704): Loss_D: 0.1954 Loss_G: 0.6552 MSE 95344.8984
===> Epoch[17](510/704): Loss_D: 0.1667 Loss_G: 0.6179 MSE 100538.6406
===> Epoch[17](540/704): Loss_D: 0.2062 Loss_G: 0.4834 MSE 95032.3438
===> Epoch[17](570/704): Loss_D: 0.2089 Loss_G: 0.5329 MSE 97677.8750
===> Epoch[17](600/704): Loss_D: 0.2366 Loss_G: 0.6190 MSE 97519.2031
===> Epoch[17](630/704): Loss_D: 0.1246 Loss_G: 0.7046 MSE 94255.8438
===> Epoch[17](660/704): Loss_D: 0.1499 Loss_G: 0.6801 MSE 102083.4062
===> Epoch[17](690/704): Loss_D: 0.1743 Loss_G: 0.5527 MSE 99198.9219
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[18](0/704): Loss_D: 0.3063 Loss_G: 0.4642 MSE 101511.1406
===> Epoch[18](30/704): Loss_D: 0.2127 Loss_G: 0.5941 MSE 99294.6328
===> Epoch[18](60/704): Loss_D: 0.2291 Loss_G: 0.5690 MSE 101851.0625
===> Epoch[18](90/704): Loss_D: 0.3610 Loss_G: 0.4285 MSE 100988.3906
===> Epoch[18](120/704): Loss_D: 0.2092 Loss_G: 0.6388 MSE 95157.3750
===> Epoch[18](150/704): Loss_D: 0.1770 Loss_G: 0.7406 MSE 97368.9219
===> Epoch[18](180/704): Loss_D: 0.1779 Loss_G: 0.6396 MSE 97971.4375
===> Epoch[18](210/704): Loss_D: 0.1355 Loss_G: 0.6589 MSE 97230.6875
===> Epoch[18](240/704): Loss_D: 0.1892 Loss_G: 0.5737 MSE 96049.4844
===> Epoch[18](270/704): Loss_D: 0.2772 Loss_G: 0.6905 MSE 96756.2656
===> Epoch[18](300/704): Loss_D: 0.1483 Loss_G: 0.6948 MSE 105561.4375
===> Epoch[18](330/704): Loss_D: 0.1280 Loss_G: 0.9692 MSE 90086.6875
===> Epoch[18](360/704): Loss_D: 0.1599 Loss_G: 0.6452 MSE 98249.0938
===> Epoch[18](390/704): Loss_D: 0.1328 Loss_G: 0.9967 MSE 96298.1562
===> Epoch[18](420/704): Loss_D: 0.1247 Loss_G: 0.7040 MSE 106668.8438
===> Epoch[18](450/704): Loss_D: 0.2203 Loss_G: 0.8327 MSE 98793.4375
===> Epoch[18](480/704): Loss_D: 0.1565 Loss_G: 0.6222 MSE 93797.1875
===> Epoch[18](510/704): Loss_D: 0.1209 Loss_G: 0.7162 MSE 97280.8594
===> Epoch[18](540/704): Loss_D: 0.2336 Loss_G: 0.4563 MSE 98974.8438
===> Epoch[18](570/704): Loss_D: 0.1619 Loss_G: 0.6950 MSE 95400.8828
===> Epoch[18](600/704): Loss_D: 0.1587 Loss_G: 0.7649 MSE 100192.7266
===> Epoch[18](630/704): Loss_D: 0.2031 Loss_G: 0.5417 MSE 99115.6719
===> Epoch[18](660/704): Loss_D: 0.2077 Loss_G: 0.5893 MSE 100132.6719
===> Epoch[18](690/704): Loss_D: 0.2099 Loss_G: 0.4929 MSE 97832.8906
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[19](0/704): Loss_D: 0.4732 Loss_G: 0.4801 MSE 97991.5859
===> Epoch[19](30/704): Loss_D: 0.1965 Loss_G: 0.6970 MSE 98840.9531
===> Epoch[19](60/704): Loss_D: 0.1372 Loss_G: 0.5877 MSE 99352.9297
===> Epoch[19](90/704): Loss_D: 0.1632 Loss_G: 0.7079 MSE 96845.0781
===> Epoch[19](120/704): Loss_D: 0.3007 Loss_G: 0.4954 MSE 97982.7656
===> Epoch[19](150/704): Loss_D: 0.1572 Loss_G: 0.6636 MSE 96437.5469
===> Epoch[19](180/704): Loss_D: 0.1772 Loss_G: 0.7675 MSE 98016.6875
===> Epoch[19](210/704): Loss_D: 0.0969 Loss_G: 0.5205 MSE 98265.7578
===> Epoch[19](240/704): Loss_D: 0.1597 Loss_G: 1.0165 MSE 101799.0234
===> Epoch[19](270/704): Loss_D: 0.1668 Loss_G: 0.5050 MSE 99791.5938
===> Epoch[19](300/704): Loss_D: 0.1939 Loss_G: 0.7653 MSE 99178.0156
===> Epoch[19](330/704): Loss_D: 0.1018 Loss_G: 0.5589 MSE 99626.5000
===> Epoch[19](360/704): Loss_D: 0.2173 Loss_G: 0.5292 MSE 97420.1562
===> Epoch[19](390/704): Loss_D: 0.1783 Loss_G: 0.5209 MSE 95403.3438
===> Epoch[19](420/704): Loss_D: 0.1740 Loss_G: 0.6285 MSE 97673.8672
===> Epoch[19](450/704): Loss_D: 0.1810 Loss_G: 0.6776 MSE 97959.4062
===> Epoch[19](480/704): Loss_D: 0.2317 Loss_G: 0.5986 MSE 97744.4688
===> Epoch[19](510/704): Loss_D: 0.1698 Loss_G: 0.7020 MSE 102250.1719
===> Epoch[19](540/704): Loss_D: 0.2319 Loss_G: 0.5875 MSE 98909.6016
===> Epoch[19](570/704): Loss_D: 0.2182 Loss_G: 0.5600 MSE 94860.9609
===> Epoch[19](600/704): Loss_D: 0.1495 Loss_G: 0.6212 MSE 95224.6094
===> Epoch[19](630/704): Loss_D: 0.1527 Loss_G: 0.7684 MSE 99733.2969
===> Epoch[19](660/704): Loss_D: 0.2155 Loss_G: 0.6786 MSE 94980.5469
===> Epoch[19](690/704): Loss_D: 0.1319 Loss_G: 1.0602 MSE 96835.6641
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[20](0/704): Loss_D: 0.2704 Loss_G: 0.4476 MSE 97418.6328
===> Epoch[20](30/704): Loss_D: 0.1594 Loss_G: 0.8141 MSE 93373.2344
===> Epoch[20](60/704): Loss_D: 0.1826 Loss_G: 0.5635 MSE 98069.8594
===> Epoch[20](90/704): Loss_D: 0.1894 Loss_G: 0.6814 MSE 104379.7422
===> Epoch[20](120/704): Loss_D: 0.2361 Loss_G: 0.5093 MSE 100280.9531
===> Epoch[20](150/704): Loss_D: 0.1244 Loss_G: 0.6919 MSE 95546.2656
===> Epoch[20](180/704): Loss_D: 0.1051 Loss_G: 0.6988 MSE 103208.2500
===> Epoch[20](210/704): Loss_D: 0.2577 Loss_G: 0.5528 MSE 98317.6406
===> Epoch[20](240/704): Loss_D: 0.1026 Loss_G: 1.0696 MSE 92712.5625
===> Epoch[20](270/704): Loss_D: 0.0901 Loss_G: 0.7759 MSE 92669.5078
===> Epoch[20](300/704): Loss_D: 0.1785 Loss_G: 0.5715 MSE 97411.7578
===> Epoch[20](330/704): Loss_D: 0.1414 Loss_G: 0.6198 MSE 102025.7969
===> Epoch[20](360/704): Loss_D: 0.2041 Loss_G: 0.5314 MSE 98685.9609
===> Epoch[20](390/704): Loss_D: 0.1764 Loss_G: 0.6975 MSE 100071.1016
===> Epoch[20](420/704): Loss_D: 0.1792 Loss_G: 0.6020 MSE 101319.4062
===> Epoch[20](450/704): Loss_D: 0.1652 Loss_G: 0.6174 MSE 103075.9688
===> Epoch[20](480/704): Loss_D: 0.1773 Loss_G: 0.6012 MSE 99138.2656
===> Epoch[20](510/704): Loss_D: 0.2076 Loss_G: 0.5907 MSE 104332.2812
===> Epoch[20](540/704): Loss_D: 0.1992 Loss_G: 0.6943 MSE 100627.8594
===> Epoch[20](570/704): Loss_D: 0.1632 Loss_G: 0.9059 MSE 97225.0000
===> Epoch[20](600/704): Loss_D: 0.1381 Loss_G: 0.5710 MSE 98904.4297
===> Epoch[20](630/704): Loss_D: 0.1161 Loss_G: 0.9086 MSE 99267.3281
===> Epoch[20](660/704): Loss_D: 0.1831 Loss_G: 0.6334 MSE 92258.4219
===> Epoch[20](690/704): Loss_D: 0.1440 Loss_G: 0.7347 MSE 98911.5312
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[21](0/704): Loss_D: 0.1629 Loss_G: 0.5209 MSE 99884.0781
===> Epoch[21](30/704): Loss_D: 0.1582 Loss_G: 0.7465 MSE 103424.0391
===> Epoch[21](60/704): Loss_D: 0.1822 Loss_G: 0.6035 MSE 101206.7812
===> Epoch[21](90/704): Loss_D: 0.2098 Loss_G: 0.4209 MSE 101851.7344
===> Epoch[21](120/704): Loss_D: 0.1898 Loss_G: 0.8049 MSE 94267.4688
===> Epoch[21](150/704): Loss_D: 0.2955 Loss_G: 0.4256 MSE 93111.2656
===> Epoch[21](180/704): Loss_D: 0.1672 Loss_G: 0.5789 MSE 94466.6250
===> Epoch[21](210/704): Loss_D: 0.1683 Loss_G: 0.7399 MSE 99507.0703
===> Epoch[21](240/704): Loss_D: 0.1803 Loss_G: 0.5985 MSE 98429.0000
===> Epoch[21](270/704): Loss_D: 0.1189 Loss_G: 0.6992 MSE 101698.9297
===> Epoch[21](300/704): Loss_D: 0.2459 Loss_G: 0.5472 MSE 100607.8516
===> Epoch[21](330/704): Loss_D: 0.2203 Loss_G: 0.7042 MSE 95440.3438
===> Epoch[21](360/704): Loss_D: 0.2195 Loss_G: 0.4729 MSE 99611.2344
===> Epoch[21](390/704): Loss_D: 0.1141 Loss_G: 0.6437 MSE 91607.1094
===> Epoch[21](420/704): Loss_D: 0.2010 Loss_G: 0.8817 MSE 101933.1172
===> Epoch[21](450/704): Loss_D: 0.0943 Loss_G: 0.8101 MSE 94086.6562
===> Epoch[21](480/704): Loss_D: 0.1845 Loss_G: 0.6280 MSE 98693.1562
===> Epoch[21](510/704): Loss_D: 0.1339 Loss_G: 0.7805 MSE 101651.3516
===> Epoch[21](540/704): Loss_D: 0.1468 Loss_G: 0.5824 MSE 100865.9688
===> Epoch[21](570/704): Loss_D: 0.1311 Loss_G: 0.7716 MSE 98651.6562
===> Epoch[21](600/704): Loss_D: 0.1451 Loss_G: 0.7701 MSE 99902.6719
===> Epoch[21](630/704): Loss_D: 0.2435 Loss_G: 0.5378 MSE 98535.6797
===> Epoch[21](660/704): Loss_D: 0.1797 Loss_G: 1.0493 MSE 98050.7812
===> Epoch[21](690/704): Loss_D: 0.1574 Loss_G: 0.7265 MSE 96449.8984
learning rate = 0.0010000
learning rate = 0.0010000
Saving
===> Epoch[22](0/704): Loss_D: 0.2075 Loss_G: 1.4401 MSE 97701.9688
===> Epoch[22](30/704): Loss_D: 0.1680 Loss_G: 0.7837 MSE 103929.1016
===> Epoch[22](60/704): Loss_D: 0.1457 Loss_G: 0.6725 MSE 99877.8125
===> Epoch[22](90/704): Loss_D: 0.1930 Loss_G: 0.6734 MSE 101798.1875
===> Epoch[22](120/704): Loss_D: 0.1588 Loss_G: 0.7781 MSE 99048.8750
===> Epoch[22](150/704): Loss_D: 0.2021 Loss_G: 0.5576 MSE 92102.0625
===> Epoch[22](180/704): Loss_D: 0.1924 Loss_G: 0.6437 MSE 99269.1406
===> Epoch[22](210/704): Loss_D: 0.1434 Loss_G: 0.6767 MSE 99838.2969
===> Epoch[22](240/704): Loss_D: 0.1600 Loss_G: 0.5253 MSE 97109.8750
===> Epoch[22](270/704): Loss_D: 0.1797 Loss_G: 0.5787 MSE 100517.3203
===> Epoch[22](300/704): Loss_D: 0.1764 Loss_G: 0.5074 MSE 99882.0547
===> Epoch[22](330/704): Loss_D: 0.1683 Loss_G: 0.7887 MSE 96720.2656
===> Epoch[22](360/704): Loss_D: 0.1756 Loss_G: 0.6784 MSE 100721.2812
===> Epoch[22](390/704): Loss_D: 0.2332 Loss_G: 0.6433 MSE 99863.1094
===> Epoch[22](420/704): Loss_D: 0.1154 Loss_G: 0.6672 MSE 98774.1562
===> Epoch[22](450/704): Loss_D: 0.1968 Loss_G: 0.7810 MSE 105787.2969
===> Epoch[22](480/704): Loss_D: 0.1063 Loss_G: 0.8015 MSE 101832.9531
===> Epoch[22](510/704): Loss_D: 0.2188 Loss_G: 0.5899 MSE 104805.5781
===> Epoch[22](540/704): Loss_D: 0.2021 Loss_G: 0.5906 MSE 98788.8047
===> Epoch[22](570/704): Loss_D: 0.1518 Loss_G: 0.7242 MSE 100425.4766
===> Epoch[22](600/704): Loss_D: 0.1075 Loss_G: 0.9503 MSE 100249.0391
===> Epoch[22](630/704): Loss_D: 0.1801 Loss_G: 0.7493 MSE 97108.0547
===> Epoch[22](660/704): Loss_D: 0.2093 Loss_G: 0.6970 MSE 96316.4531
===> Epoch[22](690/704): Loss_D: 0.2203 Loss_G: 0.4009 MSE 94831.4688
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[23](0/704): Loss_D: 0.6150 Loss_G: 0.9206 MSE 100283.8125
===> Epoch[23](30/704): Loss_D: 0.1575 Loss_G: 0.7452 MSE 95137.7656
===> Epoch[23](60/704): Loss_D: 0.1358 Loss_G: 0.7317 MSE 100721.2969
===> Epoch[23](90/704): Loss_D: 0.2468 Loss_G: 0.5681 MSE 105049.8125
===> Epoch[23](120/704): Loss_D: 0.1748 Loss_G: 0.8045 MSE 98388.3594
===> Epoch[23](150/704): Loss_D: 0.3147 Loss_G: 0.3694 MSE 102128.1250
===> Epoch[23](180/704): Loss_D: 0.1773 Loss_G: 0.6203 MSE 94838.7500
===> Epoch[23](210/704): Loss_D: 0.1811 Loss_G: 0.6760 MSE 96938.7031
===> Epoch[23](240/704): Loss_D: 0.0868 Loss_G: 0.7316 MSE 93753.1406
===> Epoch[23](270/704): Loss_D: 0.2681 Loss_G: 0.3405 MSE 98896.2344
===> Epoch[23](300/704): Loss_D: 0.2149 Loss_G: 0.4846 MSE 98458.3125
===> Epoch[23](330/704): Loss_D: 0.2073 Loss_G: 0.5201 MSE 97352.1953
===> Epoch[23](360/704): Loss_D: 0.0929 Loss_G: 0.7023 MSE 96426.0703
===> Epoch[23](390/704): Loss_D: 0.1598 Loss_G: 0.7616 MSE 101822.9688
===> Epoch[23](420/704): Loss_D: 0.2288 Loss_G: 0.5270 MSE 103268.6328
===> Epoch[23](450/704): Loss_D: 0.1368 Loss_G: 0.6619 MSE 98724.9531
===> Epoch[23](480/704): Loss_D: 0.1436 Loss_G: 0.6527 MSE 104146.5781
===> Epoch[23](510/704): Loss_D: 0.1411 Loss_G: 0.5666 MSE 98748.6719
===> Epoch[23](540/704): Loss_D: 0.1780 Loss_G: 0.5665 MSE 94345.9531
===> Epoch[23](570/704): Loss_D: 0.1419 Loss_G: 0.6450 MSE 97004.7031
===> Epoch[23](600/704): Loss_D: 0.3598 Loss_G: 0.8842 MSE 102567.4375
===> Epoch[23](630/704): Loss_D: 0.1921 Loss_G: 0.4808 MSE 95693.8125
===> Epoch[23](660/704): Loss_D: 0.1621 Loss_G: 0.4620 MSE 100756.2500
===> Epoch[23](690/704): Loss_D: 0.0997 Loss_G: 0.8838 MSE 100306.4219
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[24](0/704): Loss_D: 0.4603 Loss_G: 0.6457 MSE 94102.9688
===> Epoch[24](30/704): Loss_D: 0.2557 Loss_G: 0.5640 MSE 100848.5469
===> Epoch[24](60/704): Loss_D: 0.1797 Loss_G: 0.5574 MSE 95050.7422
===> Epoch[24](90/704): Loss_D: 0.2132 Loss_G: 0.7557 MSE 99558.2188
===> Epoch[24](120/704): Loss_D: 0.1599 Loss_G: 0.5349 MSE 103654.7500
===> Epoch[24](150/704): Loss_D: 0.2030 Loss_G: 0.6523 MSE 106419.3438
===> Epoch[24](180/704): Loss_D: 0.1564 Loss_G: 0.5401 MSE 105161.4688
===> Epoch[24](210/704): Loss_D: 0.1411 Loss_G: 0.7038 MSE 101443.5781
===> Epoch[24](240/704): Loss_D: 0.1680 Loss_G: 0.7347 MSE 102462.8438
===> Epoch[24](270/704): Loss_D: 0.1822 Loss_G: 0.6589 MSE 102206.5781
===> Epoch[24](300/704): Loss_D: 0.1701 Loss_G: 0.5071 MSE 100200.5938
===> Epoch[24](330/704): Loss_D: 0.1926 Loss_G: 0.5844 MSE 99439.8750
===> Epoch[24](360/704): Loss_D: 0.2045 Loss_G: 0.7772 MSE 103925.2266
===> Epoch[24](390/704): Loss_D: 0.1007 Loss_G: 0.7160 MSE 104840.6562
===> Epoch[24](420/704): Loss_D: 0.2632 Loss_G: 0.3785 MSE 108324.2969
===> Epoch[24](450/704): Loss_D: 0.1869 Loss_G: 0.6189 MSE 105818.6562
===> Epoch[24](480/704): Loss_D: 0.1644 Loss_G: 0.5310 MSE 98110.2969
===> Epoch[24](510/704): Loss_D: 0.2405 Loss_G: 0.3935 MSE 99239.2578
===> Epoch[24](540/704): Loss_D: 0.1628 Loss_G: 0.5973 MSE 101787.5938
===> Epoch[24](570/704): Loss_D: 0.2101 Loss_G: 0.6676 MSE 99739.2188
===> Epoch[24](600/704): Loss_D: 0.1401 Loss_G: 0.5749 MSE 102364.1094
===> Epoch[24](630/704): Loss_D: 0.0913 Loss_G: 0.8093 MSE 94529.1719
===> Epoch[24](660/704): Loss_D: 0.1969 Loss_G: 0.4495 MSE 94527.1250
===> Epoch[24](690/704): Loss_D: 0.2053 Loss_G: 0.4959 MSE 98103.9375
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[25](0/704): Loss_D: 0.2709 Loss_G: 0.8883 MSE 99721.1328
===> Epoch[25](30/704): Loss_D: 0.1079 Loss_G: 0.6883 MSE 104210.9062
===> Epoch[25](60/704): Loss_D: 0.3190 Loss_G: 0.4777 MSE 101569.5312
===> Epoch[25](90/704): Loss_D: 0.2092 Loss_G: 0.5030 MSE 102085.3438
===> Epoch[25](120/704): Loss_D: 0.2343 Loss_G: 0.3403 MSE 97301.7109
===> Epoch[25](150/704): Loss_D: 0.1919 Loss_G: 0.5176 MSE 98595.7422
===> Epoch[25](180/704): Loss_D: 0.2769 Loss_G: 0.4020 MSE 98381.3516
===> Epoch[25](210/704): Loss_D: 0.1365 Loss_G: 0.7047 MSE 98511.0312
===> Epoch[25](240/704): Loss_D: 0.2466 Loss_G: 0.5953 MSE 101724.9375
===> Epoch[25](270/704): Loss_D: 0.1724 Loss_G: 0.6243 MSE 107232.1484
===> Epoch[25](300/704): Loss_D: 0.2184 Loss_G: 0.4981 MSE 100151.3359
===> Epoch[25](330/704): Loss_D: 0.2152 Loss_G: 0.6479 MSE 96969.9844
===> Epoch[25](360/704): Loss_D: 0.1959 Loss_G: 0.6040 MSE 95098.4062
===> Epoch[25](390/704): Loss_D: 0.1369 Loss_G: 0.5740 MSE 97321.7344
===> Epoch[25](420/704): Loss_D: 0.1858 Loss_G: 0.6136 MSE 98907.4062
===> Epoch[25](450/704): Loss_D: 0.2464 Loss_G: 0.5163 MSE 98775.0312
===> Epoch[25](480/704): Loss_D: 0.2090 Loss_G: 0.4893 MSE 99349.5078
===> Epoch[25](510/704): Loss_D: 0.3022 Loss_G: 0.5354 MSE 101375.8125
===> Epoch[25](540/704): Loss_D: 0.2258 Loss_G: 0.5773 MSE 96254.5625
===> Epoch[25](570/704): Loss_D: 0.1765 Loss_G: 0.5206 MSE 102186.9141
===> Epoch[25](600/704): Loss_D: 0.1536 Loss_G: 0.7033 MSE 99189.5625
===> Epoch[25](630/704): Loss_D: 0.2010 Loss_G: 0.6675 MSE 92354.4297
===> Epoch[25](660/704): Loss_D: 0.2072 Loss_G: 0.6918 MSE 100649.5938
===> Epoch[25](690/704): Loss_D: 0.1081 Loss_G: 0.6755 MSE 95196.5469
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[26](0/704): Loss_D: 0.3685 Loss_G: 0.6440 MSE 94276.3125
===> Epoch[26](30/704): Loss_D: 0.1315 Loss_G: 0.7155 MSE 102140.2500
===> Epoch[26](60/704): Loss_D: 0.2098 Loss_G: 0.4210 MSE 101247.1484
===> Epoch[26](90/704): Loss_D: 0.3234 Loss_G: 0.6030 MSE 98755.7969
===> Epoch[26](120/704): Loss_D: 0.0936 Loss_G: 0.5220 MSE 97335.6562
===> Epoch[26](150/704): Loss_D: 0.2266 Loss_G: 0.6183 MSE 100275.6094
===> Epoch[26](180/704): Loss_D: 0.1864 Loss_G: 0.6411 MSE 102824.1016
===> Epoch[26](210/704): Loss_D: 0.2567 Loss_G: 0.4502 MSE 97095.4531
===> Epoch[26](240/704): Loss_D: 0.2324 Loss_G: 0.5710 MSE 97146.2656
===> Epoch[26](270/704): Loss_D: 0.1018 Loss_G: 0.7374 MSE 97643.0078
===> Epoch[26](300/704): Loss_D: 0.2316 Loss_G: 0.3799 MSE 99328.7812
===> Epoch[26](330/704): Loss_D: 0.2162 Loss_G: 0.5982 MSE 96123.2500
===> Epoch[26](360/704): Loss_D: 0.0862 Loss_G: 0.6746 MSE 98929.4922
===> Epoch[26](390/704): Loss_D: 0.1947 Loss_G: 0.5715 MSE 97803.2656
===> Epoch[26](420/704): Loss_D: 0.1256 Loss_G: 0.6962 MSE 99525.6719
===> Epoch[26](450/704): Loss_D: 0.2543 Loss_G: 0.4409 MSE 97178.7812
===> Epoch[26](480/704): Loss_D: 0.1672 Loss_G: 0.5990 MSE 96977.9531
===> Epoch[26](510/704): Loss_D: 0.2568 Loss_G: 0.5398 MSE 100782.8359
===> Epoch[26](540/704): Loss_D: 0.2091 Loss_G: 0.5138 MSE 94595.0938
===> Epoch[26](570/704): Loss_D: 0.2269 Loss_G: 0.4393 MSE 100850.7812
===> Epoch[26](600/704): Loss_D: 0.2051 Loss_G: 0.4535 MSE 98296.6484
===> Epoch[26](630/704): Loss_D: 0.2065 Loss_G: 0.6370 MSE 98008.9688
===> Epoch[26](660/704): Loss_D: 0.1274 Loss_G: 0.7205 MSE 97089.6016
===> Epoch[26](690/704): Loss_D: 0.1813 Loss_G: 0.5457 MSE 99403.2969
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[27](0/704): Loss_D: 0.2636 Loss_G: 0.7889 MSE 97578.4375
===> Epoch[27](30/704): Loss_D: 0.2352 Loss_G: 0.5362 MSE 95226.0312
===> Epoch[27](60/704): Loss_D: 0.1712 Loss_G: 0.5140 MSE 100110.9219
===> Epoch[27](90/704): Loss_D: 0.2065 Loss_G: 0.5200 MSE 101252.1094
===> Epoch[27](120/704): Loss_D: 0.1470 Loss_G: 0.7270 MSE 97624.1250
===> Epoch[27](150/704): Loss_D: 0.0965 Loss_G: 0.8380 MSE 99328.9922
===> Epoch[27](180/704): Loss_D: 0.1556 Loss_G: 0.7156 MSE 97681.3281
===> Epoch[27](210/704): Loss_D: 0.1348 Loss_G: 0.7245 MSE 96550.4375
===> Epoch[27](240/704): Loss_D: 0.1573 Loss_G: 0.5128 MSE 103808.6797
===> Epoch[27](270/704): Loss_D: 0.1648 Loss_G: 0.8084 MSE 99074.9453
===> Epoch[27](300/704): Loss_D: 0.2057 Loss_G: 0.6356 MSE 95613.5391
===> Epoch[27](330/704): Loss_D: 0.2028 Loss_G: 0.4521 MSE 96819.9062
===> Epoch[27](360/704): Loss_D: 0.1811 Loss_G: 0.6455 MSE 92305.7734
===> Epoch[27](390/704): Loss_D: 0.2214 Loss_G: 0.5061 MSE 97410.4375
===> Epoch[27](420/704): Loss_D: 0.1394 Loss_G: 0.6105 MSE 95519.0625
===> Epoch[27](450/704): Loss_D: 0.1955 Loss_G: 0.5873 MSE 91314.0859
===> Epoch[27](480/704): Loss_D: 0.0980 Loss_G: 0.6952 MSE 97851.8828
===> Epoch[27](510/704): Loss_D: 0.1692 Loss_G: 0.6119 MSE 96663.2188
===> Epoch[27](540/704): Loss_D: 0.2375 Loss_G: 0.5624 MSE 96867.5000
===> Epoch[27](570/704): Loss_D: 0.1874 Loss_G: 0.7599 MSE 94537.5469
===> Epoch[27](600/704): Loss_D: 0.1813 Loss_G: 0.6494 MSE 99005.2812
===> Epoch[27](630/704): Loss_D: 0.0904 Loss_G: 0.7540 MSE 97934.4453
===> Epoch[27](660/704): Loss_D: 0.1801 Loss_G: 0.5928 MSE 94995.8906
===> Epoch[27](690/704): Loss_D: 0.1306 Loss_G: 0.7871 MSE 98486.2578
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[28](0/704): Loss_D: 0.1183 Loss_G: 0.7378 MSE 95441.4922
===> Epoch[28](30/704): Loss_D: 0.2045 Loss_G: 0.5035 MSE 94968.7188
===> Epoch[28](60/704): Loss_D: 0.1654 Loss_G: 0.6372 MSE 93482.5078
===> Epoch[28](90/704): Loss_D: 0.1598 Loss_G: 0.5207 MSE 90941.7500
===> Epoch[28](120/704): Loss_D: 0.1003 Loss_G: 0.7109 MSE 104218.0938
===> Epoch[28](150/704): Loss_D: 0.1188 Loss_G: 0.7161 MSE 98115.4531
===> Epoch[28](180/704): Loss_D: 0.1356 Loss_G: 0.6489 MSE 93598.1797
===> Epoch[28](210/704): Loss_D: 0.2419 Loss_G: 0.4484 MSE 92209.1328
===> Epoch[28](240/704): Loss_D: 0.2088 Loss_G: 0.8246 MSE 92612.5625
===> Epoch[28](270/704): Loss_D: 0.1957 Loss_G: 0.7009 MSE 98236.7969
===> Epoch[28](300/704): Loss_D: 0.1429 Loss_G: 0.6292 MSE 99475.4375
===> Epoch[28](330/704): Loss_D: 0.1671 Loss_G: 0.6565 MSE 104269.7266
===> Epoch[28](360/704): Loss_D: 0.1917 Loss_G: 0.5185 MSE 96023.7109
===> Epoch[28](390/704): Loss_D: 0.1554 Loss_G: 0.7391 MSE 96257.2031
===> Epoch[28](420/704): Loss_D: 0.1389 Loss_G: 0.7106 MSE 97165.4141
===> Epoch[28](450/704): Loss_D: 0.1703 Loss_G: 0.6417 MSE 99980.5234
===> Epoch[28](480/704): Loss_D: 0.1534 Loss_G: 0.6579 MSE 98247.1094
===> Epoch[28](510/704): Loss_D: 0.1919 Loss_G: 0.6010 MSE 93762.8125
===> Epoch[28](540/704): Loss_D: 0.1208 Loss_G: 0.5882 MSE 96539.6641
===> Epoch[28](570/704): Loss_D: 0.1367 Loss_G: 0.6782 MSE 100061.7031
===> Epoch[28](600/704): Loss_D: 0.2381 Loss_G: 0.5214 MSE 94902.3750
===> Epoch[28](630/704): Loss_D: 0.1379 Loss_G: 0.7171 MSE 100195.8594
===> Epoch[28](660/704): Loss_D: 0.2192 Loss_G: 0.6074 MSE 100170.2812
===> Epoch[28](690/704): Loss_D: 0.1495 Loss_G: 0.7559 MSE 94228.2266
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[29](0/704): Loss_D: 0.1047 Loss_G: 1.3330 MSE 100916.6562
===> Epoch[29](30/704): Loss_D: 0.1469 Loss_G: 0.6633 MSE 98858.8359
===> Epoch[29](60/704): Loss_D: 0.2135 Loss_G: 0.7286 MSE 102333.4062
===> Epoch[29](90/704): Loss_D: 0.1159 Loss_G: 0.7062 MSE 99724.8203
===> Epoch[29](120/704): Loss_D: 0.1426 Loss_G: 0.7853 MSE 96169.0000
===> Epoch[29](150/704): Loss_D: 0.2154 Loss_G: 0.5971 MSE 102982.2031
===> Epoch[29](180/704): Loss_D: 0.1755 Loss_G: 0.7155 MSE 101807.7969
===> Epoch[29](210/704): Loss_D: 0.2256 Loss_G: 0.5724 MSE 104289.3516
===> Epoch[29](240/704): Loss_D: 0.1730 Loss_G: 0.5779 MSE 101278.6719
===> Epoch[29](270/704): Loss_D: 0.1763 Loss_G: 0.6544 MSE 96467.3828
===> Epoch[29](300/704): Loss_D: 0.2460 Loss_G: 0.4287 MSE 101649.0312
===> Epoch[29](330/704): Loss_D: 0.1495 Loss_G: 0.7998 MSE 97867.3438
===> Epoch[29](360/704): Loss_D: 0.1257 Loss_G: 0.6479 MSE 98258.8359
===> Epoch[29](390/704): Loss_D: 0.2586 Loss_G: 0.5792 MSE 100686.9844
===> Epoch[29](420/704): Loss_D: 0.1187 Loss_G: 0.7404 MSE 100602.2812
===> Epoch[29](450/704): Loss_D: 0.2541 Loss_G: 0.7174 MSE 99347.2969
===> Epoch[29](480/704): Loss_D: 0.4132 Loss_G: 0.3161 MSE 95910.8672
===> Epoch[29](510/704): Loss_D: 0.2262 Loss_G: 0.6124 MSE 100076.8516
===> Epoch[29](540/704): Loss_D: 0.1505 Loss_G: 0.5639 MSE 98923.1250
===> Epoch[29](570/704): Loss_D: 0.2107 Loss_G: 0.6702 MSE 98314.7812
===> Epoch[29](600/704): Loss_D: 0.1906 Loss_G: 0.6924 MSE 93686.8281
===> Epoch[29](630/704): Loss_D: 0.2517 Loss_G: 0.6105 MSE 99728.7188
===> Epoch[29](660/704): Loss_D: 0.1639 Loss_G: 0.6838 MSE 91705.8750
===> Epoch[29](690/704): Loss_D: 0.1670 Loss_G: 0.4813 MSE 101894.5078
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[30](0/704): Loss_D: 0.4304 Loss_G: 0.5075 MSE 94602.6562
===> Epoch[30](30/704): Loss_D: 0.1110 Loss_G: 0.7108 MSE 94448.6094
===> Epoch[30](60/704): Loss_D: 0.1093 Loss_G: 0.7152 MSE 95342.1641
===> Epoch[30](90/704): Loss_D: 0.1594 Loss_G: 0.5836 MSE 95938.0000
===> Epoch[30](120/704): Loss_D: 0.1968 Loss_G: 0.6514 MSE 104047.8438
===> Epoch[30](150/704): Loss_D: 0.1678 Loss_G: 0.9726 MSE 97021.5312
===> Epoch[30](180/704): Loss_D: 0.1804 Loss_G: 0.5291 MSE 89356.4375
===> Epoch[30](210/704): Loss_D: 0.1716 Loss_G: 0.6228 MSE 103714.4609
===> Epoch[30](240/704): Loss_D: 0.2258 Loss_G: 0.4159 MSE 101438.6641
===> Epoch[30](270/704): Loss_D: 0.1338 Loss_G: 0.8762 MSE 103356.6875
===> Epoch[30](300/704): Loss_D: 0.2114 Loss_G: 0.4934 MSE 97154.1406
===> Epoch[30](330/704): Loss_D: 0.1762 Loss_G: 0.7463 MSE 98852.0000
===> Epoch[30](360/704): Loss_D: 0.1600 Loss_G: 0.7115 MSE 98508.8125
===> Epoch[30](390/704): Loss_D: 0.1301 Loss_G: 0.6431 MSE 98330.0547
===> Epoch[30](420/704): Loss_D: 0.1939 Loss_G: 0.4462 MSE 96773.1719
===> Epoch[30](450/704): Loss_D: 0.1878 Loss_G: 0.7557 MSE 96343.5469
===> Epoch[30](480/704): Loss_D: 0.1811 Loss_G: 0.4038 MSE 106015.5000
===> Epoch[30](510/704): Loss_D: 0.1744 Loss_G: 0.5332 MSE 105207.2500
===> Epoch[30](540/704): Loss_D: 0.1604 Loss_G: 0.6433 MSE 98959.7188
===> Epoch[30](570/704): Loss_D: 0.1611 Loss_G: 0.7339 MSE 104020.2578
===> Epoch[30](600/704): Loss_D: 0.2402 Loss_G: 0.5376 MSE 99837.2266
===> Epoch[30](630/704): Loss_D: 0.1128 Loss_G: 1.1047 MSE 101800.1250
===> Epoch[30](660/704): Loss_D: 0.1425 Loss_G: 0.9968 MSE 95940.1719
===> Epoch[30](690/704): Loss_D: 0.0859 Loss_G: 0.8717 MSE 104717.1719
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[31](0/704): Loss_D: 0.2326 Loss_G: 0.7882 MSE 96703.8438
===> Epoch[31](30/704): Loss_D: 0.2177 Loss_G: 0.5372 MSE 98044.8281
===> Epoch[31](60/704): Loss_D: 0.4168 Loss_G: 0.4389 MSE 102318.3906
===> Epoch[31](90/704): Loss_D: 0.1483 Loss_G: 0.6583 MSE 101631.5859
===> Epoch[31](120/704): Loss_D: 0.1716 Loss_G: 0.5631 MSE 96200.0781
===> Epoch[31](150/704): Loss_D: 0.1436 Loss_G: 0.5636 MSE 104514.6719
===> Epoch[31](180/704): Loss_D: 0.1136 Loss_G: 0.6179 MSE 96455.0000
===> Epoch[31](210/704): Loss_D: 0.1766 Loss_G: 0.5360 MSE 93525.7266
===> Epoch[31](240/704): Loss_D: 0.1820 Loss_G: 0.5134 MSE 94292.2344
===> Epoch[31](270/704): Loss_D: 0.1360 Loss_G: 0.7916 MSE 96344.0312
===> Epoch[31](300/704): Loss_D: 0.1254 Loss_G: 0.6999 MSE 95378.8828
===> Epoch[31](330/704): Loss_D: 0.2286 Loss_G: 0.4300 MSE 99674.1250
===> Epoch[31](360/704): Loss_D: 0.1760 Loss_G: 0.5211 MSE 96600.5312
===> Epoch[31](390/704): Loss_D: 0.1745 Loss_G: 0.5197 MSE 98708.7969
===> Epoch[31](420/704): Loss_D: 0.2027 Loss_G: 0.7858 MSE 94158.1406
===> Epoch[31](450/704): Loss_D: 0.1876 Loss_G: 0.4690 MSE 99803.7656
===> Epoch[31](480/704): Loss_D: 0.2095 Loss_G: 0.4854 MSE 98443.3438
===> Epoch[31](510/704): Loss_D: 0.1461 Loss_G: 0.6391 MSE 95026.0703
===> Epoch[31](540/704): Loss_D: 0.2135 Loss_G: 0.7219 MSE 95913.9688
===> Epoch[31](570/704): Loss_D: 0.2262 Loss_G: 0.4533 MSE 97631.9297
===> Epoch[31](600/704): Loss_D: 0.1980 Loss_G: 0.5957 MSE 99766.2812
===> Epoch[31](630/704): Loss_D: 0.2467 Loss_G: 0.4041 MSE 95691.2656
===> Epoch[31](660/704): Loss_D: 0.2259 Loss_G: 0.9074 MSE 95631.6016
===> Epoch[31](690/704): Loss_D: 0.1488 Loss_G: 0.5316 MSE 100497.8906
learning rate = 0.0010000
learning rate = 0.0010000
Saving
===> Epoch[32](0/704): Loss_D: 0.3847 Loss_G: 0.5397 MSE 96142.8750
===> Epoch[32](30/704): Loss_D: 0.1566 Loss_G: 0.5977 MSE 104844.7188
===> Epoch[32](60/704): Loss_D: 0.1320 Loss_G: 0.8829 MSE 101795.0000
===> Epoch[32](90/704): Loss_D: 0.1345 Loss_G: 0.5606 MSE 101186.7500
===> Epoch[32](120/704): Loss_D: 0.0942 Loss_G: 0.5596 MSE 90160.2266
===> Epoch[32](150/704): Loss_D: 0.1564 Loss_G: 0.6181 MSE 96895.0312
===> Epoch[32](180/704): Loss_D: 0.1269 Loss_G: 0.6319 MSE 99312.0000
===> Epoch[32](210/704): Loss_D: 0.1683 Loss_G: 0.6144 MSE 98259.5625
===> Epoch[32](240/704): Loss_D: 0.1607 Loss_G: 0.5367 MSE 99757.8359
===> Epoch[32](270/704): Loss_D: 0.2355 Loss_G: 0.4911 MSE 99824.5938
===> Epoch[32](300/704): Loss_D: 0.1815 Loss_G: 0.6214 MSE 102565.9453
===> Epoch[32](330/704): Loss_D: 0.1180 Loss_G: 0.7219 MSE 98989.1562
===> Epoch[32](360/704): Loss_D: 0.1751 Loss_G: 0.6640 MSE 96735.0234
===> Epoch[32](390/704): Loss_D: 0.2087 Loss_G: 0.6015 MSE 95054.9453
===> Epoch[32](420/704): Loss_D: 0.1514 Loss_G: 0.7470 MSE 100697.2344
===> Epoch[32](450/704): Loss_D: 0.2274 Loss_G: 0.5020 MSE 96384.5156
===> Epoch[32](480/704): Loss_D: 0.2016 Loss_G: 0.5256 MSE 97553.3750
===> Epoch[32](510/704): Loss_D: 0.1335 Loss_G: 0.6336 MSE 103536.4766
===> Epoch[32](540/704): Loss_D: 0.1454 Loss_G: 0.6798 MSE 101964.6875
===> Epoch[32](570/704): Loss_D: 0.1673 Loss_G: 0.6625 MSE 101471.5156
===> Epoch[32](600/704): Loss_D: 0.1596 Loss_G: 0.3966 MSE 100466.5312
===> Epoch[32](630/704): Loss_D: 0.1769 Loss_G: 0.4875 MSE 100733.3281
===> Epoch[32](660/704): Loss_D: 0.1961 Loss_G: 0.5504 MSE 100526.7188
===> Epoch[32](690/704): Loss_D: 0.1804 Loss_G: 0.5351 MSE 103563.7422
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[33](0/704): Loss_D: 0.4058 Loss_G: 0.4440 MSE 103218.9219
===> Epoch[33](30/704): Loss_D: 0.1891 Loss_G: 0.7350 MSE 105104.4766
===> Epoch[33](60/704): Loss_D: 0.1030 Loss_G: 0.9076 MSE 105273.7188
===> Epoch[33](90/704): Loss_D: 0.1744 Loss_G: 0.6267 MSE 98583.3750
===> Epoch[33](120/704): Loss_D: 0.1789 Loss_G: 0.6661 MSE 104234.3672
===> Epoch[33](150/704): Loss_D: 0.2531 Loss_G: 0.4124 MSE 98157.8750
===> Epoch[33](180/704): Loss_D: 0.2252 Loss_G: 0.5364 MSE 104049.3750
===> Epoch[33](210/704): Loss_D: 0.1143 Loss_G: 0.5745 MSE 97268.3125
===> Epoch[33](240/704): Loss_D: 0.2416 Loss_G: 0.4538 MSE 101853.0391
===> Epoch[33](270/704): Loss_D: 0.2374 Loss_G: 0.3983 MSE 99743.2656
===> Epoch[33](300/704): Loss_D: 0.2368 Loss_G: 0.4013 MSE 100135.3750
===> Epoch[33](330/704): Loss_D: 0.1602 Loss_G: 0.6789 MSE 104854.7656
===> Epoch[33](360/704): Loss_D: 0.2248 Loss_G: 0.4746 MSE 101214.4688
===> Epoch[33](390/704): Loss_D: 0.0999 Loss_G: 0.6280 MSE 96723.9453
===> Epoch[33](420/704): Loss_D: 0.1182 Loss_G: 1.0925 MSE 105966.9844
===> Epoch[33](450/704): Loss_D: 0.2411 Loss_G: 0.4886 MSE 99286.6250
===> Epoch[33](480/704): Loss_D: 0.1416 Loss_G: 0.8230 MSE 100554.3516
===> Epoch[33](510/704): Loss_D: 0.1524 Loss_G: 0.5383 MSE 101612.1562
===> Epoch[33](540/704): Loss_D: 0.2446 Loss_G: 0.6167 MSE 97035.5781
===> Epoch[33](570/704): Loss_D: 0.2592 Loss_G: 0.4890 MSE 98493.1406
===> Epoch[33](600/704): Loss_D: 0.1865 Loss_G: 0.6198 MSE 95946.5469
===> Epoch[33](630/704): Loss_D: 0.1868 Loss_G: 0.5711 MSE 96754.5859
===> Epoch[33](660/704): Loss_D: 0.1843 Loss_G: 0.5410 MSE 98012.8906
===> Epoch[33](690/704): Loss_D: 0.1598 Loss_G: 0.8043 MSE 97004.2969
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[34](0/704): Loss_D: 0.6911 Loss_G: 0.5473 MSE 102687.2969
===> Epoch[34](30/704): Loss_D: 0.2312 Loss_G: 0.4557 MSE 102597.2344
===> Epoch[34](60/704): Loss_D: 0.2158 Loss_G: 0.5998 MSE 102350.6484
===> Epoch[34](90/704): Loss_D: 0.2311 Loss_G: 0.6246 MSE 103070.6953
===> Epoch[34](120/704): Loss_D: 0.2203 Loss_G: 0.6184 MSE 95688.7969
===> Epoch[34](150/704): Loss_D: 0.2293 Loss_G: 0.4149 MSE 106829.1250
===> Epoch[34](180/704): Loss_D: 0.2115 Loss_G: 0.4120 MSE 104712.7891
===> Epoch[34](210/704): Loss_D: 0.2383 Loss_G: 0.4064 MSE 102092.5703
===> Epoch[34](240/704): Loss_D: 0.3366 Loss_G: 0.4935 MSE 98556.3750
===> Epoch[34](270/704): Loss_D: 0.2139 Loss_G: 0.4867 MSE 103836.1641
===> Epoch[34](300/704): Loss_D: 0.1929 Loss_G: 0.5550 MSE 103989.1719
===> Epoch[34](330/704): Loss_D: 0.0657 Loss_G: 0.7999 MSE 99193.6250
===> Epoch[34](360/704): Loss_D: 0.1841 Loss_G: 0.4972 MSE 100800.4922
===> Epoch[34](390/704): Loss_D: 0.1439 Loss_G: 0.5446 MSE 99814.3125
===> Epoch[34](420/704): Loss_D: 0.1129 Loss_G: 0.6241 MSE 100007.3594
===> Epoch[34](450/704): Loss_D: 0.1165 Loss_G: 0.4599 MSE 99608.7344
===> Epoch[34](480/704): Loss_D: 0.1900 Loss_G: 0.5697 MSE 98381.0469
===> Epoch[34](510/704): Loss_D: 0.1755 Loss_G: 0.4556 MSE 98657.7656
===> Epoch[34](540/704): Loss_D: 0.2702 Loss_G: 0.3883 MSE 98508.9219
===> Epoch[34](570/704): Loss_D: 0.1376 Loss_G: 0.5661 MSE 102541.0781
===> Epoch[34](600/704): Loss_D: 0.1961 Loss_G: 0.7625 MSE 100989.1250
===> Epoch[34](630/704): Loss_D: 0.1874 Loss_G: 0.5148 MSE 98282.9219
===> Epoch[34](660/704): Loss_D: 0.1918 Loss_G: 0.5438 MSE 103835.7812
===> Epoch[34](690/704): Loss_D: 0.1850 Loss_G: 0.5958 MSE 107065.7969
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[35](0/704): Loss_D: 0.2676 Loss_G: 0.9376 MSE 103698.0469
===> Epoch[35](30/704): Loss_D: 0.2022 Loss_G: 0.5073 MSE 96255.5469
===> Epoch[35](60/704): Loss_D: 0.2858 Loss_G: 0.5680 MSE 101858.1406
===> Epoch[35](90/704): Loss_D: 0.2395 Loss_G: 0.3899 MSE 99336.6484
===> Epoch[35](120/704): Loss_D: 0.1562 Loss_G: 0.5136 MSE 103511.9219
===> Epoch[35](150/704): Loss_D: 0.2690 Loss_G: 0.3747 MSE 95856.3672
===> Epoch[35](180/704): Loss_D: 0.1889 Loss_G: 0.5474 MSE 95868.4375
===> Epoch[35](210/704): Loss_D: 0.2068 Loss_G: 0.3660 MSE 97447.8125
===> Epoch[35](240/704): Loss_D: 0.0928 Loss_G: 0.5558 MSE 97261.6875
===> Epoch[35](270/704): Loss_D: 0.3116 Loss_G: 0.2813 MSE 95046.6641
===> Epoch[35](300/704): Loss_D: 0.1729 Loss_G: 0.4517 MSE 96512.9219
===> Epoch[35](330/704): Loss_D: 0.2196 Loss_G: 0.6635 MSE 100056.4219
===> Epoch[35](360/704): Loss_D: 0.1936 Loss_G: 0.6095 MSE 104831.9844
===> Epoch[35](390/704): Loss_D: 0.2141 Loss_G: 0.4744 MSE 97529.7891
===> Epoch[35](420/704): Loss_D: 0.2387 Loss_G: 0.3775 MSE 100524.8594
===> Epoch[35](450/704): Loss_D: 0.2450 Loss_G: 0.3852 MSE 94865.6484
===> Epoch[35](480/704): Loss_D: 0.1342 Loss_G: 0.4077 MSE 96838.4844
===> Epoch[35](510/704): Loss_D: 0.2237 Loss_G: 0.3555 MSE 99029.3438
===> Epoch[35](540/704): Loss_D: 0.1918 Loss_G: 0.4904 MSE 102968.9062
===> Epoch[35](570/704): Loss_D: 0.1586 Loss_G: 0.9338 MSE 104959.5859
===> Epoch[35](600/704): Loss_D: 0.1176 Loss_G: 0.6989 MSE 99976.5938
===> Epoch[35](630/704): Loss_D: 0.1299 Loss_G: 0.7653 MSE 96203.8594
===> Epoch[35](660/704): Loss_D: 0.1558 Loss_G: 0.8248 MSE 96089.8984
===> Epoch[35](690/704): Loss_D: 0.1996 Loss_G: 0.4511 MSE 98517.6562
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[36](0/704): Loss_D: 0.3749 Loss_G: 0.4283 MSE 101171.2812
===> Epoch[36](30/704): Loss_D: 0.2049 Loss_G: 0.5916 MSE 95998.4297
===> Epoch[36](60/704): Loss_D: 0.1922 Loss_G: 0.6102 MSE 97713.3984
===> Epoch[36](90/704): Loss_D: 0.2334 Loss_G: 0.4595 MSE 97904.9453
===> Epoch[36](120/704): Loss_D: 0.1920 Loss_G: 0.4626 MSE 96183.6719
===> Epoch[36](150/704): Loss_D: 0.2085 Loss_G: 0.4760 MSE 100321.6562
===> Epoch[36](180/704): Loss_D: 0.2006 Loss_G: 0.4521 MSE 98198.9922
===> Epoch[36](210/704): Loss_D: 0.1709 Loss_G: 0.5869 MSE 98462.0312
===> Epoch[36](240/704): Loss_D: 0.1812 Loss_G: 0.4685 MSE 98252.5156
===> Epoch[36](270/704): Loss_D: 0.2296 Loss_G: 0.5067 MSE 102137.1719
===> Epoch[36](300/704): Loss_D: 0.3811 Loss_G: 0.4468 MSE 103746.8984
===> Epoch[36](330/704): Loss_D: 0.1608 Loss_G: 0.4793 MSE 109229.1562
===> Epoch[36](360/704): Loss_D: 0.1574 Loss_G: 0.5134 MSE 97995.3125
===> Epoch[36](390/704): Loss_D: 0.1918 Loss_G: 0.8266 MSE 94371.5781
===> Epoch[36](420/704): Loss_D: 0.2535 Loss_G: 0.5647 MSE 99203.1562
===> Epoch[36](450/704): Loss_D: 0.1697 Loss_G: 0.4468 MSE 100075.3125
===> Epoch[36](480/704): Loss_D: 0.2412 Loss_G: 0.4007 MSE 96410.4844
===> Epoch[36](510/704): Loss_D: 0.1753 Loss_G: 0.4896 MSE 102741.3281
===> Epoch[36](540/704): Loss_D: 0.1507 Loss_G: 0.6575 MSE 93991.4219
===> Epoch[36](570/704): Loss_D: 0.2009 Loss_G: 0.4809 MSE 95248.0469
===> Epoch[36](600/704): Loss_D: 0.2937 Loss_G: 0.4958 MSE 97098.1641
===> Epoch[36](630/704): Loss_D: 0.1821 Loss_G: 0.4968 MSE 97701.7188
===> Epoch[36](660/704): Loss_D: 0.2027 Loss_G: 0.5680 MSE 100017.8906
===> Epoch[36](690/704): Loss_D: 0.1901 Loss_G: 0.5427 MSE 98575.6172
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[37](0/704): Loss_D: 0.1779 Loss_G: 0.8398 MSE 97246.1094
===> Epoch[37](30/704): Loss_D: 0.2152 Loss_G: 0.5815 MSE 96312.5312
===> Epoch[37](60/704): Loss_D: 0.1224 Loss_G: 0.5720 MSE 99824.8594
===> Epoch[37](90/704): Loss_D: 0.1702 Loss_G: 0.6279 MSE 97315.2734
===> Epoch[37](120/704): Loss_D: 0.2315 Loss_G: 0.4154 MSE 95338.8125
===> Epoch[37](150/704): Loss_D: 0.2235 Loss_G: 0.3222 MSE 97613.3203
===> Epoch[37](180/704): Loss_D: 0.1726 Loss_G: 0.6055 MSE 95656.4688
===> Epoch[37](210/704): Loss_D: 0.1809 Loss_G: 0.4549 MSE 93672.5859
===> Epoch[37](240/704): Loss_D: 0.2074 Loss_G: 0.4492 MSE 95638.1094
===> Epoch[37](270/704): Loss_D: 0.2180 Loss_G: 0.5641 MSE 97015.0312
===> Epoch[37](300/704): Loss_D: 0.2920 Loss_G: 0.4523 MSE 96207.1953
===> Epoch[37](330/704): Loss_D: 0.2143 Loss_G: 0.4570 MSE 97043.7109
===> Epoch[37](360/704): Loss_D: 0.1963 Loss_G: 0.4700 MSE 93970.6719
===> Epoch[37](390/704): Loss_D: 0.2125 Loss_G: 0.3912 MSE 98202.8984
===> Epoch[37](420/704): Loss_D: 0.2462 Loss_G: 0.5886 MSE 98417.6094
===> Epoch[37](450/704): Loss_D: 0.1549 Loss_G: 0.6155 MSE 100150.8281
===> Epoch[37](480/704): Loss_D: 0.1711 Loss_G: 0.4352 MSE 100599.5547
===> Epoch[37](510/704): Loss_D: 0.1779 Loss_G: 0.5238 MSE 98372.6406
===> Epoch[37](540/704): Loss_D: 0.3535 Loss_G: 0.2402 MSE 99667.0312
===> Epoch[37](570/704): Loss_D: 0.1994 Loss_G: 0.4249 MSE 96404.3594
===> Epoch[37](600/704): Loss_D: 0.2239 Loss_G: 0.4522 MSE 98048.8125
===> Epoch[37](630/704): Loss_D: 0.1719 Loss_G: 0.7149 MSE 94860.2969
===> Epoch[37](660/704): Loss_D: 0.1898 Loss_G: 0.5181 MSE 94042.0078
===> Epoch[37](690/704): Loss_D: 0.2571 Loss_G: 0.5191 MSE 95000.0312
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[38](0/704): Loss_D: 0.1978 Loss_G: 0.4700 MSE 101474.3672
===> Epoch[38](30/704): Loss_D: 0.1888 Loss_G: 0.3905 MSE 100017.4531
===> Epoch[38](60/704): Loss_D: 0.2305 Loss_G: 0.4832 MSE 101108.7891
===> Epoch[38](90/704): Loss_D: 0.2405 Loss_G: 0.3460 MSE 99889.4688
===> Epoch[38](120/704): Loss_D: 0.2103 Loss_G: 0.5540 MSE 98891.9688
===> Epoch[38](150/704): Loss_D: 0.2118 Loss_G: 0.4531 MSE 97183.8906
===> Epoch[38](180/704): Loss_D: 0.1510 Loss_G: 0.5661 MSE 98147.9219
===> Epoch[38](210/704): Loss_D: 0.1783 Loss_G: 0.4573 MSE 97761.0078
===> Epoch[38](240/704): Loss_D: 0.2159 Loss_G: 0.4983 MSE 93782.3125
===> Epoch[38](270/704): Loss_D: 0.1653 Loss_G: 0.5210 MSE 96424.6094
===> Epoch[38](300/704): Loss_D: 0.2517 Loss_G: 0.3829 MSE 95883.1562
===> Epoch[38](330/704): Loss_D: 0.2176 Loss_G: 0.5314 MSE 96628.1719
===> Epoch[38](360/704): Loss_D: 0.1863 Loss_G: 0.5582 MSE 93203.4141
===> Epoch[38](390/704): Loss_D: 0.1297 Loss_G: 0.7310 MSE 97596.6562
===> Epoch[38](420/704): Loss_D: 0.2048 Loss_G: 0.5262 MSE 95300.3125
===> Epoch[38](450/704): Loss_D: 0.2134 Loss_G: 0.5113 MSE 98501.3047
===> Epoch[38](480/704): Loss_D: 0.1225 Loss_G: 0.6312 MSE 99752.6719
===> Epoch[38](510/704): Loss_D: 0.1922 Loss_G: 0.4821 MSE 105564.5078
===> Epoch[38](540/704): Loss_D: 0.2133 Loss_G: 0.4961 MSE 99025.3906
===> Epoch[38](570/704): Loss_D: 0.1860 Loss_G: 0.6331 MSE 95219.1797
===> Epoch[38](600/704): Loss_D: 0.2525 Loss_G: 0.5588 MSE 94382.6016
===> Epoch[38](630/704): Loss_D: 0.1988 Loss_G: 0.4867 MSE 93676.5469
===> Epoch[38](660/704): Loss_D: 0.1970 Loss_G: 0.5821 MSE 94372.0156
===> Epoch[38](690/704): Loss_D: 0.1569 Loss_G: 0.6851 MSE 97753.8125
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[39](0/704): Loss_D: 0.5768 Loss_G: 0.3355 MSE 99719.8750
===> Epoch[39](30/704): Loss_D: 0.1808 Loss_G: 0.4794 MSE 101058.6797
===> Epoch[39](60/704): Loss_D: 0.1408 Loss_G: 0.5818 MSE 104135.2500
===> Epoch[39](90/704): Loss_D: 0.2342 Loss_G: 0.4626 MSE 104161.1562
===> Epoch[39](120/704): Loss_D: 0.2377 Loss_G: 0.4234 MSE 101542.0781
===> Epoch[39](150/704): Loss_D: 0.1523 Loss_G: 0.4793 MSE 103491.9844
===> Epoch[39](180/704): Loss_D: 0.1407 Loss_G: 0.5698 MSE 104980.5000
===> Epoch[39](210/704): Loss_D: 0.2308 Loss_G: 0.5542 MSE 96592.6562
===> Epoch[39](240/704): Loss_D: 0.1437 Loss_G: 0.6524 MSE 96597.6094
===> Epoch[39](270/704): Loss_D: 0.1354 Loss_G: 0.6805 MSE 97206.3906
===> Epoch[39](300/704): Loss_D: 0.1527 Loss_G: 0.5500 MSE 101191.6719
===> Epoch[39](330/704): Loss_D: 0.1902 Loss_G: 0.5949 MSE 95858.8594
===> Epoch[39](360/704): Loss_D: 0.1893 Loss_G: 0.6156 MSE 97481.7500
===> Epoch[39](390/704): Loss_D: 0.1760 Loss_G: 0.5766 MSE 100033.2031
===> Epoch[39](420/704): Loss_D: 0.1718 Loss_G: 0.6440 MSE 105908.6406
===> Epoch[39](450/704): Loss_D: 0.2277 Loss_G: 0.4543 MSE 101152.2188
===> Epoch[39](480/704): Loss_D: 0.1478 Loss_G: 0.7606 MSE 100847.9609
===> Epoch[39](510/704): Loss_D: 0.1998 Loss_G: 0.5185 MSE 95349.1797
===> Epoch[39](540/704): Loss_D: 0.1775 Loss_G: 0.4962 MSE 100814.7969
===> Epoch[39](570/704): Loss_D: 0.1993 Loss_G: 0.5129 MSE 98617.9375
===> Epoch[39](600/704): Loss_D: 0.1618 Loss_G: 0.5520 MSE 98903.4609
===> Epoch[39](630/704): Loss_D: 0.2677 Loss_G: 0.5258 MSE 93251.3281
===> Epoch[39](660/704): Loss_D: 0.1960 Loss_G: 0.4806 MSE 98286.1250
===> Epoch[39](690/704): Loss_D: 0.1574 Loss_G: 0.7207 MSE 101256.2031
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[40](0/704): Loss_D: 0.2805 Loss_G: 0.4682 MSE 97936.3906
===> Epoch[40](30/704): Loss_D: 0.1319 Loss_G: 0.7479 MSE 99162.1328
===> Epoch[40](60/704): Loss_D: 0.2110 Loss_G: 0.4284 MSE 99105.1172
===> Epoch[40](90/704): Loss_D: 0.2240 Loss_G: 0.4244 MSE 99852.7656
===> Epoch[40](120/704): Loss_D: 0.2051 Loss_G: 0.4571 MSE 105031.7031
===> Epoch[40](150/704): Loss_D: 0.1883 Loss_G: 0.5762 MSE 100848.3750
===> Epoch[40](180/704): Loss_D: 0.1975 Loss_G: 0.4738 MSE 102214.9219
===> Epoch[40](210/704): Loss_D: 0.1790 Loss_G: 0.4991 MSE 103783.9609
===> Epoch[40](240/704): Loss_D: 0.1411 Loss_G: 0.6522 MSE 105640.3906
===> Epoch[40](270/704): Loss_D: 0.2158 Loss_G: 0.4811 MSE 104756.3672
===> Epoch[40](300/704): Loss_D: 0.3059 Loss_G: 0.4848 MSE 102342.5625
===> Epoch[40](330/704): Loss_D: 0.2356 Loss_G: 0.3890 MSE 103775.6719
===> Epoch[40](360/704): Loss_D: 0.1160 Loss_G: 0.6147 MSE 96660.2734
===> Epoch[40](390/704): Loss_D: 0.1521 Loss_G: 0.5359 MSE 101436.2656
===> Epoch[40](420/704): Loss_D: 0.2088 Loss_G: 0.4267 MSE 98763.2812
===> Epoch[40](450/704): Loss_D: 0.2480 Loss_G: 0.3948 MSE 96575.2188
===> Epoch[40](480/704): Loss_D: 0.2048 Loss_G: 0.5360 MSE 90911.2344
===> Epoch[40](510/704): Loss_D: 0.2381 Loss_G: 0.4654 MSE 94369.6875
===> Epoch[40](540/704): Loss_D: 0.2450 Loss_G: 0.4065 MSE 98042.9531
===> Epoch[40](570/704): Loss_D: 0.1965 Loss_G: 0.4394 MSE 94717.3828
===> Epoch[40](600/704): Loss_D: 0.2569 Loss_G: 0.3321 MSE 100207.1719
===> Epoch[40](630/704): Loss_D: 0.2138 Loss_G: 0.4733 MSE 102373.8594
===> Epoch[40](660/704): Loss_D: 0.1494 Loss_G: 0.6581 MSE 100760.1875
===> Epoch[40](690/704): Loss_D: 0.2092 Loss_G: 0.4725 MSE 96277.5000
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[41](0/704): Loss_D: 0.2913 Loss_G: 0.8274 MSE 101916.5156
===> Epoch[41](30/704): Loss_D: 0.2820 Loss_G: 0.5621 MSE 100938.1719
===> Epoch[41](60/704): Loss_D: 0.1997 Loss_G: 0.4253 MSE 99253.5938
===> Epoch[41](90/704): Loss_D: 0.2092 Loss_G: 0.5690 MSE 102231.7344
===> Epoch[41](120/704): Loss_D: 0.2001 Loss_G: 0.4446 MSE 96855.7422
===> Epoch[41](150/704): Loss_D: 0.1642 Loss_G: 0.6883 MSE 93214.8125
===> Epoch[41](180/704): Loss_D: 0.1828 Loss_G: 0.4813 MSE 92167.3906
===> Epoch[41](210/704): Loss_D: 0.1137 Loss_G: 0.5512 MSE 100190.7891
===> Epoch[41](240/704): Loss_D: 0.2494 Loss_G: 0.4603 MSE 101108.7734
===> Epoch[41](270/704): Loss_D: 0.2272 Loss_G: 0.3824 MSE 95791.8281
===> Epoch[41](300/704): Loss_D: 0.2116 Loss_G: 0.5580 MSE 99215.8906
===> Epoch[41](330/704): Loss_D: 0.2329 Loss_G: 0.4680 MSE 95747.2656
===> Epoch[41](360/704): Loss_D: 0.1586 Loss_G: 0.7262 MSE 99558.7969
===> Epoch[41](390/704): Loss_D: 0.1869 Loss_G: 0.5966 MSE 96462.8359
===> Epoch[41](420/704): Loss_D: 0.2214 Loss_G: 0.4228 MSE 98011.6484
===> Epoch[41](450/704): Loss_D: 0.1780 Loss_G: 0.4555 MSE 99198.2656
===> Epoch[41](480/704): Loss_D: 0.1624 Loss_G: 0.5704 MSE 104891.8047
===> Epoch[41](510/704): Loss_D: 0.1228 Loss_G: 0.6260 MSE 98954.8906
===> Epoch[41](540/704): Loss_D: 0.1814 Loss_G: 0.5283 MSE 100788.0391
===> Epoch[41](570/704): Loss_D: 0.1195 Loss_G: 0.6844 MSE 100291.4531
===> Epoch[41](600/704): Loss_D: 0.1909 Loss_G: 0.5650 MSE 99041.0312
===> Epoch[41](630/704): Loss_D: 0.1725 Loss_G: 0.6105 MSE 104508.6094
===> Epoch[41](660/704): Loss_D: 0.1950 Loss_G: 0.4848 MSE 96785.9141
===> Epoch[41](690/704): Loss_D: 0.1507 Loss_G: 0.5578 MSE 94561.4609
learning rate = 0.0010000
learning rate = 0.0010000
Saving
===> Epoch[42](0/704): Loss_D: 0.2271 Loss_G: 0.7000 MSE 94515.7969
===> Epoch[42](30/704): Loss_D: 0.2517 Loss_G: 0.3514 MSE 95521.6250
===> Epoch[42](60/704): Loss_D: 0.2229 Loss_G: 0.3591 MSE 96695.9219
===> Epoch[42](90/704): Loss_D: 0.1675 Loss_G: 0.6549 MSE 99647.4844
===> Epoch[42](120/704): Loss_D: 0.1731 Loss_G: 0.4629 MSE 108569.1094
===> Epoch[42](150/704): Loss_D: 0.1904 Loss_G: 0.4901 MSE 100287.7344
===> Epoch[42](180/704): Loss_D: 0.1435 Loss_G: 0.6343 MSE 103460.7656
===> Epoch[42](210/704): Loss_D: 0.1536 Loss_G: 0.5770 MSE 100348.4688
===> Epoch[42](240/704): Loss_D: 0.1822 Loss_G: 0.4805 MSE 97581.5312
===> Epoch[42](270/704): Loss_D: 0.2224 Loss_G: 0.5363 MSE 102029.2422
===> Epoch[42](300/704): Loss_D: 0.2368 Loss_G: 0.4942 MSE 94521.0547
===> Epoch[42](330/704): Loss_D: 0.2106 Loss_G: 0.4503 MSE 92667.6406
===> Epoch[42](360/704): Loss_D: 0.2318 Loss_G: 0.4930 MSE 100108.1250
===> Epoch[42](390/704): Loss_D: 0.2216 Loss_G: 0.3446 MSE 93324.6953
===> Epoch[42](420/704): Loss_D: 0.1676 Loss_G: 0.4673 MSE 95633.5391
===> Epoch[42](450/704): Loss_D: 0.1774 Loss_G: 0.4711 MSE 97347.6406
===> Epoch[42](480/704): Loss_D: 0.1688 Loss_G: 0.5816 MSE 94689.0781
===> Epoch[42](510/704): Loss_D: 0.1583 Loss_G: 0.5805 MSE 96175.7969
===> Epoch[42](540/704): Loss_D: 0.2289 Loss_G: 0.4054 MSE 95264.8750
===> Epoch[42](570/704): Loss_D: 0.2007 Loss_G: 0.7746 MSE 92640.8281
===> Epoch[42](600/704): Loss_D: 0.1236 Loss_G: 0.6663 MSE 91660.5625
===> Epoch[42](630/704): Loss_D: 0.1865 Loss_G: 0.4584 MSE 95053.0938
===> Epoch[42](660/704): Loss_D: 0.1700 Loss_G: 0.5932 MSE 100746.8672
===> Epoch[42](690/704): Loss_D: 0.1543 Loss_G: 0.5806 MSE 92405.3906
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[43](0/704): Loss_D: 0.3088 Loss_G: 0.5767 MSE 94195.5938
===> Epoch[43](30/704): Loss_D: 0.1855 Loss_G: 0.4693 MSE 96688.7109
===> Epoch[43](60/704): Loss_D: 0.2536 Loss_G: 0.4054 MSE 94812.8125
===> Epoch[43](90/704): Loss_D: 0.1693 Loss_G: 0.5739 MSE 93739.9297
===> Epoch[43](120/704): Loss_D: 0.2075 Loss_G: 0.5485 MSE 96468.1016
===> Epoch[43](150/704): Loss_D: 0.0996 Loss_G: 0.8789 MSE 99957.6562
===> Epoch[43](180/704): Loss_D: 0.0760 Loss_G: 0.7221 MSE 93328.2500
===> Epoch[43](210/704): Loss_D: 0.1443 Loss_G: 0.6139 MSE 100267.6172
===> Epoch[43](240/704): Loss_D: 0.1502 Loss_G: 0.5621 MSE 97198.5859
===> Epoch[43](270/704): Loss_D: 0.1314 Loss_G: 0.5703 MSE 100444.5781
===> Epoch[43](300/704): Loss_D: 0.2394 Loss_G: 0.4055 MSE 101640.4062
===> Epoch[43](330/704): Loss_D: 0.1790 Loss_G: 0.4168 MSE 98940.0078
===> Epoch[43](360/704): Loss_D: 0.2427 Loss_G: 0.4417 MSE 99425.2344
===> Epoch[43](390/704): Loss_D: 0.1690 Loss_G: 0.6828 MSE 103026.1875
===> Epoch[43](420/704): Loss_D: 0.2028 Loss_G: 0.4374 MSE 97579.5156
===> Epoch[43](450/704): Loss_D: 0.2101 Loss_G: 0.4417 MSE 95858.2891
===> Epoch[43](480/704): Loss_D: 0.2131 Loss_G: 0.4042 MSE 99186.3125
===> Epoch[43](510/704): Loss_D: 0.2289 Loss_G: 0.4060 MSE 99595.0469
===> Epoch[43](540/704): Loss_D: 0.1777 Loss_G: 0.4657 MSE 92749.8125
===> Epoch[43](570/704): Loss_D: 0.1724 Loss_G: 0.4954 MSE 95498.6094
===> Epoch[43](600/704): Loss_D: 0.2662 Loss_G: 0.4955 MSE 95072.7812
===> Epoch[43](630/704): Loss_D: 0.1669 Loss_G: 0.4731 MSE 100038.3516
===> Epoch[43](660/704): Loss_D: 0.1859 Loss_G: 0.6285 MSE 98368.4766
===> Epoch[43](690/704): Loss_D: 0.2018 Loss_G: 0.4841 MSE 98389.3125
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[44](0/704): Loss_D: 0.2745 Loss_G: 0.6394 MSE 101134.3906
===> Epoch[44](30/704): Loss_D: 0.1390 Loss_G: 0.7401 MSE 101812.8984
===> Epoch[44](60/704): Loss_D: 0.1729 Loss_G: 0.6139 MSE 103804.8281
===> Epoch[44](90/704): Loss_D: 0.1968 Loss_G: 0.4740 MSE 100565.8906
===> Epoch[44](120/704): Loss_D: 0.2144 Loss_G: 0.4519 MSE 102577.5938
===> Epoch[44](150/704): Loss_D: 0.1449 Loss_G: 0.6902 MSE 100128.6094
===> Epoch[44](180/704): Loss_D: 0.1603 Loss_G: 0.6203 MSE 97421.3125
===> Epoch[44](210/704): Loss_D: 0.1831 Loss_G: 0.4917 MSE 94244.9141
===> Epoch[44](240/704): Loss_D: 0.1676 Loss_G: 0.6151 MSE 95099.2812
===> Epoch[44](270/704): Loss_D: 0.1659 Loss_G: 0.5190 MSE 99065.8281
===> Epoch[44](300/704): Loss_D: 0.1790 Loss_G: 0.5714 MSE 92995.1562
===> Epoch[44](330/704): Loss_D: 0.1225 Loss_G: 0.5307 MSE 100830.6797
===> Epoch[44](360/704): Loss_D: 0.1634 Loss_G: 0.6725 MSE 99253.8047
===> Epoch[44](390/704): Loss_D: 0.1752 Loss_G: 0.5197 MSE 98383.4531
===> Epoch[44](420/704): Loss_D: 0.1637 Loss_G: 0.5736 MSE 96890.2031
===> Epoch[44](450/704): Loss_D: 0.1668 Loss_G: 0.6955 MSE 99010.8047
===> Epoch[44](480/704): Loss_D: 0.1833 Loss_G: 0.4913 MSE 100060.5469
===> Epoch[44](510/704): Loss_D: 0.1698 Loss_G: 0.5688 MSE 97576.1016
===> Epoch[44](540/704): Loss_D: 0.1912 Loss_G: 0.5239 MSE 100468.6562
===> Epoch[44](570/704): Loss_D: 0.1056 Loss_G: 0.6181 MSE 99437.7969
===> Epoch[44](600/704): Loss_D: 0.1665 Loss_G: 0.5766 MSE 101754.5234
===> Epoch[44](630/704): Loss_D: 0.1662 Loss_G: 0.5840 MSE 96399.7891
===> Epoch[44](660/704): Loss_D: 0.1686 Loss_G: 0.5203 MSE 100943.6562
===> Epoch[44](690/704): Loss_D: 0.1699 Loss_G: 0.5203 MSE 98516.7344
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[45](0/704): Loss_D: 0.3860 Loss_G: 0.4952 MSE 94083.6562
===> Epoch[45](30/704): Loss_D: 0.2059 Loss_G: 0.5112 MSE 100051.4297
===> Epoch[45](60/704): Loss_D: 0.1815 Loss_G: 0.5592 MSE 98005.8672
===> Epoch[45](90/704): Loss_D: 0.2430 Loss_G: 0.5429 MSE 103763.7656
===> Epoch[45](120/704): Loss_D: 0.1201 Loss_G: 0.7280 MSE 103334.9062
===> Epoch[45](150/704): Loss_D: 0.2267 Loss_G: 0.5041 MSE 99345.7500
===> Epoch[45](180/704): Loss_D: 0.2218 Loss_G: 0.4351 MSE 93512.8750
===> Epoch[45](210/704): Loss_D: 0.1973 Loss_G: 0.4878 MSE 98869.5000
===> Epoch[45](240/704): Loss_D: 0.1860 Loss_G: 0.4461 MSE 104774.7031
===> Epoch[45](270/704): Loss_D: 0.2360 Loss_G: 0.3847 MSE 93027.4688
===> Epoch[45](300/704): Loss_D: 0.2305 Loss_G: 0.4458 MSE 99596.0625
===> Epoch[45](330/704): Loss_D: 0.1689 Loss_G: 0.5048 MSE 101655.2578
===> Epoch[45](360/704): Loss_D: 0.1770 Loss_G: 0.4956 MSE 96742.7188
===> Epoch[45](390/704): Loss_D: 0.1461 Loss_G: 0.5472 MSE 95967.9766
===> Epoch[45](420/704): Loss_D: 0.1380 Loss_G: 0.5068 MSE 92488.4531
===> Epoch[45](450/704): Loss_D: 0.1412 Loss_G: 0.7985 MSE 100308.9922
===> Epoch[45](480/704): Loss_D: 0.2240 Loss_G: 0.4364 MSE 97850.0391
===> Epoch[45](510/704): Loss_D: 0.2211 Loss_G: 0.4543 MSE 101449.1406
===> Epoch[45](540/704): Loss_D: 0.1870 Loss_G: 0.4908 MSE 104156.8984
===> Epoch[45](570/704): Loss_D: 0.2718 Loss_G: 0.4225 MSE 102248.0625
===> Epoch[45](600/704): Loss_D: 0.1762 Loss_G: 0.6830 MSE 101459.8672
===> Epoch[45](630/704): Loss_D: 0.1297 Loss_G: 0.6551 MSE 100843.9688
===> Epoch[45](660/704): Loss_D: 0.2002 Loss_G: 0.4044 MSE 102214.4922
===> Epoch[45](690/704): Loss_D: 0.2797 Loss_G: 0.3783 MSE 100093.7734
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[46](0/704): Loss_D: 0.6500 Loss_G: 0.5048 MSE 98899.4062
===> Epoch[46](30/704): Loss_D: 0.2881 Loss_G: 0.6084 MSE 100637.5859
===> Epoch[46](60/704): Loss_D: 0.1542 Loss_G: 0.5790 MSE 94384.1484
===> Epoch[46](90/704): Loss_D: 0.1923 Loss_G: 0.6290 MSE 98320.7812
===> Epoch[46](120/704): Loss_D: 0.2267 Loss_G: 0.4538 MSE 102827.3906
===> Epoch[46](150/704): Loss_D: 0.2446 Loss_G: 0.4579 MSE 103610.7578
===> Epoch[46](180/704): Loss_D: 0.2008 Loss_G: 0.4604 MSE 101492.5000
===> Epoch[46](210/704): Loss_D: 0.1490 Loss_G: 0.6148 MSE 100596.5625
===> Epoch[46](240/704): Loss_D: 0.1267 Loss_G: 0.7724 MSE 101062.6484
===> Epoch[46](270/704): Loss_D: 0.1864 Loss_G: 0.6220 MSE 94861.6406
===> Epoch[46](300/704): Loss_D: 0.2404 Loss_G: 0.5012 MSE 100415.3906
===> Epoch[46](330/704): Loss_D: 0.2013 Loss_G: 0.5824 MSE 94954.2656
===> Epoch[46](360/704): Loss_D: 0.3363 Loss_G: 0.4040 MSE 95146.9453
===> Epoch[46](390/704): Loss_D: 0.0870 Loss_G: 0.8087 MSE 94937.0625
===> Epoch[46](420/704): Loss_D: 0.1881 Loss_G: 0.4996 MSE 98401.8281
===> Epoch[46](450/704): Loss_D: 0.2285 Loss_G: 0.5203 MSE 98300.9531
===> Epoch[46](480/704): Loss_D: 0.1710 Loss_G: 0.5528 MSE 93682.1719
===> Epoch[46](510/704): Loss_D: 0.1442 Loss_G: 0.6548 MSE 101574.9922
===> Epoch[46](540/704): Loss_D: 0.1415 Loss_G: 0.5819 MSE 98428.1719
===> Epoch[46](570/704): Loss_D: 0.0937 Loss_G: 0.7315 MSE 99928.5391
===> Epoch[46](600/704): Loss_D: 0.1799 Loss_G: 0.5711 MSE 104049.1953
===> Epoch[46](630/704): Loss_D: 0.1954 Loss_G: 0.5276 MSE 95406.9453
===> Epoch[46](660/704): Loss_D: 0.1558 Loss_G: 0.5262 MSE 99561.1484
===> Epoch[46](690/704): Loss_D: 0.2244 Loss_G: 0.4940 MSE 94768.6562
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[47](0/704): Loss_D: 0.3744 Loss_G: 0.7821 MSE 97553.7266
===> Epoch[47](30/704): Loss_D: 0.2090 Loss_G: 0.5774 MSE 98830.6250
===> Epoch[47](60/704): Loss_D: 0.1923 Loss_G: 0.5957 MSE 96386.6094
===> Epoch[47](90/704): Loss_D: 0.2325 Loss_G: 0.4841 MSE 99121.9297
===> Epoch[47](120/704): Loss_D: 0.1876 Loss_G: 0.5614 MSE 98459.3984
===> Epoch[47](150/704): Loss_D: 0.1640 Loss_G: 0.5888 MSE 96867.2031
===> Epoch[47](180/704): Loss_D: 0.1139 Loss_G: 0.5579 MSE 94932.6250
===> Epoch[47](210/704): Loss_D: 0.1925 Loss_G: 0.4068 MSE 96615.5156
===> Epoch[47](240/704): Loss_D: 0.1592 Loss_G: 0.5343 MSE 94817.9219
===> Epoch[47](270/704): Loss_D: 0.1933 Loss_G: 0.5662 MSE 100314.3438
===> Epoch[47](300/704): Loss_D: 0.1307 Loss_G: 0.6114 MSE 95906.9531
===> Epoch[47](330/704): Loss_D: 0.1705 Loss_G: 0.6319 MSE 96524.9531
===> Epoch[47](360/704): Loss_D: 0.1467 Loss_G: 0.4978 MSE 106842.7188
===> Epoch[47](390/704): Loss_D: 0.1734 Loss_G: 0.5298 MSE 100222.2969
===> Epoch[47](420/704): Loss_D: 0.1855 Loss_G: 0.8301 MSE 94974.0000
===> Epoch[47](450/704): Loss_D: 0.2364 Loss_G: 0.5226 MSE 94562.1719
===> Epoch[47](480/704): Loss_D: 0.1732 Loss_G: 0.6162 MSE 98369.0312
===> Epoch[47](510/704): Loss_D: 0.1961 Loss_G: 0.4923 MSE 104731.7031
===> Epoch[47](540/704): Loss_D: 0.1519 Loss_G: 0.5670 MSE 101892.0312
===> Epoch[47](570/704): Loss_D: 0.1680 Loss_G: 0.6097 MSE 106127.6719
===> Epoch[47](600/704): Loss_D: 0.1723 Loss_G: 0.5579 MSE 106361.9297
===> Epoch[47](630/704): Loss_D: 0.2053 Loss_G: 0.4035 MSE 106796.5625
===> Epoch[47](660/704): Loss_D: 0.1568 Loss_G: 0.6361 MSE 101314.4922
===> Epoch[47](690/704): Loss_D: 0.1478 Loss_G: 0.7200 MSE 97866.1094
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[48](0/704): Loss_D: 0.3627 Loss_G: 0.6404 MSE 96780.2500
===> Epoch[48](30/704): Loss_D: 0.0878 Loss_G: 0.6555 MSE 96572.5469
===> Epoch[48](60/704): Loss_D: 0.1669 Loss_G: 0.6725 MSE 102054.7188
===> Epoch[48](90/704): Loss_D: 0.1718 Loss_G: 0.5128 MSE 97692.9922
===> Epoch[48](120/704): Loss_D: 0.1986 Loss_G: 0.6914 MSE 98712.0312
===> Epoch[48](150/704): Loss_D: 0.1558 Loss_G: 0.5776 MSE 101374.9219
===> Epoch[48](180/704): Loss_D: 0.1872 Loss_G: 0.6633 MSE 104002.5781
===> Epoch[48](210/704): Loss_D: 0.2049 Loss_G: 0.7255 MSE 99921.1719
===> Epoch[48](240/704): Loss_D: 0.1282 Loss_G: 0.7023 MSE 104560.0625
===> Epoch[48](270/704): Loss_D: 0.2133 Loss_G: 0.4009 MSE 105518.4844
===> Epoch[48](300/704): Loss_D: 0.1622 Loss_G: 0.5090 MSE 102198.9375
===> Epoch[48](330/704): Loss_D: 0.2239 Loss_G: 0.3605 MSE 98780.7891
===> Epoch[48](360/704): Loss_D: 0.1462 Loss_G: 0.7200 MSE 97296.7812
===> Epoch[48](390/704): Loss_D: 0.2043 Loss_G: 0.5257 MSE 99540.3125
===> Epoch[48](420/704): Loss_D: 0.1413 Loss_G: 0.6723 MSE 96980.2812
===> Epoch[48](450/704): Loss_D: 0.1293 Loss_G: 0.6773 MSE 97019.5781
===> Epoch[48](480/704): Loss_D: 0.2384 Loss_G: 0.4835 MSE 98035.7344
===> Epoch[48](510/704): Loss_D: 0.1576 Loss_G: 0.6690 MSE 94961.1719
===> Epoch[48](540/704): Loss_D: 0.1647 Loss_G: 0.5831 MSE 94391.5000
===> Epoch[48](570/704): Loss_D: 0.1542 Loss_G: 0.5484 MSE 96187.8203
===> Epoch[48](600/704): Loss_D: 0.1871 Loss_G: 0.5519 MSE 100000.4844
===> Epoch[48](630/704): Loss_D: 0.2297 Loss_G: 0.4373 MSE 95659.0391
===> Epoch[48](660/704): Loss_D: 0.2154 Loss_G: 0.3923 MSE 97657.0156
===> Epoch[48](690/704): Loss_D: 0.0782 Loss_G: 0.6215 MSE 96384.5312
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[49](0/704): Loss_D: 0.2291 Loss_G: 0.6524 MSE 102027.1406
===> Epoch[49](30/704): Loss_D: 0.1714 Loss_G: 0.9069 MSE 100691.1250
===> Epoch[49](60/704): Loss_D: 0.2005 Loss_G: 0.4745 MSE 101533.5312
===> Epoch[49](90/704): Loss_D: 0.1772 Loss_G: 0.6207 MSE 99143.4844
===> Epoch[49](120/704): Loss_D: 0.1321 Loss_G: 0.7057 MSE 101596.7109
===> Epoch[49](150/704): Loss_D: 0.2231 Loss_G: 0.5089 MSE 101448.5312
===> Epoch[49](180/704): Loss_D: 0.1685 Loss_G: 0.5458 MSE 102780.7266
===> Epoch[49](210/704): Loss_D: 0.1629 Loss_G: 0.6506 MSE 101885.7188
===> Epoch[49](240/704): Loss_D: 0.2294 Loss_G: 0.8314 MSE 97080.2422
===> Epoch[49](270/704): Loss_D: 0.3211 Loss_G: 0.5584 MSE 99064.7344
===> Epoch[49](300/704): Loss_D: 0.2019 Loss_G: 0.5750 MSE 99174.7344
===> Epoch[49](330/704): Loss_D: 0.1630 Loss_G: 0.6122 MSE 96686.1641
===> Epoch[49](360/704): Loss_D: 0.2600 Loss_G: 0.2875 MSE 99861.2188
===> Epoch[49](390/704): Loss_D: 0.1170 Loss_G: 0.6130 MSE 98925.5938
===> Epoch[49](420/704): Loss_D: 0.1109 Loss_G: 0.6490 MSE 99836.8672
===> Epoch[49](450/704): Loss_D: 0.0751 Loss_G: 0.6544 MSE 97129.1250
===> Epoch[49](480/704): Loss_D: 0.2437 Loss_G: 0.4040 MSE 93835.7109
===> Epoch[49](510/704): Loss_D: 0.2250 Loss_G: 0.5128 MSE 94320.0156
===> Epoch[49](540/704): Loss_D: 0.2242 Loss_G: 0.4695 MSE 95162.6406
===> Epoch[49](570/704): Loss_D: 0.1658 Loss_G: 0.5292 MSE 95107.7969
===> Epoch[49](600/704): Loss_D: 0.1961 Loss_G: 0.4584 MSE 95385.1484
===> Epoch[49](630/704): Loss_D: 0.1748 Loss_G: 0.5658 MSE 95309.5469
===> Epoch[49](660/704): Loss_D: 0.2023 Loss_G: 0.5570 MSE 96574.7500
===> Epoch[49](690/704): Loss_D: 0.1510 Loss_G: 0.6214 MSE 101524.1562
learning rate = 0.0010000
learning rate = 0.0010000
===> Epoch[50](0/704): Loss_D: 0.3161 Loss_G: 0.5938 MSE 104710.0625
===> Epoch[50](30/704): Loss_D: 0.1847 Loss_G: 0.6098 MSE 99724.6719
===> Epoch[50](60/704): Loss_D: 0.1244 Loss_G: 0.5936 MSE 102283.3359
===> Epoch[50](90/704): Loss_D: 0.1680 Loss_G: 0.4136 MSE 100294.6406
===> Epoch[50](120/704): Loss_D: 0.1506 Loss_G: 0.6647 MSE 101925.0156
===> Epoch[50](150/704): Loss_D: 0.1474 Loss_G: 0.7791 MSE 99480.8125
===> Epoch[50](180/704): Loss_D: 0.1862 Loss_G: 0.4433 MSE 101976.8359
===> Epoch[50](210/704): Loss_D: 0.1283 Loss_G: 0.6966 MSE 98114.4922
===> Epoch[50](240/704): Loss_D: 0.1214 Loss_G: 0.5437 MSE 98865.4375
===> Epoch[50](270/704): Loss_D: 0.1724 Loss_G: 0.5297 MSE 96666.7656
===> Epoch[50](300/704): Loss_D: 0.2186 Loss_G: 0.5259 MSE 97516.2500
===> Epoch[50](330/704): Loss_D: 0.1236 Loss_G: 0.6834 MSE 100341.5000
===> Epoch[50](360/704): Loss_D: 0.1850 Loss_G: 0.5671 MSE 92179.5156
===> Epoch[50](390/704): Loss_D: 0.2066 Loss_G: 0.5560 MSE 96128.2734
===> Epoch[50](420/704): Loss_D: 0.1283 Loss_G: 0.9218 MSE 95951.1406
===> Epoch[50](450/704): Loss_D: 0.1612 Loss_G: 0.6107 MSE 102519.6406
===> Epoch[50](480/704): Loss_D: 0.1186 Loss_G: 0.5711 MSE 104352.2344
===> Epoch[50](510/704): Loss_D: 0.2370 Loss_G: 0.4330 MSE 98335.7656
===> Epoch[50](540/704): Loss_D: 0.1723 Loss_G: 0.5475 MSE 95385.7266
===> Epoch[50](570/704): Loss_D: 0.2062 Loss_G: 0.6393 MSE 101230.0625
===> Epoch[50](600/704): Loss_D: 0.1922 Loss_G: 0.4535 MSE 95849.1094
===> Epoch[50](630/704): Loss_D: 0.1633 Loss_G: 0.5988 MSE 103758.5312
===> Epoch[50](660/704): Loss_D: 0.1527 Loss_G: 0.6744 MSE 101368.5469
===> Epoch[50](690/704): Loss_D: 0.1770 Loss_G: 0.4743 MSE 97336.8906
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[51](0/704): Loss_D: 0.2119 Loss_G: 0.2413 MSE 102739.7266
===> Epoch[51](30/704): Loss_D: 0.1639 Loss_G: 0.3707 MSE 96190.7656
===> Epoch[51](60/704): Loss_D: 0.2503 Loss_G: 0.3320 MSE 98879.5312
===> Epoch[51](90/704): Loss_D: 0.2133 Loss_G: 0.4008 MSE 96226.6016
===> Epoch[51](120/704): Loss_D: 0.1364 Loss_G: 0.4184 MSE 97235.8438
===> Epoch[51](150/704): Loss_D: 0.2205 Loss_G: 0.3741 MSE 99570.7344
===> Epoch[51](180/704): Loss_D: 0.1902 Loss_G: 0.4127 MSE 98468.7500
===> Epoch[51](210/704): Loss_D: 0.1573 Loss_G: 0.4521 MSE 103157.5625
===> Epoch[51](240/704): Loss_D: 0.2211 Loss_G: 0.3447 MSE 95967.1406
===> Epoch[51](270/704): Loss_D: 0.2146 Loss_G: 0.3805 MSE 98601.0703
===> Epoch[51](300/704): Loss_D: 0.2226 Loss_G: 0.3959 MSE 96339.0547
===> Epoch[51](330/704): Loss_D: 0.2017 Loss_G: 0.3809 MSE 98792.7969
===> Epoch[51](360/704): Loss_D: 0.2210 Loss_G: 0.4130 MSE 95736.6250
===> Epoch[51](390/704): Loss_D: 0.2284 Loss_G: 0.4244 MSE 94539.0781
===> Epoch[51](420/704): Loss_D: 0.1725 Loss_G: 0.3984 MSE 95308.0469
===> Epoch[51](450/704): Loss_D: 0.2123 Loss_G: 0.3703 MSE 94384.5156
===> Epoch[51](480/704): Loss_D: 0.2178 Loss_G: 0.3570 MSE 97190.7812
===> Epoch[51](510/704): Loss_D: 0.1815 Loss_G: 0.3978 MSE 96244.5078
===> Epoch[51](540/704): Loss_D: 0.1938 Loss_G: 0.4339 MSE 96077.7422
===> Epoch[51](570/704): Loss_D: 0.2444 Loss_G: 0.3474 MSE 95673.1719
===> Epoch[51](600/704): Loss_D: 0.1843 Loss_G: 0.3645 MSE 92268.8516
===> Epoch[51](630/704): Loss_D: 0.2217 Loss_G: 0.3900 MSE 96868.3125
===> Epoch[51](660/704): Loss_D: 0.2398 Loss_G: 0.3048 MSE 93465.1484
===> Epoch[51](690/704): Loss_D: 0.1885 Loss_G: 0.3866 MSE 95860.7344
learning rate = 0.0001000
learning rate = 0.0001000
Saving
===> Epoch[52](0/704): Loss_D: 0.1688 Loss_G: 0.3515 MSE 94038.8047
===> Epoch[52](30/704): Loss_D: 0.2714 Loss_G: 0.3335 MSE 93017.4531
===> Epoch[52](60/704): Loss_D: 0.2051 Loss_G: 0.3703 MSE 95193.1719
===> Epoch[52](90/704): Loss_D: 0.2222 Loss_G: 0.3278 MSE 93260.6641
===> Epoch[52](120/704): Loss_D: 0.2099 Loss_G: 0.3867 MSE 95719.2500
===> Epoch[52](150/704): Loss_D: 0.1701 Loss_G: 0.3587 MSE 93524.5469
===> Epoch[52](180/704): Loss_D: 0.2514 Loss_G: 0.3018 MSE 91622.5625
===> Epoch[52](210/704): Loss_D: 0.2210 Loss_G: 0.3441 MSE 91514.8438
===> Epoch[52](240/704): Loss_D: 0.1682 Loss_G: 0.3650 MSE 95183.5625
===> Epoch[52](270/704): Loss_D: 0.1801 Loss_G: 0.3887 MSE 94109.2422
===> Epoch[52](300/704): Loss_D: 0.2837 Loss_G: 0.3050 MSE 92098.1875
===> Epoch[52](330/704): Loss_D: 0.1991 Loss_G: 0.3404 MSE 93796.3125
===> Epoch[52](360/704): Loss_D: 0.2512 Loss_G: 0.3103 MSE 101044.5156
===> Epoch[52](390/704): Loss_D: 0.2417 Loss_G: 0.3313 MSE 95334.0312
===> Epoch[52](420/704): Loss_D: 0.1921 Loss_G: 0.3461 MSE 97570.0859
===> Epoch[52](450/704): Loss_D: 0.1432 Loss_G: 0.4087 MSE 99736.4922
===> Epoch[52](480/704): Loss_D: 0.2291 Loss_G: 0.3049 MSE 94553.7656
===> Epoch[52](510/704): Loss_D: 0.2022 Loss_G: 0.4068 MSE 99629.8672
===> Epoch[52](540/704): Loss_D: 0.1515 Loss_G: 0.4234 MSE 93523.5703
===> Epoch[52](570/704): Loss_D: 0.2550 Loss_G: 0.3039 MSE 95147.9766
===> Epoch[52](600/704): Loss_D: 0.1969 Loss_G: 0.3517 MSE 98278.7969
===> Epoch[52](630/704): Loss_D: 0.1897 Loss_G: 0.3800 MSE 99967.1250
===> Epoch[52](660/704): Loss_D: 0.2282 Loss_G: 0.3331 MSE 94729.2500
===> Epoch[52](690/704): Loss_D: 0.2265 Loss_G: 0.4571 MSE 95676.6094
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[53](0/704): Loss_D: 0.1773 Loss_G: 0.2730 MSE 98572.5781
===> Epoch[53](30/704): Loss_D: 0.1881 Loss_G: 0.3532 MSE 97998.1641
===> Epoch[53](60/704): Loss_D: 0.2641 Loss_G: 0.3030 MSE 96079.5000
===> Epoch[53](90/704): Loss_D: 0.2336 Loss_G: 0.3279 MSE 97230.3281
===> Epoch[53](120/704): Loss_D: 0.2446 Loss_G: 0.3588 MSE 97256.4531
===> Epoch[53](150/704): Loss_D: 0.1994 Loss_G: 0.3096 MSE 99371.1406
===> Epoch[53](180/704): Loss_D: 0.2673 Loss_G: 0.3408 MSE 96701.6562
===> Epoch[53](210/704): Loss_D: 0.1929 Loss_G: 0.3312 MSE 99345.2891
===> Epoch[53](240/704): Loss_D: 0.2065 Loss_G: 0.3472 MSE 99927.7734
===> Epoch[53](270/704): Loss_D: 0.2721 Loss_G: 0.3978 MSE 97962.9141
===> Epoch[53](300/704): Loss_D: 0.2359 Loss_G: 0.4054 MSE 101671.7344
===> Epoch[53](330/704): Loss_D: 0.1985 Loss_G: 0.3515 MSE 96597.5078
===> Epoch[53](360/704): Loss_D: 0.2193 Loss_G: 0.3335 MSE 94571.7422
===> Epoch[53](390/704): Loss_D: 0.2857 Loss_G: 0.3441 MSE 99015.2812
===> Epoch[53](420/704): Loss_D: 0.1982 Loss_G: 0.4212 MSE 103023.1250
===> Epoch[53](450/704): Loss_D: 0.1956 Loss_G: 0.3907 MSE 100566.9375
===> Epoch[53](480/704): Loss_D: 0.2052 Loss_G: 0.3588 MSE 101251.6094
===> Epoch[53](510/704): Loss_D: 0.1319 Loss_G: 0.4672 MSE 95942.0938
===> Epoch[53](540/704): Loss_D: 0.1701 Loss_G: 0.3525 MSE 103950.5391
===> Epoch[53](570/704): Loss_D: 0.2042 Loss_G: 0.3447 MSE 101399.9688
===> Epoch[53](600/704): Loss_D: 0.2557 Loss_G: 0.3231 MSE 109279.8438
===> Epoch[53](630/704): Loss_D: 0.2517 Loss_G: 0.3103 MSE 103926.5625
===> Epoch[53](660/704): Loss_D: 0.2688 Loss_G: 0.3489 MSE 97783.1094
===> Epoch[53](690/704): Loss_D: 0.2826 Loss_G: 0.3090 MSE 102191.2500
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[54](0/704): Loss_D: 0.1649 Loss_G: 0.3739 MSE 98683.2031
===> Epoch[54](30/704): Loss_D: 0.2666 Loss_G: 0.2743 MSE 101169.5781
===> Epoch[54](60/704): Loss_D: 0.1714 Loss_G: 0.3360 MSE 98545.5781
===> Epoch[54](90/704): Loss_D: 0.2298 Loss_G: 0.3561 MSE 101054.5312
===> Epoch[54](120/704): Loss_D: 0.2031 Loss_G: 0.3365 MSE 98928.7031
===> Epoch[54](150/704): Loss_D: 0.3271 Loss_G: 0.3093 MSE 101279.3359
===> Epoch[54](180/704): Loss_D: 0.1907 Loss_G: 0.3424 MSE 97758.6562
===> Epoch[54](210/704): Loss_D: 0.2627 Loss_G: 0.2787 MSE 98677.9297
===> Epoch[54](240/704): Loss_D: 0.2817 Loss_G: 0.2864 MSE 96017.7344
===> Epoch[54](270/704): Loss_D: 0.2446 Loss_G: 0.2850 MSE 105847.5625
===> Epoch[54](300/704): Loss_D: 0.3008 Loss_G: 0.2412 MSE 103431.7031
===> Epoch[54](330/704): Loss_D: 0.2454 Loss_G: 0.2645 MSE 97909.9453
===> Epoch[54](360/704): Loss_D: 0.2730 Loss_G: 0.2663 MSE 98256.0391
===> Epoch[54](390/704): Loss_D: 0.2737 Loss_G: 0.2692 MSE 107295.1875
===> Epoch[54](420/704): Loss_D: 0.2209 Loss_G: 0.3453 MSE 100549.6406
===> Epoch[54](450/704): Loss_D: 0.2328 Loss_G: 0.3650 MSE 103326.9766
===> Epoch[54](480/704): Loss_D: 0.2387 Loss_G: 0.3725 MSE 99764.9453
===> Epoch[54](510/704): Loss_D: 0.2478 Loss_G: 0.3036 MSE 102929.2891
===> Epoch[54](540/704): Loss_D: 0.2632 Loss_G: 0.3154 MSE 101879.9688
===> Epoch[54](570/704): Loss_D: 0.2199 Loss_G: 0.3293 MSE 99181.9297
===> Epoch[54](600/704): Loss_D: 0.3235 Loss_G: 0.2561 MSE 104542.2266
===> Epoch[54](630/704): Loss_D: 0.2584 Loss_G: 0.2533 MSE 100464.0625
===> Epoch[54](660/704): Loss_D: 0.2544 Loss_G: 0.2969 MSE 99955.9531
===> Epoch[54](690/704): Loss_D: 0.1939 Loss_G: 0.3442 MSE 104439.2891
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[55](0/704): Loss_D: 0.2951 Loss_G: 0.1748 MSE 102872.7969
===> Epoch[55](30/704): Loss_D: 0.2332 Loss_G: 0.2701 MSE 104941.0312
===> Epoch[55](60/704): Loss_D: 0.2565 Loss_G: 0.2825 MSE 104689.9531
===> Epoch[55](90/704): Loss_D: 0.2133 Loss_G: 0.3131 MSE 101817.0000
===> Epoch[55](120/704): Loss_D: 0.2522 Loss_G: 0.2711 MSE 110772.0703
===> Epoch[55](150/704): Loss_D: 0.2632 Loss_G: 0.2656 MSE 100633.5312
===> Epoch[55](180/704): Loss_D: 0.3066 Loss_G: 0.2801 MSE 106905.2812
===> Epoch[55](210/704): Loss_D: 0.2767 Loss_G: 0.2867 MSE 102253.7656
===> Epoch[55](240/704): Loss_D: 0.3251 Loss_G: 0.2153 MSE 100528.9375
===> Epoch[55](270/704): Loss_D: 0.2603 Loss_G: 0.2523 MSE 101230.8984
===> Epoch[55](300/704): Loss_D: 0.3275 Loss_G: 0.2350 MSE 101210.5703
===> Epoch[55](330/704): Loss_D: 0.2569 Loss_G: 0.2831 MSE 97940.3281
===> Epoch[55](360/704): Loss_D: 0.2853 Loss_G: 0.2539 MSE 99863.6406
===> Epoch[55](390/704): Loss_D: 0.2536 Loss_G: 0.2916 MSE 99369.5781
===> Epoch[55](420/704): Loss_D: 0.2532 Loss_G: 0.3185 MSE 98934.0859
===> Epoch[55](450/704): Loss_D: 0.2884 Loss_G: 0.2481 MSE 95226.0156
===> Epoch[55](480/704): Loss_D: 0.2999 Loss_G: 0.2319 MSE 103182.7109
===> Epoch[55](510/704): Loss_D: 0.2413 Loss_G: 0.2728 MSE 104462.1953
===> Epoch[55](540/704): Loss_D: 0.2372 Loss_G: 0.3241 MSE 103310.2188
===> Epoch[55](570/704): Loss_D: 0.2064 Loss_G: 0.3572 MSE 98530.0547
===> Epoch[55](600/704): Loss_D: 0.2377 Loss_G: 0.2611 MSE 98524.9297
===> Epoch[55](630/704): Loss_D: 0.3003 Loss_G: 0.2732 MSE 104993.6094
===> Epoch[55](660/704): Loss_D: 0.2361 Loss_G: 0.2909 MSE 100698.9219
===> Epoch[55](690/704): Loss_D: 0.2535 Loss_G: 0.3064 MSE 101628.9688
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[56](0/704): Loss_D: 0.2813 Loss_G: 0.2513 MSE 104234.1094
===> Epoch[56](30/704): Loss_D: 0.3043 Loss_G: 0.2360 MSE 101697.9688
===> Epoch[56](60/704): Loss_D: 0.2665 Loss_G: 0.2399 MSE 103749.3750
===> Epoch[56](90/704): Loss_D: 0.2258 Loss_G: 0.2995 MSE 101027.1016
===> Epoch[56](120/704): Loss_D: 0.2034 Loss_G: 0.3033 MSE 101259.4141
===> Epoch[56](150/704): Loss_D: 0.2397 Loss_G: 0.2596 MSE 98889.4688
===> Epoch[56](180/704): Loss_D: 0.3178 Loss_G: 0.2438 MSE 96121.5156
===> Epoch[56](210/704): Loss_D: 0.2218 Loss_G: 0.3402 MSE 97250.5000
===> Epoch[56](240/704): Loss_D: 0.2614 Loss_G: 0.2597 MSE 99521.5156
===> Epoch[56](270/704): Loss_D: 0.2466 Loss_G: 0.3167 MSE 102599.1016
===> Epoch[56](300/704): Loss_D: 0.2710 Loss_G: 0.2675 MSE 99863.5781
===> Epoch[56](330/704): Loss_D: 0.2811 Loss_G: 0.2774 MSE 102411.4062
===> Epoch[56](360/704): Loss_D: 0.2447 Loss_G: 0.2727 MSE 101630.5625
===> Epoch[56](390/704): Loss_D: 0.2609 Loss_G: 0.2440 MSE 101273.7656
===> Epoch[56](420/704): Loss_D: 0.2715 Loss_G: 0.2605 MSE 98950.6094
===> Epoch[56](450/704): Loss_D: 0.3127 Loss_G: 0.2738 MSE 100416.7344
===> Epoch[56](480/704): Loss_D: 0.2400 Loss_G: 0.2777 MSE 102178.5938
===> Epoch[56](510/704): Loss_D: 0.2124 Loss_G: 0.3003 MSE 99907.2188
===> Epoch[56](540/704): Loss_D: 0.2079 Loss_G: 0.3200 MSE 99816.0938
===> Epoch[56](570/704): Loss_D: 0.2317 Loss_G: 0.2936 MSE 100982.7969
===> Epoch[56](600/704): Loss_D: 0.2123 Loss_G: 0.3181 MSE 100383.2344
===> Epoch[56](630/704): Loss_D: 0.2155 Loss_G: 0.3358 MSE 104052.4844
===> Epoch[56](660/704): Loss_D: 0.1895 Loss_G: 0.3528 MSE 98047.0781
===> Epoch[56](690/704): Loss_D: 0.2709 Loss_G: 0.2851 MSE 103522.3438
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[57](0/704): Loss_D: 0.2679 Loss_G: 0.1509 MSE 99793.7109
===> Epoch[57](30/704): Loss_D: 0.2468 Loss_G: 0.3187 MSE 101071.6172
===> Epoch[57](60/704): Loss_D: 0.3043 Loss_G: 0.2645 MSE 99777.0781
===> Epoch[57](90/704): Loss_D: 0.2022 Loss_G: 0.2691 MSE 97215.0156
===> Epoch[57](120/704): Loss_D: 0.3502 Loss_G: 0.2070 MSE 97258.1172
===> Epoch[57](150/704): Loss_D: 0.2536 Loss_G: 0.2678 MSE 99439.1484
===> Epoch[57](180/704): Loss_D: 0.3027 Loss_G: 0.2628 MSE 101395.1484
===> Epoch[57](210/704): Loss_D: 0.2808 Loss_G: 0.2539 MSE 96831.2656
===> Epoch[57](240/704): Loss_D: 0.2528 Loss_G: 0.2775 MSE 101068.0312
===> Epoch[57](270/704): Loss_D: 0.1982 Loss_G: 0.2990 MSE 100512.4453
===> Epoch[57](300/704): Loss_D: 0.2715 Loss_G: 0.2280 MSE 100078.8203
===> Epoch[57](330/704): Loss_D: 0.2639 Loss_G: 0.2680 MSE 102945.5859
===> Epoch[57](360/704): Loss_D: 0.2062 Loss_G: 0.3125 MSE 103479.2031
===> Epoch[57](390/704): Loss_D: 0.1781 Loss_G: 0.4063 MSE 103263.9922
===> Epoch[57](420/704): Loss_D: 0.2000 Loss_G: 0.3317 MSE 97631.4297
===> Epoch[57](450/704): Loss_D: 0.2648 Loss_G: 0.2810 MSE 100994.8281
===> Epoch[57](480/704): Loss_D: 0.2447 Loss_G: 0.3062 MSE 102836.7500
===> Epoch[57](510/704): Loss_D: 0.3094 Loss_G: 0.2898 MSE 93852.4531
===> Epoch[57](540/704): Loss_D: 0.2427 Loss_G: 0.2937 MSE 100952.2578
===> Epoch[57](570/704): Loss_D: 0.2100 Loss_G: 0.3257 MSE 99163.9844
===> Epoch[57](600/704): Loss_D: 0.2690 Loss_G: 0.2659 MSE 96309.6094
===> Epoch[57](630/704): Loss_D: 0.2702 Loss_G: 0.2523 MSE 100783.7578
===> Epoch[57](660/704): Loss_D: 0.2470 Loss_G: 0.3008 MSE 98923.6719
===> Epoch[57](690/704): Loss_D: 0.2536 Loss_G: 0.2925 MSE 99416.5234
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[58](0/704): Loss_D: 0.2084 Loss_G: 0.2499 MSE 103221.4688
===> Epoch[58](30/704): Loss_D: 0.2563 Loss_G: 0.2831 MSE 104729.8281
===> Epoch[58](60/704): Loss_D: 0.2361 Loss_G: 0.2823 MSE 95512.6875
===> Epoch[58](90/704): Loss_D: 0.2679 Loss_G: 0.2757 MSE 99115.7812
===> Epoch[58](120/704): Loss_D: 0.2266 Loss_G: 0.2852 MSE 99361.3594
===> Epoch[58](150/704): Loss_D: 0.2832 Loss_G: 0.2373 MSE 104988.9844
===> Epoch[58](180/704): Loss_D: 0.2900 Loss_G: 0.2374 MSE 98889.2500
===> Epoch[58](210/704): Loss_D: 0.2747 Loss_G: 0.2384 MSE 99232.7188
===> Epoch[58](240/704): Loss_D: 0.2570 Loss_G: 0.2875 MSE 98773.5938
===> Epoch[58](270/704): Loss_D: 0.2690 Loss_G: 0.2436 MSE 98131.5703
===> Epoch[58](300/704): Loss_D: 0.2065 Loss_G: 0.3174 MSE 95375.4531
===> Epoch[58](330/704): Loss_D: 0.2094 Loss_G: 0.3960 MSE 103037.8750
===> Epoch[58](360/704): Loss_D: 0.2406 Loss_G: 0.2697 MSE 99722.9375
===> Epoch[58](390/704): Loss_D: 0.2604 Loss_G: 0.2676 MSE 101913.1953
===> Epoch[58](420/704): Loss_D: 0.2054 Loss_G: 0.3478 MSE 98321.8906
===> Epoch[58](450/704): Loss_D: 0.2401 Loss_G: 0.3078 MSE 103604.5312
===> Epoch[58](480/704): Loss_D: 0.2603 Loss_G: 0.2845 MSE 97261.6797
===> Epoch[58](510/704): Loss_D: 0.2272 Loss_G: 0.2940 MSE 102985.8125
===> Epoch[58](540/704): Loss_D: 0.2775 Loss_G: 0.2736 MSE 100169.4375
===> Epoch[58](570/704): Loss_D: 0.2047 Loss_G: 0.2976 MSE 104502.5703
===> Epoch[58](600/704): Loss_D: 0.2483 Loss_G: 0.3165 MSE 101803.2578
===> Epoch[58](630/704): Loss_D: 0.2364 Loss_G: 0.2868 MSE 105316.2500
===> Epoch[58](660/704): Loss_D: 0.2196 Loss_G: 0.3175 MSE 102965.7188
===> Epoch[58](690/704): Loss_D: 0.1903 Loss_G: 0.3542 MSE 103083.1250
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[59](0/704): Loss_D: 0.0902 Loss_G: 0.5593 MSE 100610.7656
===> Epoch[59](30/704): Loss_D: 0.2720 Loss_G: 0.2466 MSE 102792.7031
===> Epoch[59](60/704): Loss_D: 0.2217 Loss_G: 0.2899 MSE 104100.2812
===> Epoch[59](90/704): Loss_D: 0.2760 Loss_G: 0.2972 MSE 100631.7812
===> Epoch[59](120/704): Loss_D: 0.2398 Loss_G: 0.2847 MSE 102838.4609
===> Epoch[59](150/704): Loss_D: 0.2388 Loss_G: 0.2481 MSE 100329.2656
===> Epoch[59](180/704): Loss_D: 0.2796 Loss_G: 0.2708 MSE 100866.0938
===> Epoch[59](210/704): Loss_D: 0.2493 Loss_G: 0.2913 MSE 101765.0938
===> Epoch[59](240/704): Loss_D: 0.2385 Loss_G: 0.2763 MSE 103747.8125
===> Epoch[59](270/704): Loss_D: 0.2516 Loss_G: 0.2839 MSE 102032.5938
===> Epoch[59](300/704): Loss_D: 0.1977 Loss_G: 0.3154 MSE 96555.2031
===> Epoch[59](330/704): Loss_D: 0.2051 Loss_G: 0.3075 MSE 100255.0312
===> Epoch[59](360/704): Loss_D: 0.2390 Loss_G: 0.3388 MSE 99890.8828
===> Epoch[59](390/704): Loss_D: 0.1972 Loss_G: 0.3517 MSE 99456.8438
===> Epoch[59](420/704): Loss_D: 0.1921 Loss_G: 0.3198 MSE 98544.0000
===> Epoch[59](450/704): Loss_D: 0.1999 Loss_G: 0.3268 MSE 102042.7812
===> Epoch[59](480/704): Loss_D: 0.1945 Loss_G: 0.3164 MSE 98499.2656
===> Epoch[59](510/704): Loss_D: 0.1799 Loss_G: 0.3444 MSE 101170.5312
===> Epoch[59](540/704): Loss_D: 0.2610 Loss_G: 0.2495 MSE 94005.6016
===> Epoch[59](570/704): Loss_D: 0.1703 Loss_G: 0.3591 MSE 103766.2422
===> Epoch[59](600/704): Loss_D: 0.1965 Loss_G: 0.3789 MSE 102516.5781
===> Epoch[59](630/704): Loss_D: 0.2255 Loss_G: 0.3285 MSE 105653.1875
===> Epoch[59](660/704): Loss_D: 0.2343 Loss_G: 0.3065 MSE 100790.2656
===> Epoch[59](690/704): Loss_D: 0.2237 Loss_G: 0.3107 MSE 98662.4531
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[60](0/704): Loss_D: 0.2515 Loss_G: 0.2148 MSE 100262.4062
===> Epoch[60](30/704): Loss_D: 0.1843 Loss_G: 0.3730 MSE 100473.6406
===> Epoch[60](60/704): Loss_D: 0.2230 Loss_G: 0.3092 MSE 94267.9141
===> Epoch[60](90/704): Loss_D: 0.1555 Loss_G: 0.4285 MSE 97585.6094
===> Epoch[60](120/704): Loss_D: 0.1860 Loss_G: 0.3499 MSE 100787.5234
===> Epoch[60](150/704): Loss_D: 0.2108 Loss_G: 0.3158 MSE 100789.2969
===> Epoch[60](180/704): Loss_D: 0.2045 Loss_G: 0.2971 MSE 105937.9219
===> Epoch[60](210/704): Loss_D: 0.1162 Loss_G: 0.5262 MSE 100627.2969
===> Epoch[60](240/704): Loss_D: 0.2339 Loss_G: 0.2703 MSE 96422.7969
===> Epoch[60](270/704): Loss_D: 0.2614 Loss_G: 0.2442 MSE 100571.0781
===> Epoch[60](300/704): Loss_D: 0.2685 Loss_G: 0.3504 MSE 105703.1406
===> Epoch[60](330/704): Loss_D: 0.2607 Loss_G: 0.2092 MSE 103333.0234
===> Epoch[60](360/704): Loss_D: 0.2649 Loss_G: 0.2636 MSE 101357.0625
===> Epoch[60](390/704): Loss_D: 0.2468 Loss_G: 0.2699 MSE 103622.0781
===> Epoch[60](420/704): Loss_D: 0.2580 Loss_G: 0.2975 MSE 94784.1875
===> Epoch[60](450/704): Loss_D: 0.2872 Loss_G: 0.2545 MSE 93168.3203
===> Epoch[60](480/704): Loss_D: 0.2616 Loss_G: 0.2284 MSE 102416.9531
===> Epoch[60](510/704): Loss_D: 0.2164 Loss_G: 0.3376 MSE 99789.0625
===> Epoch[60](540/704): Loss_D: 0.3065 Loss_G: 0.3376 MSE 99498.1562
===> Epoch[60](570/704): Loss_D: 0.1764 Loss_G: 0.3599 MSE 97152.2969
===> Epoch[60](600/704): Loss_D: 0.1321 Loss_G: 0.4235 MSE 98306.6562
===> Epoch[60](630/704): Loss_D: 0.2151 Loss_G: 0.3447 MSE 97457.9062
===> Epoch[60](660/704): Loss_D: 0.3278 Loss_G: 0.2460 MSE 101294.6172
===> Epoch[60](690/704): Loss_D: 0.4097 Loss_G: 0.1857 MSE 98824.8438
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[61](0/704): Loss_D: 0.3410 Loss_G: 0.1500 MSE 98137.4062
===> Epoch[61](30/704): Loss_D: 0.1782 Loss_G: 0.3448 MSE 99472.1406
===> Epoch[61](60/704): Loss_D: 0.2808 Loss_G: 0.2898 MSE 101339.7656
===> Epoch[61](90/704): Loss_D: 0.2267 Loss_G: 0.2999 MSE 97798.4844
===> Epoch[61](120/704): Loss_D: 0.2570 Loss_G: 0.3332 MSE 99296.9062
===> Epoch[61](150/704): Loss_D: 0.2859 Loss_G: 0.2979 MSE 95610.9219
===> Epoch[61](180/704): Loss_D: 0.2932 Loss_G: 0.1888 MSE 93838.8438
===> Epoch[61](210/704): Loss_D: 0.2116 Loss_G: 0.3202 MSE 99037.7344
===> Epoch[61](240/704): Loss_D: 0.2812 Loss_G: 0.3037 MSE 93799.9531
===> Epoch[61](270/704): Loss_D: 0.2892 Loss_G: 0.2372 MSE 96815.5156
===> Epoch[61](300/704): Loss_D: 0.2407 Loss_G: 0.3017 MSE 104357.5781
===> Epoch[61](330/704): Loss_D: 0.2303 Loss_G: 0.3277 MSE 102353.2344
===> Epoch[61](360/704): Loss_D: 0.2432 Loss_G: 0.3017 MSE 99735.3672
===> Epoch[61](390/704): Loss_D: 0.2092 Loss_G: 0.3661 MSE 103329.9297
===> Epoch[61](420/704): Loss_D: 0.2306 Loss_G: 0.3165 MSE 105779.2656
===> Epoch[61](450/704): Loss_D: 0.2172 Loss_G: 0.3238 MSE 103169.8359
===> Epoch[61](480/704): Loss_D: 0.3179 Loss_G: 0.2263 MSE 102184.6875
===> Epoch[61](510/704): Loss_D: 0.2667 Loss_G: 0.2794 MSE 100503.7578
===> Epoch[61](540/704): Loss_D: 0.2885 Loss_G: 0.2812 MSE 99821.3594
===> Epoch[61](570/704): Loss_D: 0.2720 Loss_G: 0.2412 MSE 91648.7031
===> Epoch[61](600/704): Loss_D: 0.2793 Loss_G: 0.2554 MSE 98120.8984
===> Epoch[61](630/704): Loss_D: 0.2268 Loss_G: 0.2603 MSE 95993.0156
===> Epoch[61](660/704): Loss_D: 0.2727 Loss_G: 0.2567 MSE 102832.8594
===> Epoch[61](690/704): Loss_D: 0.2418 Loss_G: 0.2903 MSE 103669.0859
learning rate = 0.0001000
learning rate = 0.0001000
Saving
===> Epoch[62](0/704): Loss_D: 0.1923 Loss_G: 0.2421 MSE 103484.5469
===> Epoch[62](30/704): Loss_D: 0.2445 Loss_G: 0.3247 MSE 108385.9141
===> Epoch[62](60/704): Loss_D: 0.2815 Loss_G: 0.2345 MSE 97830.8281
===> Epoch[62](90/704): Loss_D: 0.2699 Loss_G: 0.2462 MSE 99139.9844
===> Epoch[62](120/704): Loss_D: 0.2687 Loss_G: 0.2502 MSE 95586.5938
===> Epoch[62](150/704): Loss_D: 0.2327 Loss_G: 0.2733 MSE 92701.9688
===> Epoch[62](180/704): Loss_D: 0.2400 Loss_G: 0.2714 MSE 95610.5469
===> Epoch[62](210/704): Loss_D: 0.2507 Loss_G: 0.2999 MSE 95077.5859
===> Epoch[62](240/704): Loss_D: 0.2724 Loss_G: 0.2623 MSE 97990.8203
===> Epoch[62](270/704): Loss_D: 0.2316 Loss_G: 0.2902 MSE 95533.5469
===> Epoch[62](300/704): Loss_D: 0.2718 Loss_G: 0.2822 MSE 97700.2266
===> Epoch[62](330/704): Loss_D: 0.2256 Loss_G: 0.2746 MSE 102322.1719
===> Epoch[62](360/704): Loss_D: 0.2444 Loss_G: 0.2694 MSE 101696.3594
===> Epoch[62](390/704): Loss_D: 0.2803 Loss_G: 0.2395 MSE 104278.3438
===> Epoch[62](420/704): Loss_D: 0.2227 Loss_G: 0.2949 MSE 102432.8828
===> Epoch[62](450/704): Loss_D: 0.2591 Loss_G: 0.2486 MSE 102031.6406
===> Epoch[62](480/704): Loss_D: 0.2602 Loss_G: 0.2577 MSE 96656.2344
===> Epoch[62](510/704): Loss_D: 0.2944 Loss_G: 0.2439 MSE 96555.3672
===> Epoch[62](540/704): Loss_D: 0.1996 Loss_G: 0.3229 MSE 106433.8125
===> Epoch[62](570/704): Loss_D: 0.2882 Loss_G: 0.2772 MSE 102444.8672
===> Epoch[62](600/704): Loss_D: 0.2281 Loss_G: 0.3027 MSE 106724.0078
===> Epoch[62](630/704): Loss_D: 0.2691 Loss_G: 0.2927 MSE 103720.6562
===> Epoch[62](660/704): Loss_D: 0.2907 Loss_G: 0.2490 MSE 101362.8047
===> Epoch[62](690/704): Loss_D: 0.2648 Loss_G: 0.2648 MSE 96144.5000
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[63](0/704): Loss_D: 0.2594 Loss_G: 0.2022 MSE 98710.1094
===> Epoch[63](30/704): Loss_D: 0.2050 Loss_G: 0.3298 MSE 101366.4531
===> Epoch[63](60/704): Loss_D: 0.2242 Loss_G: 0.3095 MSE 104455.0859
===> Epoch[63](90/704): Loss_D: 0.2220 Loss_G: 0.2754 MSE 99193.1797
===> Epoch[63](120/704): Loss_D: 0.2788 Loss_G: 0.2368 MSE 96733.8281
===> Epoch[63](150/704): Loss_D: 0.2476 Loss_G: 0.3026 MSE 101540.3594
===> Epoch[63](180/704): Loss_D: 0.2511 Loss_G: 0.3132 MSE 98155.8203
===> Epoch[63](210/704): Loss_D: 0.2389 Loss_G: 0.3172 MSE 96964.4062
===> Epoch[63](240/704): Loss_D: 0.2199 Loss_G: 0.3287 MSE 98238.8906
===> Epoch[63](270/704): Loss_D: 0.2356 Loss_G: 0.2776 MSE 102692.9531
===> Epoch[63](300/704): Loss_D: 0.2264 Loss_G: 0.3239 MSE 97374.7656
===> Epoch[63](330/704): Loss_D: 0.2227 Loss_G: 0.3539 MSE 99212.0000
===> Epoch[63](360/704): Loss_D: 0.2365 Loss_G: 0.3466 MSE 100200.7969
===> Epoch[63](390/704): Loss_D: 0.2341 Loss_G: 0.3000 MSE 100620.4688
===> Epoch[63](420/704): Loss_D: 0.2240 Loss_G: 0.2767 MSE 100151.0938
===> Epoch[63](450/704): Loss_D: 0.2202 Loss_G: 0.2946 MSE 102481.2344
===> Epoch[63](480/704): Loss_D: 0.2475 Loss_G: 0.2779 MSE 103226.2656
===> Epoch[63](510/704): Loss_D: 0.2421 Loss_G: 0.2931 MSE 96736.6328
===> Epoch[63](540/704): Loss_D: 0.2507 Loss_G: 0.3109 MSE 100756.1250
===> Epoch[63](570/704): Loss_D: 0.2854 Loss_G: 0.2924 MSE 100906.8203
===> Epoch[63](600/704): Loss_D: 0.2613 Loss_G: 0.2701 MSE 103481.2344
===> Epoch[63](630/704): Loss_D: 0.2152 Loss_G: 0.3391 MSE 101252.3672
===> Epoch[63](660/704): Loss_D: 0.2335 Loss_G: 0.3031 MSE 98507.1719
===> Epoch[63](690/704): Loss_D: 0.2663 Loss_G: 0.2474 MSE 101922.1094
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[64](0/704): Loss_D: 0.2311 Loss_G: 0.1885 MSE 101880.7344
===> Epoch[64](30/704): Loss_D: 0.2363 Loss_G: 0.2954 MSE 101520.7188
===> Epoch[64](60/704): Loss_D: 0.2164 Loss_G: 0.3081 MSE 104258.2656
===> Epoch[64](90/704): Loss_D: 0.2366 Loss_G: 0.2894 MSE 102694.7500
===> Epoch[64](120/704): Loss_D: 0.2401 Loss_G: 0.2945 MSE 100651.4844
===> Epoch[64](150/704): Loss_D: 0.2295 Loss_G: 0.2924 MSE 102192.7812
===> Epoch[64](180/704): Loss_D: 0.2878 Loss_G: 0.2672 MSE 100703.8906
===> Epoch[64](210/704): Loss_D: 0.1992 Loss_G: 0.3069 MSE 101572.7812
===> Epoch[64](240/704): Loss_D: 0.2069 Loss_G: 0.3435 MSE 98270.7500
===> Epoch[64](270/704): Loss_D: 0.2410 Loss_G: 0.2939 MSE 100610.2422
===> Epoch[64](300/704): Loss_D: 0.2153 Loss_G: 0.3143 MSE 99263.6172
===> Epoch[64](330/704): Loss_D: 0.2378 Loss_G: 0.3172 MSE 101984.6797
===> Epoch[64](360/704): Loss_D: 0.2270 Loss_G: 0.2804 MSE 100489.0156
===> Epoch[64](390/704): Loss_D: 0.2186 Loss_G: 0.2658 MSE 102786.3750
===> Epoch[64](420/704): Loss_D: 0.2368 Loss_G: 0.2844 MSE 102337.1250
===> Epoch[64](450/704): Loss_D: 0.2262 Loss_G: 0.3432 MSE 106339.3125
===> Epoch[64](480/704): Loss_D: 0.1837 Loss_G: 0.3541 MSE 103316.7500
===> Epoch[64](510/704): Loss_D: 0.2683 Loss_G: 0.2970 MSE 100703.9844
===> Epoch[64](540/704): Loss_D: 0.2084 Loss_G: 0.3168 MSE 106281.7266
===> Epoch[64](570/704): Loss_D: 0.2308 Loss_G: 0.3557 MSE 110581.2266
===> Epoch[64](600/704): Loss_D: 0.1880 Loss_G: 0.3355 MSE 109944.9688
===> Epoch[64](630/704): Loss_D: 0.1982 Loss_G: 0.3557 MSE 109589.7734
===> Epoch[64](660/704): Loss_D: 0.2218 Loss_G: 0.3582 MSE 105758.8438
===> Epoch[64](690/704): Loss_D: 0.2099 Loss_G: 0.3262 MSE 100063.6562
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[65](0/704): Loss_D: 0.1640 Loss_G: 0.3131 MSE 102519.0312
===> Epoch[65](30/704): Loss_D: 0.1804 Loss_G: 0.3932 MSE 106193.7344
===> Epoch[65](60/704): Loss_D: 0.2194 Loss_G: 0.3309 MSE 104207.0000
===> Epoch[65](90/704): Loss_D: 0.2217 Loss_G: 0.3246 MSE 102798.8906
===> Epoch[65](120/704): Loss_D: 0.2458 Loss_G: 0.3199 MSE 107307.4062
===> Epoch[65](150/704): Loss_D: 0.2110 Loss_G: 0.3792 MSE 101813.2969
===> Epoch[65](180/704): Loss_D: 0.2402 Loss_G: 0.2953 MSE 103304.0859
===> Epoch[65](210/704): Loss_D: 0.2042 Loss_G: 0.3762 MSE 99529.9219
===> Epoch[65](240/704): Loss_D: 0.2266 Loss_G: 0.3453 MSE 100696.7969
===> Epoch[65](270/704): Loss_D: 0.1829 Loss_G: 0.3668 MSE 104797.9922
===> Epoch[65](300/704): Loss_D: 0.2168 Loss_G: 0.3461 MSE 105058.1797
===> Epoch[65](330/704): Loss_D: 0.2059 Loss_G: 0.3363 MSE 96941.0859
===> Epoch[65](360/704): Loss_D: 0.1703 Loss_G: 0.3747 MSE 99045.7188
===> Epoch[65](390/704): Loss_D: 0.2172 Loss_G: 0.3216 MSE 96330.2734
===> Epoch[65](420/704): Loss_D: 0.2494 Loss_G: 0.2791 MSE 96654.6562
===> Epoch[65](450/704): Loss_D: 0.2323 Loss_G: 0.2941 MSE 99450.1406
===> Epoch[65](480/704): Loss_D: 0.1999 Loss_G: 0.3881 MSE 99719.2891
===> Epoch[65](510/704): Loss_D: 0.2180 Loss_G: 0.3263 MSE 100293.9375
===> Epoch[65](540/704): Loss_D: 0.2233 Loss_G: 0.3225 MSE 100457.5781
===> Epoch[65](570/704): Loss_D: 0.2210 Loss_G: 0.2972 MSE 96718.8828
===> Epoch[65](600/704): Loss_D: 0.1962 Loss_G: 0.3549 MSE 102741.8984
===> Epoch[65](630/704): Loss_D: 0.2223 Loss_G: 0.2956 MSE 101048.4141
===> Epoch[65](660/704): Loss_D: 0.2256 Loss_G: 0.3026 MSE 98842.2500
===> Epoch[65](690/704): Loss_D: 0.2323 Loss_G: 0.3121 MSE 100521.4922
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[66](0/704): Loss_D: 0.1561 Loss_G: 0.4379 MSE 103307.7656
===> Epoch[66](30/704): Loss_D: 0.1696 Loss_G: 0.4133 MSE 103182.4531
===> Epoch[66](60/704): Loss_D: 0.2220 Loss_G: 0.3368 MSE 104220.4844
===> Epoch[66](90/704): Loss_D: 0.2008 Loss_G: 0.3504 MSE 101339.5938
===> Epoch[66](120/704): Loss_D: 0.2189 Loss_G: 0.3428 MSE 100115.5703
===> Epoch[66](150/704): Loss_D: 0.2041 Loss_G: 0.3545 MSE 103846.0234
===> Epoch[66](180/704): Loss_D: 0.2513 Loss_G: 0.2909 MSE 106915.9688
===> Epoch[66](210/704): Loss_D: 0.2307 Loss_G: 0.2884 MSE 99304.9297
===> Epoch[66](240/704): Loss_D: 0.2914 Loss_G: 0.2955 MSE 97347.4688
===> Epoch[66](270/704): Loss_D: 0.2043 Loss_G: 0.3752 MSE 100983.8359
===> Epoch[66](300/704): Loss_D: 0.2251 Loss_G: 0.3398 MSE 99260.1641
===> Epoch[66](330/704): Loss_D: 0.2136 Loss_G: 0.3647 MSE 100733.7031
===> Epoch[66](360/704): Loss_D: 0.1810 Loss_G: 0.3438 MSE 101264.0469
===> Epoch[66](390/704): Loss_D: 0.2177 Loss_G: 0.3243 MSE 97610.4062
===> Epoch[66](420/704): Loss_D: 0.1812 Loss_G: 0.3226 MSE 104547.0625
===> Epoch[66](450/704): Loss_D: 0.2257 Loss_G: 0.3584 MSE 99459.7969
===> Epoch[66](480/704): Loss_D: 0.1357 Loss_G: 0.3908 MSE 102610.3047
===> Epoch[66](510/704): Loss_D: 0.1549 Loss_G: 0.3835 MSE 99705.6875
===> Epoch[66](540/704): Loss_D: 0.2121 Loss_G: 0.3077 MSE 98522.5000
===> Epoch[66](570/704): Loss_D: 0.1733 Loss_G: 0.3288 MSE 102043.2891
===> Epoch[66](600/704): Loss_D: 0.1936 Loss_G: 0.3389 MSE 98907.5469
===> Epoch[66](630/704): Loss_D: 0.1630 Loss_G: 0.3821 MSE 102332.0312
===> Epoch[66](660/704): Loss_D: 0.1859 Loss_G: 0.3764 MSE 103086.6875
===> Epoch[66](690/704): Loss_D: 0.2260 Loss_G: 0.2742 MSE 99784.1562
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[67](0/704): Loss_D: 0.1746 Loss_G: 0.2701 MSE 103884.8594
===> Epoch[67](30/704): Loss_D: 0.1905 Loss_G: 0.3543 MSE 100656.5312
===> Epoch[67](60/704): Loss_D: 0.2481 Loss_G: 0.2874 MSE 100862.7891
===> Epoch[67](90/704): Loss_D: 0.2866 Loss_G: 0.2679 MSE 101603.2422
===> Epoch[67](120/704): Loss_D: 0.2338 Loss_G: 0.3373 MSE 101860.2578
===> Epoch[67](150/704): Loss_D: 0.2211 Loss_G: 0.2852 MSE 102161.9844
===> Epoch[67](180/704): Loss_D: 0.2251 Loss_G: 0.3020 MSE 98717.4922
===> Epoch[67](210/704): Loss_D: 0.1991 Loss_G: 0.3224 MSE 105749.5391
===> Epoch[67](240/704): Loss_D: 0.2066 Loss_G: 0.4039 MSE 98630.0781
===> Epoch[67](270/704): Loss_D: 0.2382 Loss_G: 0.3118 MSE 104786.5625
===> Epoch[67](300/704): Loss_D: 0.2119 Loss_G: 0.2858 MSE 104170.7656
===> Epoch[67](330/704): Loss_D: 0.2253 Loss_G: 0.3827 MSE 99695.6172
===> Epoch[67](360/704): Loss_D: 0.1802 Loss_G: 0.3610 MSE 105623.7656
===> Epoch[67](390/704): Loss_D: 0.2237 Loss_G: 0.2937 MSE 103776.3438
===> Epoch[67](420/704): Loss_D: 0.2166 Loss_G: 0.3234 MSE 99917.0156
===> Epoch[67](450/704): Loss_D: 0.2346 Loss_G: 0.3057 MSE 99288.4141
===> Epoch[67](480/704): Loss_D: 0.1920 Loss_G: 0.3516 MSE 99149.2656
===> Epoch[67](510/704): Loss_D: 0.2977 Loss_G: 0.3095 MSE 99057.4453
===> Epoch[67](540/704): Loss_D: 0.2189 Loss_G: 0.2948 MSE 100436.3281
===> Epoch[67](570/704): Loss_D: 0.2698 Loss_G: 0.2814 MSE 103079.3281
===> Epoch[67](600/704): Loss_D: 0.2270 Loss_G: 0.2919 MSE 102594.7422
===> Epoch[67](630/704): Loss_D: 0.2640 Loss_G: 0.3057 MSE 109168.9219
===> Epoch[67](660/704): Loss_D: 0.2237 Loss_G: 0.3188 MSE 101852.3125
===> Epoch[67](690/704): Loss_D: 0.1780 Loss_G: 0.3696 MSE 104337.9062
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[68](0/704): Loss_D: 0.2144 Loss_G: 0.4142 MSE 105729.5312
===> Epoch[68](30/704): Loss_D: 0.2254 Loss_G: 0.3369 MSE 105129.4922
===> Epoch[68](60/704): Loss_D: 0.2039 Loss_G: 0.3299 MSE 102301.5859
===> Epoch[68](90/704): Loss_D: 0.2071 Loss_G: 0.4306 MSE 104821.1250
===> Epoch[68](120/704): Loss_D: 0.2574 Loss_G: 0.2691 MSE 100184.5781
===> Epoch[68](150/704): Loss_D: 0.2414 Loss_G: 0.3614 MSE 103335.0547
===> Epoch[68](180/704): Loss_D: 0.1966 Loss_G: 0.3454 MSE 101917.8125
===> Epoch[68](210/704): Loss_D: 0.2684 Loss_G: 0.2667 MSE 100814.4219
===> Epoch[68](240/704): Loss_D: 0.2315 Loss_G: 0.2595 MSE 106313.0703
===> Epoch[68](270/704): Loss_D: 0.2659 Loss_G: 0.2378 MSE 105530.4062
===> Epoch[68](300/704): Loss_D: 0.2403 Loss_G: 0.2436 MSE 106915.6719
===> Epoch[68](330/704): Loss_D: 0.2142 Loss_G: 0.3467 MSE 110150.7109
===> Epoch[68](360/704): Loss_D: 0.2688 Loss_G: 0.2654 MSE 100296.9297
===> Epoch[68](390/704): Loss_D: 0.2383 Loss_G: 0.3005 MSE 104959.4531
===> Epoch[68](420/704): Loss_D: 0.2076 Loss_G: 0.3171 MSE 106372.5312
===> Epoch[68](450/704): Loss_D: 0.2146 Loss_G: 0.3939 MSE 105140.6562
===> Epoch[68](480/704): Loss_D: 0.2216 Loss_G: 0.3281 MSE 100181.3516
===> Epoch[68](510/704): Loss_D: 0.2261 Loss_G: 0.3469 MSE 105373.2109
===> Epoch[68](540/704): Loss_D: 0.2065 Loss_G: 0.3338 MSE 106497.1250
===> Epoch[68](570/704): Loss_D: 0.2335 Loss_G: 0.2917 MSE 108111.5078
===> Epoch[68](600/704): Loss_D: 0.2447 Loss_G: 0.2565 MSE 100073.6172
===> Epoch[68](630/704): Loss_D: 0.2493 Loss_G: 0.3119 MSE 102446.2188
===> Epoch[68](660/704): Loss_D: 0.1896 Loss_G: 0.3564 MSE 106175.7344
===> Epoch[68](690/704): Loss_D: 0.2323 Loss_G: 0.3038 MSE 103126.3281
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[69](0/704): Loss_D: 0.1929 Loss_G: 0.2431 MSE 102566.6016
===> Epoch[69](30/704): Loss_D: 0.1850 Loss_G: 0.3257 MSE 98593.9766
===> Epoch[69](60/704): Loss_D: 0.1985 Loss_G: 0.3834 MSE 104620.6719
===> Epoch[69](90/704): Loss_D: 0.1757 Loss_G: 0.4081 MSE 104986.0938
===> Epoch[69](120/704): Loss_D: 0.2922 Loss_G: 0.1985 MSE 102998.2891
===> Epoch[69](150/704): Loss_D: 0.3431 Loss_G: 0.2678 MSE 105954.8594
===> Epoch[69](180/704): Loss_D: 0.2728 Loss_G: 0.2330 MSE 106331.4219
===> Epoch[69](210/704): Loss_D: 0.3341 Loss_G: 0.2685 MSE 106107.3672
===> Epoch[69](240/704): Loss_D: 0.2833 Loss_G: 0.2895 MSE 105551.4688
===> Epoch[69](270/704): Loss_D: 0.2504 Loss_G: 0.2983 MSE 110791.6562
===> Epoch[69](300/704): Loss_D: 0.2193 Loss_G: 0.3339 MSE 113490.3125
===> Epoch[69](330/704): Loss_D: 0.2427 Loss_G: 0.2863 MSE 106234.2188
===> Epoch[69](360/704): Loss_D: 0.2485 Loss_G: 0.2525 MSE 105526.4531
===> Epoch[69](390/704): Loss_D: 0.2470 Loss_G: 0.2654 MSE 105473.0625
===> Epoch[69](420/704): Loss_D: 0.2380 Loss_G: 0.3309 MSE 104558.0156
===> Epoch[69](450/704): Loss_D: 0.2631 Loss_G: 0.2428 MSE 104266.8750
===> Epoch[69](480/704): Loss_D: 0.2446 Loss_G: 0.2752 MSE 104722.8906
===> Epoch[69](510/704): Loss_D: 0.2461 Loss_G: 0.3206 MSE 104920.1719
===> Epoch[69](540/704): Loss_D: 0.2043 Loss_G: 0.3423 MSE 106821.5391
===> Epoch[69](570/704): Loss_D: 0.2819 Loss_G: 0.2659 MSE 107641.6562
===> Epoch[69](600/704): Loss_D: 0.1919 Loss_G: 0.3110 MSE 110014.4609
===> Epoch[69](630/704): Loss_D: 0.1961 Loss_G: 0.3432 MSE 105898.8438
===> Epoch[69](660/704): Loss_D: 0.2141 Loss_G: 0.3336 MSE 107771.3281
===> Epoch[69](690/704): Loss_D: 0.2169 Loss_G: 0.3754 MSE 106061.5703
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[70](0/704): Loss_D: 0.1559 Loss_G: 0.3110 MSE 104254.9609
===> Epoch[70](30/704): Loss_D: 0.2854 Loss_G: 0.2767 MSE 109345.6328
===> Epoch[70](60/704): Loss_D: 0.1849 Loss_G: 0.3861 MSE 107218.5312
===> Epoch[70](90/704): Loss_D: 0.1899 Loss_G: 0.3793 MSE 101558.0625
===> Epoch[70](120/704): Loss_D: 0.2116 Loss_G: 0.3430 MSE 103013.5625
===> Epoch[70](150/704): Loss_D: 0.2000 Loss_G: 0.3488 MSE 104942.9766
===> Epoch[70](180/704): Loss_D: 0.2495 Loss_G: 0.2869 MSE 106002.8906
===> Epoch[70](210/704): Loss_D: 0.2238 Loss_G: 0.3384 MSE 104560.9375
===> Epoch[70](240/704): Loss_D: 0.1771 Loss_G: 0.3930 MSE 100202.0625
===> Epoch[70](270/704): Loss_D: 0.1581 Loss_G: 0.3767 MSE 103381.2812
===> Epoch[70](300/704): Loss_D: 0.2006 Loss_G: 0.3088 MSE 103279.2344
===> Epoch[70](330/704): Loss_D: 0.2019 Loss_G: 0.3745 MSE 100961.8828
===> Epoch[70](360/704): Loss_D: 0.2124 Loss_G: 0.2849 MSE 95949.8281
===> Epoch[70](390/704): Loss_D: 0.2165 Loss_G: 0.3836 MSE 101386.7891
===> Epoch[70](420/704): Loss_D: 0.1722 Loss_G: 0.3682 MSE 100692.3125
===> Epoch[70](450/704): Loss_D: 0.2038 Loss_G: 0.3598 MSE 106009.9844
===> Epoch[70](480/704): Loss_D: 0.2309 Loss_G: 0.3082 MSE 101411.7891
===> Epoch[70](510/704): Loss_D: 0.2641 Loss_G: 0.2996 MSE 104031.1250
===> Epoch[70](540/704): Loss_D: 0.2111 Loss_G: 0.3287 MSE 103387.6094
===> Epoch[70](570/704): Loss_D: 0.2196 Loss_G: 0.2816 MSE 103984.9219
===> Epoch[70](600/704): Loss_D: 0.1903 Loss_G: 0.3543 MSE 105466.3203
===> Epoch[70](630/704): Loss_D: 0.1991 Loss_G: 0.3116 MSE 99625.1016
===> Epoch[70](660/704): Loss_D: 0.1808 Loss_G: 0.4319 MSE 105720.2734
===> Epoch[70](690/704): Loss_D: 0.2121 Loss_G: 0.3312 MSE 107292.2188
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[71](0/704): Loss_D: 0.2519 Loss_G: 0.1974 MSE 102488.8281
===> Epoch[71](30/704): Loss_D: 0.2431 Loss_G: 0.3260 MSE 103758.5625
===> Epoch[71](60/704): Loss_D: 0.2122 Loss_G: 0.3219 MSE 99545.9453
===> Epoch[71](90/704): Loss_D: 0.2516 Loss_G: 0.2870 MSE 100227.3438
===> Epoch[71](120/704): Loss_D: 0.2205 Loss_G: 0.3338 MSE 99060.5156
===> Epoch[71](150/704): Loss_D: 0.2247 Loss_G: 0.3513 MSE 100158.3281
===> Epoch[71](180/704): Loss_D: 0.2114 Loss_G: 0.3853 MSE 99519.5703
===> Epoch[71](210/704): Loss_D: 0.2171 Loss_G: 0.2885 MSE 99644.4844
===> Epoch[71](240/704): Loss_D: 0.1805 Loss_G: 0.3570 MSE 96032.3203
===> Epoch[71](270/704): Loss_D: 0.2119 Loss_G: 0.3405 MSE 97454.3906
===> Epoch[71](300/704): Loss_D: 0.2092 Loss_G: 0.3207 MSE 102010.1172
===> Epoch[71](330/704): Loss_D: 0.2076 Loss_G: 0.3295 MSE 98442.7969
===> Epoch[71](360/704): Loss_D: 0.1916 Loss_G: 0.3386 MSE 100345.8438
===> Epoch[71](390/704): Loss_D: 0.2409 Loss_G: 0.3136 MSE 94810.4922
===> Epoch[71](420/704): Loss_D: 0.2250 Loss_G: 0.2989 MSE 95799.9688
===> Epoch[71](450/704): Loss_D: 0.2611 Loss_G: 0.2901 MSE 94366.7969
===> Epoch[71](480/704): Loss_D: 0.2034 Loss_G: 0.3604 MSE 98336.4531
===> Epoch[71](510/704): Loss_D: 0.1960 Loss_G: 0.3866 MSE 94278.2656
===> Epoch[71](540/704): Loss_D: 0.2114 Loss_G: 0.3995 MSE 96870.0938
===> Epoch[71](570/704): Loss_D: 0.2380 Loss_G: 0.3147 MSE 100772.0547
===> Epoch[71](600/704): Loss_D: 0.2006 Loss_G: 0.3812 MSE 106738.5547
===> Epoch[71](630/704): Loss_D: 0.2107 Loss_G: 0.3005 MSE 104788.7031
===> Epoch[71](660/704): Loss_D: 0.2167 Loss_G: 0.2943 MSE 103305.7500
===> Epoch[71](690/704): Loss_D: 0.1955 Loss_G: 0.2988 MSE 106937.8750
learning rate = 0.0001000
learning rate = 0.0001000
Saving
===> Epoch[72](0/704): Loss_D: 0.1486 Loss_G: 0.4375 MSE 107197.7656
===> Epoch[72](30/704): Loss_D: 0.2462 Loss_G: 0.3803 MSE 105262.0625
===> Epoch[72](60/704): Loss_D: 0.1800 Loss_G: 0.3673 MSE 107563.8047
===> Epoch[72](90/704): Loss_D: 0.1945 Loss_G: 0.3718 MSE 106427.8203
===> Epoch[72](120/704): Loss_D: 0.1439 Loss_G: 0.4721 MSE 106099.1562
===> Epoch[72](150/704): Loss_D: 0.1914 Loss_G: 0.4020 MSE 109927.6016
===> Epoch[72](180/704): Loss_D: 0.1878 Loss_G: 0.3756 MSE 105574.5625
===> Epoch[72](210/704): Loss_D: 0.1993 Loss_G: 0.3434 MSE 104151.0000
===> Epoch[72](240/704): Loss_D: 0.1898 Loss_G: 0.3489 MSE 101164.9219
===> Epoch[72](270/704): Loss_D: 0.2196 Loss_G: 0.3315 MSE 103798.4062
===> Epoch[72](300/704): Loss_D: 0.2153 Loss_G: 0.3040 MSE 103968.9844
===> Epoch[72](330/704): Loss_D: 0.2914 Loss_G: 0.2909 MSE 105226.4297
===> Epoch[72](360/704): Loss_D: 0.3198 Loss_G: 0.2401 MSE 111742.1172
===> Epoch[72](390/704): Loss_D: 0.2140 Loss_G: 0.3549 MSE 105507.3594
===> Epoch[72](420/704): Loss_D: 0.1971 Loss_G: 0.3468 MSE 111242.5312
===> Epoch[72](450/704): Loss_D: 0.2537 Loss_G: 0.2720 MSE 103297.4688
===> Epoch[72](480/704): Loss_D: 0.2037 Loss_G: 0.3025 MSE 106947.1797
===> Epoch[72](510/704): Loss_D: 0.2165 Loss_G: 0.3458 MSE 104579.5078
===> Epoch[72](540/704): Loss_D: 0.2122 Loss_G: 0.3940 MSE 104752.6250
===> Epoch[72](570/704): Loss_D: 0.2373 Loss_G: 0.2956 MSE 102898.6406
===> Epoch[72](600/704): Loss_D: 0.2377 Loss_G: 0.3163 MSE 105286.6094
===> Epoch[72](630/704): Loss_D: 0.2266 Loss_G: 0.3140 MSE 102588.9453
===> Epoch[72](660/704): Loss_D: 0.2236 Loss_G: 0.3374 MSE 102661.4844
===> Epoch[72](690/704): Loss_D: 0.1814 Loss_G: 0.3844 MSE 104919.6094
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[73](0/704): Loss_D: 0.2351 Loss_G: 0.2531 MSE 102194.8906
===> Epoch[73](30/704): Loss_D: 0.2421 Loss_G: 0.2849 MSE 105144.1875
===> Epoch[73](60/704): Loss_D: 0.2279 Loss_G: 0.3254 MSE 104230.9609
===> Epoch[73](90/704): Loss_D: 0.2537 Loss_G: 0.3326 MSE 100761.9062
===> Epoch[73](120/704): Loss_D: 0.2214 Loss_G: 0.3745 MSE 100726.8750
===> Epoch[73](150/704): Loss_D: 0.1988 Loss_G: 0.3457 MSE 97223.7891
===> Epoch[73](180/704): Loss_D: 0.2450 Loss_G: 0.3383 MSE 98774.5781
===> Epoch[73](210/704): Loss_D: 0.2069 Loss_G: 0.3787 MSE 98140.5156
===> Epoch[73](240/704): Loss_D: 0.2560 Loss_G: 0.3049 MSE 99322.9688
===> Epoch[73](270/704): Loss_D: 0.2813 Loss_G: 0.2597 MSE 99778.6250
===> Epoch[73](300/704): Loss_D: 0.1895 Loss_G: 0.3294 MSE 104772.8750
===> Epoch[73](330/704): Loss_D: 0.2025 Loss_G: 0.3015 MSE 99688.7812
===> Epoch[73](360/704): Loss_D: 0.2097 Loss_G: 0.2993 MSE 98499.0625
===> Epoch[73](390/704): Loss_D: 0.1928 Loss_G: 0.3984 MSE 100385.9844
===> Epoch[73](420/704): Loss_D: 0.2393 Loss_G: 0.3108 MSE 101007.5312
===> Epoch[73](450/704): Loss_D: 0.2320 Loss_G: 0.3084 MSE 100893.8594
===> Epoch[73](480/704): Loss_D: 0.2171 Loss_G: 0.3354 MSE 98508.9375
===> Epoch[73](510/704): Loss_D: 0.2374 Loss_G: 0.2887 MSE 100829.0469
===> Epoch[73](540/704): Loss_D: 0.2022 Loss_G: 0.3313 MSE 99164.7969
===> Epoch[73](570/704): Loss_D: 0.2301 Loss_G: 0.2643 MSE 104278.3203
===> Epoch[73](600/704): Loss_D: 0.2076 Loss_G: 0.3379 MSE 101451.8125
===> Epoch[73](630/704): Loss_D: 0.2281 Loss_G: 0.3153 MSE 99484.3047
===> Epoch[73](660/704): Loss_D: 0.2398 Loss_G: 0.3062 MSE 101106.9062
===> Epoch[73](690/704): Loss_D: 0.2239 Loss_G: 0.3346 MSE 93683.1406
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[74](0/704): Loss_D: 0.2018 Loss_G: 0.3043 MSE 104606.0938
===> Epoch[74](30/704): Loss_D: 0.2345 Loss_G: 0.2910 MSE 99103.1406
===> Epoch[74](60/704): Loss_D: 0.2470 Loss_G: 0.2769 MSE 100619.0156
===> Epoch[74](90/704): Loss_D: 0.2218 Loss_G: 0.3035 MSE 98317.5312
===> Epoch[74](120/704): Loss_D: 0.2463 Loss_G: 0.2864 MSE 98872.6953
===> Epoch[74](150/704): Loss_D: 0.2005 Loss_G: 0.3827 MSE 99905.0312
===> Epoch[74](180/704): Loss_D: 0.1874 Loss_G: 0.3595 MSE 100109.4531
===> Epoch[74](210/704): Loss_D: 0.2140 Loss_G: 0.3206 MSE 99912.0156
===> Epoch[74](240/704): Loss_D: 0.2567 Loss_G: 0.3037 MSE 99486.2812
===> Epoch[74](270/704): Loss_D: 0.2749 Loss_G: 0.2740 MSE 101234.1875
===> Epoch[74](300/704): Loss_D: 0.1878 Loss_G: 0.4152 MSE 96043.5469
===> Epoch[74](330/704): Loss_D: 0.2175 Loss_G: 0.3402 MSE 96840.6719
===> Epoch[74](360/704): Loss_D: 0.2621 Loss_G: 0.2663 MSE 96408.3672
===> Epoch[74](390/704): Loss_D: 0.1803 Loss_G: 0.3874 MSE 96080.7500
===> Epoch[74](420/704): Loss_D: 0.2349 Loss_G: 0.2932 MSE 104649.8672
===> Epoch[74](450/704): Loss_D: 0.2024 Loss_G: 0.3440 MSE 100230.6719
===> Epoch[74](480/704): Loss_D: 0.2077 Loss_G: 0.3267 MSE 98989.6328
===> Epoch[74](510/704): Loss_D: 0.1896 Loss_G: 0.4193 MSE 102440.5703
===> Epoch[74](540/704): Loss_D: 0.1819 Loss_G: 0.3785 MSE 97841.5312
===> Epoch[74](570/704): Loss_D: 0.1482 Loss_G: 0.4522 MSE 100603.3516
===> Epoch[74](600/704): Loss_D: 0.1859 Loss_G: 0.3480 MSE 97814.7812
===> Epoch[74](630/704): Loss_D: 0.1510 Loss_G: 0.4070 MSE 98367.0078
===> Epoch[74](660/704): Loss_D: 0.1677 Loss_G: 0.4142 MSE 93721.4531
===> Epoch[74](690/704): Loss_D: 0.1498 Loss_G: 0.3883 MSE 98738.4766
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[75](0/704): Loss_D: 0.1810 Loss_G: 0.2855 MSE 99461.0781
===> Epoch[75](30/704): Loss_D: 0.1643 Loss_G: 0.4258 MSE 100360.7109
===> Epoch[75](60/704): Loss_D: 0.1989 Loss_G: 0.3778 MSE 102622.6016
===> Epoch[75](90/704): Loss_D: 0.1672 Loss_G: 0.3793 MSE 99784.3828
===> Epoch[75](120/704): Loss_D: 0.2047 Loss_G: 0.3524 MSE 103666.2344
===> Epoch[75](150/704): Loss_D: 0.1919 Loss_G: 0.3557 MSE 98041.1484
===> Epoch[75](180/704): Loss_D: 0.1607 Loss_G: 0.3901 MSE 100580.4453
===> Epoch[75](210/704): Loss_D: 0.2624 Loss_G: 0.3553 MSE 103384.6094
===> Epoch[75](240/704): Loss_D: 0.2425 Loss_G: 0.3080 MSE 96847.9062
===> Epoch[75](270/704): Loss_D: 0.1938 Loss_G: 0.3401 MSE 98278.5703
===> Epoch[75](300/704): Loss_D: 0.1950 Loss_G: 0.3309 MSE 103335.6562
===> Epoch[75](330/704): Loss_D: 0.1710 Loss_G: 0.3866 MSE 97462.0000
===> Epoch[75](360/704): Loss_D: 0.1729 Loss_G: 0.3700 MSE 101735.5312
===> Epoch[75](390/704): Loss_D: 0.2326 Loss_G: 0.3513 MSE 91410.1562
===> Epoch[75](420/704): Loss_D: 0.2024 Loss_G: 0.3661 MSE 90580.5000
===> Epoch[75](450/704): Loss_D: 0.1762 Loss_G: 0.3291 MSE 96944.1406
===> Epoch[75](480/704): Loss_D: 0.1955 Loss_G: 0.4008 MSE 95537.4062
===> Epoch[75](510/704): Loss_D: 0.1795 Loss_G: 0.4035 MSE 101102.3281
===> Epoch[75](540/704): Loss_D: 0.1894 Loss_G: 0.3737 MSE 103064.3750
===> Epoch[75](570/704): Loss_D: 0.1495 Loss_G: 0.4702 MSE 103626.8359
===> Epoch[75](600/704): Loss_D: 0.1669 Loss_G: 0.5138 MSE 98124.5703
===> Epoch[75](630/704): Loss_D: 0.2248 Loss_G: 0.3607 MSE 101015.6875
===> Epoch[75](660/704): Loss_D: 0.1561 Loss_G: 0.4328 MSE 105798.7344
===> Epoch[75](690/704): Loss_D: 0.2348 Loss_G: 0.3232 MSE 101347.1719
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[76](0/704): Loss_D: 0.2068 Loss_G: 0.3081 MSE 98516.8281
===> Epoch[76](30/704): Loss_D: 0.1839 Loss_G: 0.3794 MSE 97721.4844
===> Epoch[76](60/704): Loss_D: 0.1573 Loss_G: 0.6004 MSE 98705.2188
===> Epoch[76](90/704): Loss_D: 0.2224 Loss_G: 0.4010 MSE 98306.4375
===> Epoch[76](120/704): Loss_D: 0.2105 Loss_G: 0.3454 MSE 99484.9141
===> Epoch[76](150/704): Loss_D: 0.1079 Loss_G: 0.5876 MSE 97879.9844
===> Epoch[76](180/704): Loss_D: 0.1411 Loss_G: 0.4986 MSE 101933.6016
===> Epoch[76](210/704): Loss_D: 0.1162 Loss_G: 0.4899 MSE 101065.0156
===> Epoch[76](240/704): Loss_D: 0.1095 Loss_G: 0.4787 MSE 106011.1172
===> Epoch[76](270/704): Loss_D: 0.1750 Loss_G: 0.3395 MSE 99252.4375
===> Epoch[76](300/704): Loss_D: 0.1796 Loss_G: 0.4193 MSE 100996.6875
===> Epoch[76](330/704): Loss_D: 0.1461 Loss_G: 0.4094 MSE 94765.1562
===> Epoch[76](360/704): Loss_D: 0.2003 Loss_G: 0.4872 MSE 95268.4844
===> Epoch[76](390/704): Loss_D: 0.2473 Loss_G: 0.3483 MSE 95104.2656
===> Epoch[76](420/704): Loss_D: 0.1690 Loss_G: 0.3961 MSE 100868.5781
===> Epoch[76](450/704): Loss_D: 0.1544 Loss_G: 0.3980 MSE 100578.0312
===> Epoch[76](480/704): Loss_D: 0.1850 Loss_G: 0.4079 MSE 103502.5469
===> Epoch[76](510/704): Loss_D: 0.1357 Loss_G: 0.4927 MSE 103867.3594
===> Epoch[76](540/704): Loss_D: 0.1551 Loss_G: 0.4733 MSE 105143.3906
===> Epoch[76](570/704): Loss_D: 0.1783 Loss_G: 0.4175 MSE 100917.3594
===> Epoch[76](600/704): Loss_D: 0.1501 Loss_G: 0.4250 MSE 104354.1406
===> Epoch[76](630/704): Loss_D: 0.1954 Loss_G: 0.4917 MSE 100478.5000
===> Epoch[76](660/704): Loss_D: 0.3017 Loss_G: 0.3384 MSE 103676.2969
===> Epoch[76](690/704): Loss_D: 0.1614 Loss_G: 0.3455 MSE 103258.5703
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[77](0/704): Loss_D: 0.2718 Loss_G: 0.1725 MSE 101305.1406
===> Epoch[77](30/704): Loss_D: 0.1307 Loss_G: 0.3848 MSE 99456.4844
===> Epoch[77](60/704): Loss_D: 0.2034 Loss_G: 0.3459 MSE 102244.8203
===> Epoch[77](90/704): Loss_D: 0.1427 Loss_G: 0.4213 MSE 103578.5938
===> Epoch[77](120/704): Loss_D: 0.0828 Loss_G: 0.6504 MSE 101356.0312
===> Epoch[77](150/704): Loss_D: 0.1550 Loss_G: 0.3923 MSE 103665.7031
===> Epoch[77](180/704): Loss_D: 0.2073 Loss_G: 0.3170 MSE 103351.0703
===> Epoch[77](210/704): Loss_D: 0.1837 Loss_G: 0.3955 MSE 104567.6875
===> Epoch[77](240/704): Loss_D: 0.1642 Loss_G: 0.4205 MSE 104226.9062
===> Epoch[77](270/704): Loss_D: 0.1377 Loss_G: 0.4716 MSE 104798.8906
===> Epoch[77](300/704): Loss_D: 0.1426 Loss_G: 0.4042 MSE 99379.8125
===> Epoch[77](330/704): Loss_D: 0.2060 Loss_G: 0.3743 MSE 104627.6250
===> Epoch[77](360/704): Loss_D: 0.2264 Loss_G: 0.3644 MSE 105075.4844
===> Epoch[77](390/704): Loss_D: 0.1872 Loss_G: 0.3694 MSE 104784.3125
===> Epoch[77](420/704): Loss_D: 0.1653 Loss_G: 0.4242 MSE 99769.5156
===> Epoch[77](450/704): Loss_D: 0.3598 Loss_G: 0.2818 MSE 102179.2891
===> Epoch[77](480/704): Loss_D: 0.2084 Loss_G: 0.3492 MSE 103343.2656
===> Epoch[77](510/704): Loss_D: 0.1476 Loss_G: 0.4073 MSE 102585.2812
===> Epoch[77](540/704): Loss_D: 0.2146 Loss_G: 0.3648 MSE 109706.2812
===> Epoch[77](570/704): Loss_D: 0.1878 Loss_G: 0.4126 MSE 97101.7969
===> Epoch[77](600/704): Loss_D: 0.2127 Loss_G: 0.3459 MSE 100341.5000
===> Epoch[77](630/704): Loss_D: 0.2843 Loss_G: 0.3873 MSE 95374.3672
===> Epoch[77](660/704): Loss_D: 0.2260 Loss_G: 0.3632 MSE 97061.9766
===> Epoch[77](690/704): Loss_D: 0.1599 Loss_G: 0.4749 MSE 96546.4375
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[78](0/704): Loss_D: 0.1822 Loss_G: 0.2222 MSE 93720.8906
===> Epoch[78](30/704): Loss_D: 0.2641 Loss_G: 0.2971 MSE 96405.8672
===> Epoch[78](60/704): Loss_D: 0.2891 Loss_G: 0.2581 MSE 95490.2422
===> Epoch[78](90/704): Loss_D: 0.2344 Loss_G: 0.3885 MSE 97804.4688
===> Epoch[78](120/704): Loss_D: 0.2506 Loss_G: 0.2991 MSE 91766.7656
===> Epoch[78](150/704): Loss_D: 0.1465 Loss_G: 0.5109 MSE 96496.0156
===> Epoch[78](180/704): Loss_D: 0.2018 Loss_G: 0.3666 MSE 103697.9219
===> Epoch[78](210/704): Loss_D: 0.2624 Loss_G: 0.2797 MSE 92939.7734
===> Epoch[78](240/704): Loss_D: 0.1850 Loss_G: 0.4003 MSE 98269.0781
===> Epoch[78](270/704): Loss_D: 0.1934 Loss_G: 0.3534 MSE 93831.9609
===> Epoch[78](300/704): Loss_D: 0.1672 Loss_G: 0.4065 MSE 93548.6250
===> Epoch[78](330/704): Loss_D: 0.1742 Loss_G: 0.5359 MSE 88970.1406
===> Epoch[78](360/704): Loss_D: 0.1993 Loss_G: 0.3811 MSE 98420.2188
===> Epoch[78](390/704): Loss_D: 0.2215 Loss_G: 0.3115 MSE 99801.9375
===> Epoch[78](420/704): Loss_D: 0.2094 Loss_G: 0.3366 MSE 95543.5938
===> Epoch[78](450/704): Loss_D: 0.1962 Loss_G: 0.3663 MSE 98412.8203
===> Epoch[78](480/704): Loss_D: 0.2318 Loss_G: 0.2918 MSE 94383.7812
===> Epoch[78](510/704): Loss_D: 0.1926 Loss_G: 0.3414 MSE 97507.8203
===> Epoch[78](540/704): Loss_D: 0.1828 Loss_G: 0.3606 MSE 93222.5938
===> Epoch[78](570/704): Loss_D: 0.2077 Loss_G: 0.2850 MSE 92661.8359
===> Epoch[78](600/704): Loss_D: 0.2459 Loss_G: 0.2904 MSE 96788.9688
===> Epoch[78](630/704): Loss_D: 0.2340 Loss_G: 0.3358 MSE 93622.9531
===> Epoch[78](660/704): Loss_D: 0.2649 Loss_G: 0.3070 MSE 96376.7188
===> Epoch[78](690/704): Loss_D: 0.2252 Loss_G: 0.3196 MSE 92681.8359
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[79](0/704): Loss_D: 0.2407 Loss_G: 0.2777 MSE 96349.5547
===> Epoch[79](30/704): Loss_D: 0.2483 Loss_G: 0.3274 MSE 93255.2812
===> Epoch[79](60/704): Loss_D: 0.2171 Loss_G: 0.3278 MSE 96638.9531
===> Epoch[79](90/704): Loss_D: 0.2463 Loss_G: 0.2927 MSE 95721.5312
===> Epoch[79](120/704): Loss_D: 0.2591 Loss_G: 0.2878 MSE 97302.0156
===> Epoch[79](150/704): Loss_D: 0.2330 Loss_G: 0.2878 MSE 95172.3438
===> Epoch[79](180/704): Loss_D: 0.2180 Loss_G: 0.3245 MSE 94838.2109
===> Epoch[79](210/704): Loss_D: 0.1753 Loss_G: 0.3606 MSE 99408.3125
===> Epoch[79](240/704): Loss_D: 0.2303 Loss_G: 0.2891 MSE 96717.8438
===> Epoch[79](270/704): Loss_D: 0.1937 Loss_G: 0.3420 MSE 96220.8359
===> Epoch[79](300/704): Loss_D: 0.1950 Loss_G: 0.3421 MSE 92804.6406
===> Epoch[79](330/704): Loss_D: 0.2033 Loss_G: 0.3357 MSE 98546.0156
===> Epoch[79](360/704): Loss_D: 0.3044 Loss_G: 0.2975 MSE 96747.3594
===> Epoch[79](390/704): Loss_D: 0.2590 Loss_G: 0.2944 MSE 99280.0469
===> Epoch[79](420/704): Loss_D: 0.2639 Loss_G: 0.3060 MSE 96902.2188
===> Epoch[79](450/704): Loss_D: 0.2385 Loss_G: 0.3260 MSE 96242.8438
===> Epoch[79](480/704): Loss_D: 0.2549 Loss_G: 0.3548 MSE 97307.1562
===> Epoch[79](510/704): Loss_D: 0.2541 Loss_G: 0.3199 MSE 98615.5156
===> Epoch[79](540/704): Loss_D: 0.2045 Loss_G: 0.3240 MSE 97356.6875
===> Epoch[79](570/704): Loss_D: 0.2256 Loss_G: 0.3273 MSE 100172.3984
===> Epoch[79](600/704): Loss_D: 0.2001 Loss_G: 0.3285 MSE 101527.2031
===> Epoch[79](630/704): Loss_D: 0.2264 Loss_G: 0.2793 MSE 98952.0312
===> Epoch[79](660/704): Loss_D: 0.2411 Loss_G: 0.2697 MSE 98362.3750
===> Epoch[79](690/704): Loss_D: 0.2381 Loss_G: 0.3370 MSE 102389.1875
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[80](0/704): Loss_D: 0.2341 Loss_G: 0.2180 MSE 103467.2422
===> Epoch[80](30/704): Loss_D: 0.1836 Loss_G: 0.3569 MSE 98440.6094
===> Epoch[80](60/704): Loss_D: 0.2110 Loss_G: 0.3483 MSE 94670.9219
===> Epoch[80](90/704): Loss_D: 0.2234 Loss_G: 0.3192 MSE 98051.1719
===> Epoch[80](120/704): Loss_D: 0.2259 Loss_G: 0.3754 MSE 97993.5156
===> Epoch[80](150/704): Loss_D: 0.1872 Loss_G: 0.3268 MSE 102071.0938
===> Epoch[80](180/704): Loss_D: 0.2063 Loss_G: 0.3468 MSE 98198.4766
===> Epoch[80](210/704): Loss_D: 0.1464 Loss_G: 0.5241 MSE 98649.0781
===> Epoch[80](240/704): Loss_D: 0.2129 Loss_G: 0.3885 MSE 99108.9609
===> Epoch[80](270/704): Loss_D: 0.1666 Loss_G: 0.3633 MSE 94362.5547
===> Epoch[80](300/704): Loss_D: 0.2035 Loss_G: 0.3438 MSE 94855.9688
===> Epoch[80](330/704): Loss_D: 0.2159 Loss_G: 0.3207 MSE 102755.0156
===> Epoch[80](360/704): Loss_D: 0.2464 Loss_G: 0.2973 MSE 97073.1719
===> Epoch[80](390/704): Loss_D: 0.1970 Loss_G: 0.3419 MSE 95458.8125
===> Epoch[80](420/704): Loss_D: 0.2564 Loss_G: 0.2717 MSE 100329.2031
===> Epoch[80](450/704): Loss_D: 0.1915 Loss_G: 0.3064 MSE 94845.3750
===> Epoch[80](480/704): Loss_D: 0.2447 Loss_G: 0.2597 MSE 97804.1719
===> Epoch[80](510/704): Loss_D: 0.2436 Loss_G: 0.3198 MSE 97749.7109
===> Epoch[80](540/704): Loss_D: 0.2262 Loss_G: 0.3134 MSE 92514.2812
===> Epoch[80](570/704): Loss_D: 0.1601 Loss_G: 0.4372 MSE 97251.3281
===> Epoch[80](600/704): Loss_D: 0.1911 Loss_G: 0.3985 MSE 94666.9375
===> Epoch[80](630/704): Loss_D: 0.2641 Loss_G: 0.2948 MSE 95137.2031
===> Epoch[80](660/704): Loss_D: 0.2055 Loss_G: 0.3759 MSE 95394.4141
===> Epoch[80](690/704): Loss_D: 0.2336 Loss_G: 0.3973 MSE 102280.1797
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[81](0/704): Loss_D: 0.2612 Loss_G: 0.2054 MSE 103276.0781
===> Epoch[81](30/704): Loss_D: 0.2074 Loss_G: 0.3416 MSE 99714.7500
===> Epoch[81](60/704): Loss_D: 0.1857 Loss_G: 0.3980 MSE 96581.2188
===> Epoch[81](90/704): Loss_D: 0.2024 Loss_G: 0.3136 MSE 99606.1797
===> Epoch[81](120/704): Loss_D: 0.2944 Loss_G: 0.2548 MSE 98623.0781
===> Epoch[81](150/704): Loss_D: 0.2667 Loss_G: 0.3130 MSE 96266.5000
===> Epoch[81](180/704): Loss_D: 0.2397 Loss_G: 0.3436 MSE 91950.0000
===> Epoch[81](210/704): Loss_D: 0.2286 Loss_G: 0.3197 MSE 99721.9609
===> Epoch[81](240/704): Loss_D: 0.1548 Loss_G: 0.4093 MSE 98429.7188
===> Epoch[81](270/704): Loss_D: 0.2234 Loss_G: 0.3120 MSE 96384.9219
===> Epoch[81](300/704): Loss_D: 0.2450 Loss_G: 0.3378 MSE 98160.9062
===> Epoch[81](330/704): Loss_D: 0.1706 Loss_G: 0.4596 MSE 98883.1719
===> Epoch[81](360/704): Loss_D: 0.2415 Loss_G: 0.3730 MSE 98013.8828
===> Epoch[81](390/704): Loss_D: 0.2302 Loss_G: 0.2911 MSE 99234.5312
===> Epoch[81](420/704): Loss_D: 0.2536 Loss_G: 0.3081 MSE 94556.7891
===> Epoch[81](450/704): Loss_D: 0.2789 Loss_G: 0.2428 MSE 99080.4062
===> Epoch[81](480/704): Loss_D: 0.2922 Loss_G: 0.3102 MSE 99137.5469
===> Epoch[81](510/704): Loss_D: 0.2201 Loss_G: 0.2807 MSE 100681.2578
===> Epoch[81](540/704): Loss_D: 0.2075 Loss_G: 0.3318 MSE 99210.5156
===> Epoch[81](570/704): Loss_D: 0.2085 Loss_G: 0.3321 MSE 96360.7031
===> Epoch[81](600/704): Loss_D: 0.1944 Loss_G: 0.4196 MSE 93791.7031
===> Epoch[81](630/704): Loss_D: 0.2601 Loss_G: 0.2626 MSE 99417.0234
===> Epoch[81](660/704): Loss_D: 0.2484 Loss_G: 0.3166 MSE 96882.9531
===> Epoch[81](690/704): Loss_D: 0.2074 Loss_G: 0.3676 MSE 99365.6484
learning rate = 0.0001000
learning rate = 0.0001000
Saving
===> Epoch[82](0/704): Loss_D: 0.2434 Loss_G: 0.2402 MSE 101967.0156
===> Epoch[82](30/704): Loss_D: 0.2363 Loss_G: 0.3421 MSE 96162.3984
===> Epoch[82](60/704): Loss_D: 0.2087 Loss_G: 0.3393 MSE 93510.2344
===> Epoch[82](90/704): Loss_D: 0.2109 Loss_G: 0.3457 MSE 97592.2969
===> Epoch[82](120/704): Loss_D: 0.2216 Loss_G: 0.3259 MSE 95377.5625
===> Epoch[82](150/704): Loss_D: 0.2440 Loss_G: 0.2841 MSE 95487.0938
===> Epoch[82](180/704): Loss_D: 0.2108 Loss_G: 0.3443 MSE 100337.6484
===> Epoch[82](210/704): Loss_D: 0.2327 Loss_G: 0.3262 MSE 100037.8125
===> Epoch[82](240/704): Loss_D: 0.2767 Loss_G: 0.2708 MSE 99636.3594
===> Epoch[82](270/704): Loss_D: 0.2309 Loss_G: 0.2904 MSE 98701.3594
===> Epoch[82](300/704): Loss_D: 0.2393 Loss_G: 0.3392 MSE 99773.0469
===> Epoch[82](330/704): Loss_D: 0.1757 Loss_G: 0.3759 MSE 92946.4141
===> Epoch[82](360/704): Loss_D: 0.1901 Loss_G: 0.3419 MSE 98498.2344
===> Epoch[82](390/704): Loss_D: 0.2471 Loss_G: 0.2752 MSE 97431.0156
===> Epoch[82](420/704): Loss_D: 0.2379 Loss_G: 0.3076 MSE 100037.8516
===> Epoch[82](450/704): Loss_D: 0.2289 Loss_G: 0.2983 MSE 96013.2656
===> Epoch[82](480/704): Loss_D: 0.1562 Loss_G: 0.3910 MSE 99350.0625
===> Epoch[82](510/704): Loss_D: 0.2298 Loss_G: 0.3264 MSE 100466.4922
===> Epoch[82](540/704): Loss_D: 0.2135 Loss_G: 0.2995 MSE 103497.0000
===> Epoch[82](570/704): Loss_D: 0.2608 Loss_G: 0.2829 MSE 99861.4922
===> Epoch[82](600/704): Loss_D: 0.3115 Loss_G: 0.2412 MSE 97683.3828
===> Epoch[82](630/704): Loss_D: 0.2285 Loss_G: 0.2700 MSE 97992.3281
===> Epoch[82](660/704): Loss_D: 0.2462 Loss_G: 0.2628 MSE 100261.1562
===> Epoch[82](690/704): Loss_D: 0.2240 Loss_G: 0.2598 MSE 98773.3125
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[83](0/704): Loss_D: 0.2658 Loss_G: 0.2552 MSE 96626.1250
===> Epoch[83](30/704): Loss_D: 0.2650 Loss_G: 0.3196 MSE 98199.2344
===> Epoch[83](60/704): Loss_D: 0.1925 Loss_G: 0.3675 MSE 99028.0625
===> Epoch[83](90/704): Loss_D: 0.2786 Loss_G: 0.2883 MSE 98374.1719
===> Epoch[83](120/704): Loss_D: 0.2171 Loss_G: 0.3646 MSE 100651.5156
===> Epoch[83](150/704): Loss_D: 0.2461 Loss_G: 0.2885 MSE 100028.0625
===> Epoch[83](180/704): Loss_D: 0.1884 Loss_G: 0.3524 MSE 105058.7656
===> Epoch[83](210/704): Loss_D: 0.2574 Loss_G: 0.2815 MSE 101354.4609
===> Epoch[83](240/704): Loss_D: 0.2959 Loss_G: 0.2192 MSE 101066.0000
===> Epoch[83](270/704): Loss_D: 0.2561 Loss_G: 0.2878 MSE 97561.2812
===> Epoch[83](300/704): Loss_D: 0.2463 Loss_G: 0.2987 MSE 100263.1953
===> Epoch[83](330/704): Loss_D: 0.2448 Loss_G: 0.3205 MSE 99218.7031
===> Epoch[83](360/704): Loss_D: 0.1672 Loss_G: 0.3773 MSE 98907.4219
===> Epoch[83](390/704): Loss_D: 0.2214 Loss_G: 0.3536 MSE 98417.9062
===> Epoch[83](420/704): Loss_D: 0.2552 Loss_G: 0.3159 MSE 98244.5625
===> Epoch[83](450/704): Loss_D: 0.2164 Loss_G: 0.3325 MSE 101815.1719
===> Epoch[83](480/704): Loss_D: 0.2408 Loss_G: 0.3532 MSE 99545.8438
===> Epoch[83](510/704): Loss_D: 0.2144 Loss_G: 0.3192 MSE 99682.4062
===> Epoch[83](540/704): Loss_D: 0.2508 Loss_G: 0.3075 MSE 101210.1094
===> Epoch[83](570/704): Loss_D: 0.2075 Loss_G: 0.3476 MSE 100302.4375
===> Epoch[83](600/704): Loss_D: 0.1795 Loss_G: 0.3065 MSE 103529.3281
===> Epoch[83](630/704): Loss_D: 0.1868 Loss_G: 0.3911 MSE 103063.8438
===> Epoch[83](660/704): Loss_D: 0.2130 Loss_G: 0.3213 MSE 102004.0234
===> Epoch[83](690/704): Loss_D: 0.2376 Loss_G: 0.2889 MSE 104371.6641
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[84](0/704): Loss_D: 0.1279 Loss_G: 0.3623 MSE 103890.3594
===> Epoch[84](30/704): Loss_D: 0.2554 Loss_G: 0.2722 MSE 107226.2969
===> Epoch[84](60/704): Loss_D: 0.2102 Loss_G: 0.2792 MSE 100628.1875
===> Epoch[84](90/704): Loss_D: 0.2738 Loss_G: 0.3291 MSE 104412.2656
===> Epoch[84](120/704): Loss_D: 0.2235 Loss_G: 0.2933 MSE 103552.2188
===> Epoch[84](150/704): Loss_D: 0.2458 Loss_G: 0.2433 MSE 103720.1094
===> Epoch[84](180/704): Loss_D: 0.2476 Loss_G: 0.3079 MSE 105036.1562
===> Epoch[84](210/704): Loss_D: 0.2763 Loss_G: 0.2772 MSE 103605.1328
===> Epoch[84](240/704): Loss_D: 0.2075 Loss_G: 0.3336 MSE 107126.1719
===> Epoch[84](270/704): Loss_D: 0.2725 Loss_G: 0.2224 MSE 104792.2266
===> Epoch[84](300/704): Loss_D: 0.1792 Loss_G: 0.3833 MSE 103798.4141
===> Epoch[84](330/704): Loss_D: 0.1874 Loss_G: 0.3752 MSE 100863.6094
===> Epoch[84](360/704): Loss_D: 0.2216 Loss_G: 0.3388 MSE 105919.7656
===> Epoch[84](390/704): Loss_D: 0.2379 Loss_G: 0.2696 MSE 102917.9609
===> Epoch[84](420/704): Loss_D: 0.1642 Loss_G: 0.3693 MSE 105572.0469
===> Epoch[84](450/704): Loss_D: 0.2028 Loss_G: 0.3543 MSE 102646.0312
===> Epoch[84](480/704): Loss_D: 0.1907 Loss_G: 0.3272 MSE 102135.0234
===> Epoch[84](510/704): Loss_D: 0.2297 Loss_G: 0.3147 MSE 103509.1797
===> Epoch[84](540/704): Loss_D: 0.2582 Loss_G: 0.3045 MSE 100110.5000
===> Epoch[84](570/704): Loss_D: 0.2318 Loss_G: 0.2792 MSE 102680.0156
===> Epoch[84](600/704): Loss_D: 0.2074 Loss_G: 0.3330 MSE 103753.6562
===> Epoch[84](630/704): Loss_D: 0.2286 Loss_G: 0.3026 MSE 102836.9844
===> Epoch[84](660/704): Loss_D: 0.1854 Loss_G: 0.3758 MSE 103105.9375
===> Epoch[84](690/704): Loss_D: 0.2873 Loss_G: 0.2880 MSE 101262.9375
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[85](0/704): Loss_D: 0.2768 Loss_G: 0.2178 MSE 104735.2969
===> Epoch[85](30/704): Loss_D: 0.2576 Loss_G: 0.2641 MSE 102128.2344
===> Epoch[85](60/704): Loss_D: 0.2139 Loss_G: 0.2903 MSE 100810.2891
===> Epoch[85](90/704): Loss_D: 0.2467 Loss_G: 0.2939 MSE 105527.1484
===> Epoch[85](120/704): Loss_D: 0.2290 Loss_G: 0.3291 MSE 100516.3984
===> Epoch[85](150/704): Loss_D: 0.2162 Loss_G: 0.3782 MSE 104077.9609
===> Epoch[85](180/704): Loss_D: 0.2083 Loss_G: 0.4065 MSE 103394.4062
===> Epoch[85](210/704): Loss_D: 0.2088 Loss_G: 0.3145 MSE 100840.6953
===> Epoch[85](240/704): Loss_D: 0.2235 Loss_G: 0.3266 MSE 100069.0547
===> Epoch[85](270/704): Loss_D: 0.2092 Loss_G: 0.3183 MSE 100700.4062
===> Epoch[85](300/704): Loss_D: 0.1951 Loss_G: 0.3701 MSE 100602.7812
===> Epoch[85](330/704): Loss_D: 0.1941 Loss_G: 0.3351 MSE 98550.5703
===> Epoch[85](360/704): Loss_D: 0.2067 Loss_G: 0.3095 MSE 97357.3594
===> Epoch[85](390/704): Loss_D: 0.2225 Loss_G: 0.3165 MSE 102143.6719
===> Epoch[85](420/704): Loss_D: 0.2256 Loss_G: 0.3208 MSE 98922.9922
===> Epoch[85](450/704): Loss_D: 0.2071 Loss_G: 0.3383 MSE 97043.1953
===> Epoch[85](480/704): Loss_D: 0.2090 Loss_G: 0.3402 MSE 94900.3359
===> Epoch[85](510/704): Loss_D: 0.2468 Loss_G: 0.3531 MSE 103130.1562
===> Epoch[85](540/704): Loss_D: 0.2564 Loss_G: 0.2874 MSE 98506.2188
===> Epoch[85](570/704): Loss_D: 0.2484 Loss_G: 0.2663 MSE 105522.1719
===> Epoch[85](600/704): Loss_D: 0.2488 Loss_G: 0.3634 MSE 102641.9531
===> Epoch[85](630/704): Loss_D: 0.1546 Loss_G: 0.3987 MSE 99525.5000
===> Epoch[85](660/704): Loss_D: 0.2382 Loss_G: 0.2943 MSE 101496.5000
===> Epoch[85](690/704): Loss_D: 0.2359 Loss_G: 0.2883 MSE 97999.8594
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[86](0/704): Loss_D: 0.1762 Loss_G: 1.1752 MSE 93576.1172
===> Epoch[86](30/704): Loss_D: 0.1837 Loss_G: 0.3496 MSE 95522.2188
===> Epoch[86](60/704): Loss_D: 0.1935 Loss_G: 0.4072 MSE 100289.3594
===> Epoch[86](90/704): Loss_D: 0.2151 Loss_G: 0.3139 MSE 98297.3672
===> Epoch[86](120/704): Loss_D: 0.2448 Loss_G: 0.2888 MSE 99270.3828
===> Epoch[86](150/704): Loss_D: 0.2556 Loss_G: 0.4727 MSE 97710.8594
===> Epoch[86](180/704): Loss_D: 0.2282 Loss_G: 0.3478 MSE 94764.3125
===> Epoch[86](210/704): Loss_D: 0.1974 Loss_G: 0.4029 MSE 101887.2891
===> Epoch[86](240/704): Loss_D: 0.1611 Loss_G: 0.3554 MSE 100053.7969
===> Epoch[86](270/704): Loss_D: 0.1662 Loss_G: 0.4006 MSE 99719.4219
===> Epoch[86](300/704): Loss_D: 0.2208 Loss_G: 0.3355 MSE 103625.4062
===> Epoch[86](330/704): Loss_D: 0.1775 Loss_G: 0.3527 MSE 97982.8906
===> Epoch[86](360/704): Loss_D: 0.1967 Loss_G: 0.3845 MSE 99246.7500
===> Epoch[86](390/704): Loss_D: 0.2373 Loss_G: 0.2781 MSE 95529.2266
===> Epoch[86](420/704): Loss_D: 0.1824 Loss_G: 0.3959 MSE 98978.2500
===> Epoch[86](450/704): Loss_D: 0.1799 Loss_G: 0.4231 MSE 98398.4531
===> Epoch[86](480/704): Loss_D: 0.2602 Loss_G: 0.3276 MSE 98114.3594
===> Epoch[86](510/704): Loss_D: 0.1634 Loss_G: 0.4131 MSE 110614.2266
===> Epoch[86](540/704): Loss_D: 0.1617 Loss_G: 0.3609 MSE 102324.7422
===> Epoch[86](570/704): Loss_D: 0.1340 Loss_G: 0.4394 MSE 101934.7422
===> Epoch[86](600/704): Loss_D: 0.1331 Loss_G: 0.4264 MSE 107616.1172
===> Epoch[86](630/704): Loss_D: 0.1123 Loss_G: 0.4441 MSE 100818.3281
===> Epoch[86](660/704): Loss_D: 0.1547 Loss_G: 0.4083 MSE 101127.2578
===> Epoch[86](690/704): Loss_D: 0.2979 Loss_G: 0.3069 MSE 105277.1875
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[87](0/704): Loss_D: 0.2068 Loss_G: 0.2316 MSE 103973.7500
===> Epoch[87](30/704): Loss_D: 0.2197 Loss_G: 0.3690 MSE 103827.3906
===> Epoch[87](60/704): Loss_D: 0.2471 Loss_G: 0.3274 MSE 102783.3203
===> Epoch[87](90/704): Loss_D: 0.2148 Loss_G: 0.3668 MSE 110229.1641
===> Epoch[87](120/704): Loss_D: 0.1620 Loss_G: 0.4108 MSE 109983.1719
===> Epoch[87](150/704): Loss_D: 0.1860 Loss_G: 0.3657 MSE 111654.3984
===> Epoch[87](180/704): Loss_D: 0.2363 Loss_G: 0.3855 MSE 104449.6562
===> Epoch[87](210/704): Loss_D: 0.1799 Loss_G: 0.3814 MSE 108506.4062
===> Epoch[87](240/704): Loss_D: 0.2065 Loss_G: 0.3439 MSE 104292.9531
===> Epoch[87](270/704): Loss_D: 0.1793 Loss_G: 0.3043 MSE 100490.2500
===> Epoch[87](300/704): Loss_D: 0.1668 Loss_G: 0.3724 MSE 108743.6875
===> Epoch[87](330/704): Loss_D: 0.1929 Loss_G: 0.3841 MSE 103827.6406
===> Epoch[87](360/704): Loss_D: 0.2248 Loss_G: 0.2855 MSE 102421.8750
===> Epoch[87](390/704): Loss_D: 0.3412 Loss_G: 0.2591 MSE 102648.5938
===> Epoch[87](420/704): Loss_D: 0.2925 Loss_G: 0.2503 MSE 104359.1172
===> Epoch[87](450/704): Loss_D: 0.2570 Loss_G: 0.2949 MSE 104870.7109
===> Epoch[87](480/704): Loss_D: 0.1937 Loss_G: 0.4263 MSE 105900.1562
===> Epoch[87](510/704): Loss_D: 0.2991 Loss_G: 0.2826 MSE 102456.0547
===> Epoch[87](540/704): Loss_D: 0.2443 Loss_G: 0.3260 MSE 104598.4609
===> Epoch[87](570/704): Loss_D: 0.2064 Loss_G: 0.2655 MSE 103214.1016
===> Epoch[87](600/704): Loss_D: 0.1556 Loss_G: 0.4424 MSE 106430.3281
===> Epoch[87](630/704): Loss_D: 0.2298 Loss_G: 0.3209 MSE 107828.5625
===> Epoch[87](660/704): Loss_D: 0.2329 Loss_G: 0.3063 MSE 105130.9531
===> Epoch[87](690/704): Loss_D: 0.2200 Loss_G: 0.3625 MSE 109082.8438
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[88](0/704): Loss_D: 0.2606 Loss_G: 0.2027 MSE 105089.1797
===> Epoch[88](30/704): Loss_D: 0.1535 Loss_G: 0.4545 MSE 106020.9219
===> Epoch[88](60/704): Loss_D: 0.1685 Loss_G: 0.3507 MSE 101422.1328
===> Epoch[88](90/704): Loss_D: 0.2015 Loss_G: 0.4176 MSE 109386.1094
===> Epoch[88](120/704): Loss_D: 0.2394 Loss_G: 0.3805 MSE 106334.1797
===> Epoch[88](150/704): Loss_D: 0.2316 Loss_G: 0.2920 MSE 109089.4219
===> Epoch[88](180/704): Loss_D: 0.1924 Loss_G: 0.3292 MSE 101019.4453
===> Epoch[88](210/704): Loss_D: 0.2009 Loss_G: 0.3080 MSE 99637.4062
===> Epoch[88](240/704): Loss_D: 0.2918 Loss_G: 0.3347 MSE 103785.9688
===> Epoch[88](270/704): Loss_D: 0.2849 Loss_G: 0.4126 MSE 106858.0938
===> Epoch[88](300/704): Loss_D: 0.1772 Loss_G: 0.4084 MSE 107881.2031
===> Epoch[88](330/704): Loss_D: 0.1677 Loss_G: 0.3719 MSE 104695.7969
===> Epoch[88](360/704): Loss_D: 0.1925 Loss_G: 0.3782 MSE 102986.3594
===> Epoch[88](390/704): Loss_D: 0.1533 Loss_G: 0.3841 MSE 100354.5469
===> Epoch[88](420/704): Loss_D: 0.2160 Loss_G: 0.3193 MSE 101515.1328
===> Epoch[88](450/704): Loss_D: 0.1959 Loss_G: 0.3189 MSE 104239.3594
===> Epoch[88](480/704): Loss_D: 0.1733 Loss_G: 0.3675 MSE 101247.6250
===> Epoch[88](510/704): Loss_D: 0.2026 Loss_G: 0.3618 MSE 100306.0625
===> Epoch[88](540/704): Loss_D: 0.1979 Loss_G: 0.3446 MSE 100015.6406
===> Epoch[88](570/704): Loss_D: 0.1830 Loss_G: 0.3647 MSE 102427.6875
===> Epoch[88](600/704): Loss_D: 0.2115 Loss_G: 0.3519 MSE 102813.6094
===> Epoch[88](630/704): Loss_D: 0.2120 Loss_G: 0.3326 MSE 99088.4375
===> Epoch[88](660/704): Loss_D: 0.1890 Loss_G: 0.3895 MSE 104746.6250
===> Epoch[88](690/704): Loss_D: 0.1766 Loss_G: 0.3282 MSE 98562.5156
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[89](0/704): Loss_D: 0.1508 Loss_G: 0.4080 MSE 95409.4531
===> Epoch[89](30/704): Loss_D: 0.1878 Loss_G: 0.3733 MSE 103929.1641
===> Epoch[89](60/704): Loss_D: 0.2263 Loss_G: 0.3985 MSE 96739.4766
===> Epoch[89](90/704): Loss_D: 0.2112 Loss_G: 0.3467 MSE 104259.2969
===> Epoch[89](120/704): Loss_D: 0.1812 Loss_G: 0.3827 MSE 103963.0156
===> Epoch[89](150/704): Loss_D: 0.1539 Loss_G: 0.3958 MSE 100766.2656
===> Epoch[89](180/704): Loss_D: 0.2202 Loss_G: 0.3621 MSE 99903.7969
===> Epoch[89](210/704): Loss_D: 0.1851 Loss_G: 0.3542 MSE 96900.6875
===> Epoch[89](240/704): Loss_D: 0.1564 Loss_G: 0.4530 MSE 101853.1875
===> Epoch[89](270/704): Loss_D: 0.2232 Loss_G: 0.3285 MSE 96542.5625
===> Epoch[89](300/704): Loss_D: 0.2318 Loss_G: 0.3544 MSE 99610.3828
===> Epoch[89](330/704): Loss_D: 0.2155 Loss_G: 0.3171 MSE 101762.5703
===> Epoch[89](360/704): Loss_D: 0.2317 Loss_G: 0.4144 MSE 103536.7031
===> Epoch[89](390/704): Loss_D: 0.1779 Loss_G: 0.3493 MSE 96648.0938
===> Epoch[89](420/704): Loss_D: 0.2494 Loss_G: 0.3079 MSE 100344.8984
===> Epoch[89](450/704): Loss_D: 0.2391 Loss_G: 0.3844 MSE 100922.0781
===> Epoch[89](480/704): Loss_D: 0.1781 Loss_G: 0.4123 MSE 104932.5156
===> Epoch[89](510/704): Loss_D: 0.2227 Loss_G: 0.3374 MSE 100052.2500
===> Epoch[89](540/704): Loss_D: 0.2309 Loss_G: 0.3849 MSE 98239.4062
===> Epoch[89](570/704): Loss_D: 0.2569 Loss_G: 0.3447 MSE 104392.6172
===> Epoch[89](600/704): Loss_D: 0.1690 Loss_G: 0.3743 MSE 100677.0781
===> Epoch[89](630/704): Loss_D: 0.1915 Loss_G: 0.3380 MSE 97701.3594
===> Epoch[89](660/704): Loss_D: 0.1949 Loss_G: 0.3699 MSE 100049.8594
===> Epoch[89](690/704): Loss_D: 0.2091 Loss_G: 0.3358 MSE 97303.2344
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[90](0/704): Loss_D: 0.1906 Loss_G: 0.3093 MSE 105138.1172
===> Epoch[90](30/704): Loss_D: 0.2846 Loss_G: 0.2767 MSE 99448.6953
===> Epoch[90](60/704): Loss_D: 0.2197 Loss_G: 0.3566 MSE 101573.8516
===> Epoch[90](90/704): Loss_D: 0.1536 Loss_G: 0.4165 MSE 105830.3281
===> Epoch[90](120/704): Loss_D: 0.1760 Loss_G: 0.3778 MSE 102965.0859
===> Epoch[90](150/704): Loss_D: 0.1747 Loss_G: 0.5101 MSE 100796.6797
===> Epoch[90](180/704): Loss_D: 0.1845 Loss_G: 0.3592 MSE 96765.6328
===> Epoch[90](210/704): Loss_D: 0.1783 Loss_G: 0.4104 MSE 99300.8359
===> Epoch[90](240/704): Loss_D: 0.1547 Loss_G: 0.3874 MSE 98403.6172
===> Epoch[90](270/704): Loss_D: 0.1646 Loss_G: 0.4240 MSE 101134.9219
===> Epoch[90](300/704): Loss_D: 0.1955 Loss_G: 0.3506 MSE 99407.0781
===> Epoch[90](330/704): Loss_D: 0.1825 Loss_G: 0.3870 MSE 99679.6719
===> Epoch[90](360/704): Loss_D: 0.2483 Loss_G: 0.2876 MSE 99512.7656
===> Epoch[90](390/704): Loss_D: 0.2698 Loss_G: 0.3273 MSE 105558.9844
===> Epoch[90](420/704): Loss_D: 0.2196 Loss_G: 0.4329 MSE 102710.6250
===> Epoch[90](450/704): Loss_D: 0.2651 Loss_G: 0.3039 MSE 102698.3359
===> Epoch[90](480/704): Loss_D: 0.1713 Loss_G: 0.3834 MSE 106839.3750
===> Epoch[90](510/704): Loss_D: 0.2665 Loss_G: 0.3156 MSE 100344.2969
===> Epoch[90](540/704): Loss_D: 0.1933 Loss_G: 0.3676 MSE 96963.1719
===> Epoch[90](570/704): Loss_D: 0.2194 Loss_G: 0.4014 MSE 103212.1953
===> Epoch[90](600/704): Loss_D: 0.2666 Loss_G: 0.3896 MSE 103162.8203
===> Epoch[90](630/704): Loss_D: 0.2048 Loss_G: 0.3707 MSE 102419.8906
===> Epoch[90](660/704): Loss_D: 0.2519 Loss_G: 0.2546 MSE 103392.2891
===> Epoch[90](690/704): Loss_D: 0.2268 Loss_G: 0.3120 MSE 100519.9297
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[91](0/704): Loss_D: 0.1689 Loss_G: 0.3015 MSE 102328.6562
===> Epoch[91](30/704): Loss_D: 0.2427 Loss_G: 0.3052 MSE 103413.5859
===> Epoch[91](60/704): Loss_D: 0.1488 Loss_G: 0.4503 MSE 107514.1250
===> Epoch[91](90/704): Loss_D: 0.1795 Loss_G: 0.3458 MSE 101635.5703
===> Epoch[91](120/704): Loss_D: 0.2508 Loss_G: 0.3424 MSE 95131.1875
===> Epoch[91](150/704): Loss_D: 0.1958 Loss_G: 0.3873 MSE 101862.8750
===> Epoch[91](180/704): Loss_D: 0.1784 Loss_G: 0.4690 MSE 101152.8594
===> Epoch[91](210/704): Loss_D: 0.1522 Loss_G: 0.4648 MSE 101878.0312
===> Epoch[91](240/704): Loss_D: 0.1711 Loss_G: 0.4225 MSE 101620.7812
===> Epoch[91](270/704): Loss_D: 0.2216 Loss_G: 0.3266 MSE 102131.7812
===> Epoch[91](300/704): Loss_D: 0.2101 Loss_G: 0.3730 MSE 101786.4688
===> Epoch[91](330/704): Loss_D: 0.2246 Loss_G: 0.3431 MSE 100291.2812
===> Epoch[91](360/704): Loss_D: 0.1836 Loss_G: 0.3820 MSE 99989.0000
===> Epoch[91](390/704): Loss_D: 0.1874 Loss_G: 0.3258 MSE 103184.4375
===> Epoch[91](420/704): Loss_D: 0.1573 Loss_G: 0.4081 MSE 104119.5625
===> Epoch[91](450/704): Loss_D: 0.1692 Loss_G: 0.4039 MSE 99982.6172
===> Epoch[91](480/704): Loss_D: 0.1835 Loss_G: 0.4023 MSE 100731.5000
===> Epoch[91](510/704): Loss_D: 0.2074 Loss_G: 0.4530 MSE 99146.8047
===> Epoch[91](540/704): Loss_D: 0.2192 Loss_G: 0.3819 MSE 99945.0156
===> Epoch[91](570/704): Loss_D: 0.1928 Loss_G: 0.3904 MSE 106131.7656
===> Epoch[91](600/704): Loss_D: 0.2784 Loss_G: 0.2869 MSE 102831.9609
===> Epoch[91](630/704): Loss_D: 0.2751 Loss_G: 0.2981 MSE 100767.6406
===> Epoch[91](660/704): Loss_D: 0.2105 Loss_G: 0.4155 MSE 100110.9219
===> Epoch[91](690/704): Loss_D: 0.2165 Loss_G: 0.3594 MSE 102924.7969
learning rate = 0.0001000
learning rate = 0.0001000
Saving
===> Epoch[92](0/704): Loss_D: 0.1224 Loss_G: 0.3632 MSE 101729.6250
===> Epoch[92](30/704): Loss_D: 0.1669 Loss_G: 0.3573 MSE 98085.2656
===> Epoch[92](60/704): Loss_D: 0.1735 Loss_G: 0.5488 MSE 98516.4219
===> Epoch[92](90/704): Loss_D: 0.1691 Loss_G: 0.3368 MSE 98670.0938
===> Epoch[92](120/704): Loss_D: 0.2294 Loss_G: 0.4300 MSE 100960.0625
===> Epoch[92](150/704): Loss_D: 0.1475 Loss_G: 0.4534 MSE 102234.5312
===> Epoch[92](180/704): Loss_D: 0.1563 Loss_G: 0.4359 MSE 99594.7109
===> Epoch[92](210/704): Loss_D: 0.1540 Loss_G: 0.4100 MSE 99937.7500
===> Epoch[92](240/704): Loss_D: 0.1474 Loss_G: 0.4170 MSE 100805.5000
===> Epoch[92](270/704): Loss_D: 0.1557 Loss_G: 0.4416 MSE 101853.9531
===> Epoch[92](300/704): Loss_D: 0.1217 Loss_G: 0.4825 MSE 103653.0000
===> Epoch[92](330/704): Loss_D: 0.1389 Loss_G: 0.4302 MSE 103793.5000
===> Epoch[92](360/704): Loss_D: 0.3971 Loss_G: 0.4089 MSE 100861.4375
===> Epoch[92](390/704): Loss_D: 0.0965 Loss_G: 0.5047 MSE 102176.9688
===> Epoch[92](420/704): Loss_D: 0.1130 Loss_G: 0.5654 MSE 101179.6016
===> Epoch[92](450/704): Loss_D: 0.1710 Loss_G: 0.4472 MSE 109294.4531
===> Epoch[92](480/704): Loss_D: 0.3449 Loss_G: 0.3858 MSE 106198.4609
===> Epoch[92](510/704): Loss_D: 0.1189 Loss_G: 0.4456 MSE 107956.1250
===> Epoch[92](540/704): Loss_D: 0.1091 Loss_G: 0.4896 MSE 103527.5312
===> Epoch[92](570/704): Loss_D: 0.1456 Loss_G: 0.5800 MSE 102634.8125
===> Epoch[92](600/704): Loss_D: 0.1324 Loss_G: 0.5108 MSE 102652.3594
===> Epoch[92](630/704): Loss_D: 0.1868 Loss_G: 0.2978 MSE 109130.6094
===> Epoch[92](660/704): Loss_D: 0.1835 Loss_G: 0.3040 MSE 107035.2266
===> Epoch[92](690/704): Loss_D: 0.2281 Loss_G: 0.3589 MSE 102370.9375
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[93](0/704): Loss_D: 0.2114 Loss_G: 0.2620 MSE 107331.6719
===> Epoch[93](30/704): Loss_D: 0.2285 Loss_G: 0.2924 MSE 106574.8281
===> Epoch[93](60/704): Loss_D: 0.1299 Loss_G: 0.4680 MSE 104732.4844
===> Epoch[93](90/704): Loss_D: 0.2354 Loss_G: 0.3661 MSE 106673.2969
===> Epoch[93](120/704): Loss_D: 0.2050 Loss_G: 0.3217 MSE 108464.9688
===> Epoch[93](150/704): Loss_D: 0.2368 Loss_G: 0.3492 MSE 104843.4844
===> Epoch[93](180/704): Loss_D: 0.1524 Loss_G: 0.3741 MSE 104906.7266
===> Epoch[93](210/704): Loss_D: 0.1939 Loss_G: 0.4146 MSE 108375.0938
===> Epoch[93](240/704): Loss_D: 0.2373 Loss_G: 0.3312 MSE 108353.4531
===> Epoch[93](270/704): Loss_D: 0.1891 Loss_G: 0.4137 MSE 104034.8125
===> Epoch[93](300/704): Loss_D: 0.2191 Loss_G: 0.3216 MSE 105756.7734
===> Epoch[93](330/704): Loss_D: 0.2702 Loss_G: 0.2748 MSE 107842.1406
===> Epoch[93](360/704): Loss_D: 0.1862 Loss_G: 0.4420 MSE 105122.1562
===> Epoch[93](390/704): Loss_D: 0.2431 Loss_G: 0.3122 MSE 110329.9688
===> Epoch[93](420/704): Loss_D: 0.2331 Loss_G: 0.3076 MSE 107293.0391
===> Epoch[93](450/704): Loss_D: 0.1890 Loss_G: 0.3360 MSE 109743.1953
===> Epoch[93](480/704): Loss_D: 0.2683 Loss_G: 0.2989 MSE 104219.7109
===> Epoch[93](510/704): Loss_D: 0.2858 Loss_G: 0.2594 MSE 96796.9609
===> Epoch[93](540/704): Loss_D: 0.2588 Loss_G: 0.2971 MSE 99557.9531
===> Epoch[93](570/704): Loss_D: 0.1893 Loss_G: 0.3190 MSE 103133.1016
===> Epoch[93](600/704): Loss_D: 0.2366 Loss_G: 0.2883 MSE 101414.8594
===> Epoch[93](630/704): Loss_D: 0.2084 Loss_G: 0.3086 MSE 102740.2969
===> Epoch[93](660/704): Loss_D: 0.1589 Loss_G: 0.4066 MSE 104359.2578
===> Epoch[93](690/704): Loss_D: 0.3041 Loss_G: 0.2967 MSE 100300.1406
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[94](0/704): Loss_D: 0.1915 Loss_G: 0.2834 MSE 98899.4375
===> Epoch[94](30/704): Loss_D: 0.2337 Loss_G: 0.3116 MSE 105748.3203
===> Epoch[94](60/704): Loss_D: 0.1858 Loss_G: 0.3973 MSE 99226.7969
===> Epoch[94](90/704): Loss_D: 0.1799 Loss_G: 0.3647 MSE 102861.2734
===> Epoch[94](120/704): Loss_D: 0.2302 Loss_G: 0.3299 MSE 104586.3594
===> Epoch[94](150/704): Loss_D: 0.1622 Loss_G: 0.4075 MSE 105278.8594
===> Epoch[94](180/704): Loss_D: 0.1974 Loss_G: 0.3126 MSE 104700.3281
===> Epoch[94](210/704): Loss_D: 0.2067 Loss_G: 0.3388 MSE 99972.1406
===> Epoch[94](240/704): Loss_D: 0.1213 Loss_G: 0.4786 MSE 105011.0938
===> Epoch[94](270/704): Loss_D: 0.1449 Loss_G: 0.4830 MSE 105817.1172
===> Epoch[94](300/704): Loss_D: 0.2094 Loss_G: 0.3153 MSE 102956.2188
===> Epoch[94](330/704): Loss_D: 0.2047 Loss_G: 0.4475 MSE 101416.6719
===> Epoch[94](360/704): Loss_D: 0.1969 Loss_G: 0.4406 MSE 104374.8984
===> Epoch[94](390/704): Loss_D: 0.1904 Loss_G: 0.3312 MSE 100126.0859
===> Epoch[94](420/704): Loss_D: 0.1793 Loss_G: 0.4338 MSE 101905.1797
===> Epoch[94](450/704): Loss_D: 0.2625 Loss_G: 0.2413 MSE 99446.7500
===> Epoch[94](480/704): Loss_D: 0.1728 Loss_G: 0.3729 MSE 101844.5391
===> Epoch[94](510/704): Loss_D: 0.2572 Loss_G: 0.3610 MSE 102050.0469
===> Epoch[94](540/704): Loss_D: 0.2819 Loss_G: 0.2935 MSE 104544.8594
===> Epoch[94](570/704): Loss_D: 0.1921 Loss_G: 0.3200 MSE 101606.1250
===> Epoch[94](600/704): Loss_D: 0.2900 Loss_G: 0.3121 MSE 103151.8516
===> Epoch[94](630/704): Loss_D: 0.2375 Loss_G: 0.3663 MSE 105203.0625
===> Epoch[94](660/704): Loss_D: 0.3353 Loss_G: 0.2705 MSE 99783.3359
===> Epoch[94](690/704): Loss_D: 0.1767 Loss_G: 0.3290 MSE 104113.0156
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[95](0/704): Loss_D: 0.2261 Loss_G: 0.2157 MSE 106167.1016
===> Epoch[95](30/704): Loss_D: 0.1986 Loss_G: 0.3202 MSE 104577.9688
===> Epoch[95](60/704): Loss_D: 0.1665 Loss_G: 0.4113 MSE 105395.7500
===> Epoch[95](90/704): Loss_D: 0.3546 Loss_G: 0.2227 MSE 103628.7266
===> Epoch[95](120/704): Loss_D: 0.2282 Loss_G: 0.3662 MSE 105103.1016
===> Epoch[95](150/704): Loss_D: 0.1974 Loss_G: 0.4423 MSE 97292.7344
===> Epoch[95](180/704): Loss_D: 0.2253 Loss_G: 0.3607 MSE 104874.6562
===> Epoch[95](210/704): Loss_D: 0.2471 Loss_G: 0.4975 MSE 104996.9609
===> Epoch[95](240/704): Loss_D: 0.1671 Loss_G: 0.3952 MSE 102920.6250
===> Epoch[95](270/704): Loss_D: 0.1729 Loss_G: 0.4599 MSE 103599.0000
===> Epoch[95](300/704): Loss_D: 0.1866 Loss_G: 0.3707 MSE 103089.6719
===> Epoch[95](330/704): Loss_D: 0.2406 Loss_G: 0.3693 MSE 106514.6562
===> Epoch[95](360/704): Loss_D: 0.1774 Loss_G: 0.3929 MSE 95904.2891
===> Epoch[95](390/704): Loss_D: 0.2914 Loss_G: 0.2777 MSE 101998.2500
===> Epoch[95](420/704): Loss_D: 0.2494 Loss_G: 0.3230 MSE 98251.3438
===> Epoch[95](450/704): Loss_D: 0.2204 Loss_G: 0.3395 MSE 99365.6562
===> Epoch[95](480/704): Loss_D: 0.1992 Loss_G: 0.3820 MSE 102362.3438
===> Epoch[95](510/704): Loss_D: 0.2034 Loss_G: 0.3843 MSE 101131.0547
===> Epoch[95](540/704): Loss_D: 0.2111 Loss_G: 0.3841 MSE 102026.9688
===> Epoch[95](570/704): Loss_D: 0.1730 Loss_G: 0.3945 MSE 101061.9688
===> Epoch[95](600/704): Loss_D: 0.1601 Loss_G: 0.4597 MSE 101122.7344
===> Epoch[95](630/704): Loss_D: 0.2224 Loss_G: 0.3062 MSE 101130.9062
===> Epoch[95](660/704): Loss_D: 0.2040 Loss_G: 0.3415 MSE 105257.2344
===> Epoch[95](690/704): Loss_D: 0.2263 Loss_G: 0.2968 MSE 103008.7500
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[96](0/704): Loss_D: 0.2322 Loss_G: 0.2135 MSE 103893.2500
===> Epoch[96](30/704): Loss_D: 0.1777 Loss_G: 0.3783 MSE 106115.7812
===> Epoch[96](60/704): Loss_D: 0.1826 Loss_G: 0.4506 MSE 99260.9375
===> Epoch[96](90/704): Loss_D: 0.2708 Loss_G: 0.2882 MSE 100171.5625
===> Epoch[96](120/704): Loss_D: 0.2718 Loss_G: 0.2555 MSE 99511.0312
===> Epoch[96](150/704): Loss_D: 0.2228 Loss_G: 0.3043 MSE 96789.2578
===> Epoch[96](180/704): Loss_D: 0.1957 Loss_G: 0.3504 MSE 100335.0625
===> Epoch[96](210/704): Loss_D: 0.1921 Loss_G: 0.3517 MSE 98387.9375
===> Epoch[96](240/704): Loss_D: 0.1524 Loss_G: 0.3943 MSE 101056.4219
===> Epoch[96](270/704): Loss_D: 0.1962 Loss_G: 0.3665 MSE 96354.8281
===> Epoch[96](300/704): Loss_D: 0.1985 Loss_G: 0.4506 MSE 98396.3203
===> Epoch[96](330/704): Loss_D: 0.2678 Loss_G: 0.3564 MSE 98369.0000
===> Epoch[96](360/704): Loss_D: 0.2113 Loss_G: 0.3255 MSE 97171.8438
===> Epoch[96](390/704): Loss_D: 0.1897 Loss_G: 0.3909 MSE 99710.4375
===> Epoch[96](420/704): Loss_D: 0.1914 Loss_G: 0.3693 MSE 98044.8125
===> Epoch[96](450/704): Loss_D: 0.1812 Loss_G: 0.3910 MSE 104009.3984
===> Epoch[96](480/704): Loss_D: 0.1761 Loss_G: 0.4349 MSE 104764.4844
===> Epoch[96](510/704): Loss_D: 0.1663 Loss_G: 0.4239 MSE 100191.0000
===> Epoch[96](540/704): Loss_D: 0.1588 Loss_G: 0.4668 MSE 101272.2344
===> Epoch[96](570/704): Loss_D: 0.2532 Loss_G: 0.3107 MSE 101357.2812
===> Epoch[96](600/704): Loss_D: 0.2379 Loss_G: 0.2897 MSE 97873.4688
===> Epoch[96](630/704): Loss_D: 0.2283 Loss_G: 0.3499 MSE 94795.0000
===> Epoch[96](660/704): Loss_D: 0.2399 Loss_G: 0.3087 MSE 98290.1172
===> Epoch[96](690/704): Loss_D: 0.2073 Loss_G: 0.3332 MSE 100861.5938
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[97](0/704): Loss_D: 0.1875 Loss_G: 0.2805 MSE 103727.6094
===> Epoch[97](30/704): Loss_D: 0.2140 Loss_G: 0.3264 MSE 98252.3594
===> Epoch[97](60/704): Loss_D: 0.2286 Loss_G: 0.3780 MSE 100918.0703
===> Epoch[97](90/704): Loss_D: 0.2404 Loss_G: 0.3729 MSE 97950.8750
===> Epoch[97](120/704): Loss_D: 0.2226 Loss_G: 0.3560 MSE 93112.4375
===> Epoch[97](150/704): Loss_D: 0.2185 Loss_G: 0.3570 MSE 102634.8047
===> Epoch[97](180/704): Loss_D: 0.2257 Loss_G: 0.2765 MSE 95893.6797
===> Epoch[97](210/704): Loss_D: 0.2945 Loss_G: 0.2905 MSE 97990.2344
===> Epoch[97](240/704): Loss_D: 0.2790 Loss_G: 0.3037 MSE 102312.4844
===> Epoch[97](270/704): Loss_D: 0.1922 Loss_G: 0.3770 MSE 100044.8906
===> Epoch[97](300/704): Loss_D: 0.2132 Loss_G: 0.3336 MSE 97795.6250
===> Epoch[97](330/704): Loss_D: 0.2275 Loss_G: 0.3372 MSE 97371.8438
===> Epoch[97](360/704): Loss_D: 0.1985 Loss_G: 0.3231 MSE 100517.4062
===> Epoch[97](390/704): Loss_D: 0.2048 Loss_G: 0.3384 MSE 100111.9844
===> Epoch[97](420/704): Loss_D: 0.1992 Loss_G: 0.4314 MSE 99803.0781
===> Epoch[97](450/704): Loss_D: 0.1998 Loss_G: 0.3806 MSE 99349.8438
===> Epoch[97](480/704): Loss_D: 0.2141 Loss_G: 0.3201 MSE 101584.4375
===> Epoch[97](510/704): Loss_D: 0.1896 Loss_G: 0.3798 MSE 97789.4844
===> Epoch[97](540/704): Loss_D: 0.1934 Loss_G: 0.4242 MSE 99453.0547
===> Epoch[97](570/704): Loss_D: 0.2591 Loss_G: 0.3660 MSE 98685.2188
===> Epoch[97](600/704): Loss_D: 0.2482 Loss_G: 0.2481 MSE 96284.2031
===> Epoch[97](630/704): Loss_D: 0.1566 Loss_G: 0.4252 MSE 94667.5625
===> Epoch[97](660/704): Loss_D: 0.2399 Loss_G: 0.3557 MSE 92558.5156
===> Epoch[97](690/704): Loss_D: 0.1761 Loss_G: 0.3468 MSE 98142.2031
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[98](0/704): Loss_D: 0.1960 Loss_G: 0.3202 MSE 97128.1094
===> Epoch[98](30/704): Loss_D: 0.1965 Loss_G: 0.3923 MSE 100485.3516
===> Epoch[98](60/704): Loss_D: 0.2105 Loss_G: 0.3497 MSE 102193.1016
===> Epoch[98](90/704): Loss_D: 0.1942 Loss_G: 0.3420 MSE 100494.9141
===> Epoch[98](120/704): Loss_D: 0.2107 Loss_G: 0.4226 MSE 102478.0078
===> Epoch[98](150/704): Loss_D: 0.1548 Loss_G: 0.4514 MSE 102475.2188
===> Epoch[98](180/704): Loss_D: 0.2055 Loss_G: 0.4164 MSE 101990.8438
===> Epoch[98](210/704): Loss_D: 0.2285 Loss_G: 0.4026 MSE 98002.7891
===> Epoch[98](240/704): Loss_D: 0.2384 Loss_G: 0.2592 MSE 101921.2109
===> Epoch[98](270/704): Loss_D: 0.1737 Loss_G: 0.3893 MSE 101705.7031
===> Epoch[98](300/704): Loss_D: 0.2063 Loss_G: 0.3464 MSE 103794.4844
===> Epoch[98](330/704): Loss_D: 0.2126 Loss_G: 0.3734 MSE 100268.2812
===> Epoch[98](360/704): Loss_D: 0.2002 Loss_G: 0.3732 MSE 98905.6406
===> Epoch[98](390/704): Loss_D: 0.2037 Loss_G: 0.3515 MSE 100460.1875
===> Epoch[98](420/704): Loss_D: 0.2417 Loss_G: 0.2932 MSE 99179.5391
===> Epoch[98](450/704): Loss_D: 0.1383 Loss_G: 0.4600 MSE 99473.5469
===> Epoch[98](480/704): Loss_D: 0.2263 Loss_G: 0.3312 MSE 100806.8203
===> Epoch[98](510/704): Loss_D: 0.1782 Loss_G: 0.3477 MSE 103277.2969
===> Epoch[98](540/704): Loss_D: 0.2785 Loss_G: 0.2651 MSE 107820.1172
===> Epoch[98](570/704): Loss_D: 0.2097 Loss_G: 0.3721 MSE 105009.9688
===> Epoch[98](600/704): Loss_D: 0.1952 Loss_G: 0.3783 MSE 100893.6328
===> Epoch[98](630/704): Loss_D: 0.1741 Loss_G: 0.3838 MSE 105720.7422
===> Epoch[98](660/704): Loss_D: 0.2287 Loss_G: 0.3360 MSE 101084.0234
===> Epoch[98](690/704): Loss_D: 0.2419 Loss_G: 0.3458 MSE 101383.8047
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[99](0/704): Loss_D: 0.2195 Loss_G: 0.2339 MSE 99731.7656
===> Epoch[99](30/704): Loss_D: 0.2204 Loss_G: 0.3216 MSE 105154.4609
===> Epoch[99](60/704): Loss_D: 0.2365 Loss_G: 0.3222 MSE 101817.4141
===> Epoch[99](90/704): Loss_D: 0.2653 Loss_G: 0.3241 MSE 100028.4062
===> Epoch[99](120/704): Loss_D: 0.1999 Loss_G: 0.3886 MSE 101967.3750
===> Epoch[99](150/704): Loss_D: 0.1947 Loss_G: 0.3213 MSE 100142.7344
===> Epoch[99](180/704): Loss_D: 0.1587 Loss_G: 0.3313 MSE 105004.6328
===> Epoch[99](210/704): Loss_D: 0.1782 Loss_G: 0.3801 MSE 105913.8750
===> Epoch[99](240/704): Loss_D: 0.1963 Loss_G: 0.3517 MSE 102265.2344
===> Epoch[99](270/704): Loss_D: 0.2487 Loss_G: 0.3908 MSE 99783.3750
===> Epoch[99](300/704): Loss_D: 0.1603 Loss_G: 0.4073 MSE 101080.9922
===> Epoch[99](330/704): Loss_D: 0.2120 Loss_G: 0.3596 MSE 107133.6875
===> Epoch[99](360/704): Loss_D: 0.2288 Loss_G: 0.3127 MSE 102221.8438
===> Epoch[99](390/704): Loss_D: 0.2300 Loss_G: 0.3066 MSE 104631.7031
===> Epoch[99](420/704): Loss_D: 0.2361 Loss_G: 0.2869 MSE 108481.8281
===> Epoch[99](450/704): Loss_D: 0.2576 Loss_G: 0.2968 MSE 105936.8750
===> Epoch[99](480/704): Loss_D: 0.2095 Loss_G: 0.3476 MSE 100623.1641
===> Epoch[99](510/704): Loss_D: 0.2948 Loss_G: 0.3188 MSE 99635.3750
===> Epoch[99](540/704): Loss_D: 0.2078 Loss_G: 0.3339 MSE 101601.0781
===> Epoch[99](570/704): Loss_D: 0.2135 Loss_G: 0.2934 MSE 104380.9688
===> Epoch[99](600/704): Loss_D: 0.1872 Loss_G: 0.3411 MSE 98042.9844
===> Epoch[99](630/704): Loss_D: 0.1684 Loss_G: 0.3753 MSE 99911.2891
===> Epoch[99](660/704): Loss_D: 0.2158 Loss_G: 0.3480 MSE 97410.4062
===> Epoch[99](690/704): Loss_D: 0.2220 Loss_G: 0.3302 MSE 99127.8594
learning rate = 0.0001000
learning rate = 0.0001000
===> Epoch[100](0/704): Loss_D: 0.2464 Loss_G: 0.1863 MSE 100218.1016
===> Epoch[100](30/704): Loss_D: 0.2354 Loss_G: 0.3309 MSE 99185.8594
===> Epoch[100](60/704): Loss_D: 0.2340 Loss_G: 0.2994 MSE 95667.4062
===> Epoch[100](90/704): Loss_D: 0.2281 Loss_G: 0.3200 MSE 100894.2578
===> Epoch[100](120/704): Loss_D: 0.2421 Loss_G: 0.3262 MSE 103233.2891
===> Epoch[100](150/704): Loss_D: 0.2158 Loss_G: 0.3651 MSE 107337.1562
===> Epoch[100](180/704): Loss_D: 0.2573 Loss_G: 0.2840 MSE 102249.2812
===> Epoch[100](210/704): Loss_D: 0.2742 Loss_G: 0.3065 MSE 97716.2812
===> Epoch[100](240/704): Loss_D: 0.2284 Loss_G: 0.3686 MSE 103273.8906
===> Epoch[100](270/704): Loss_D: 0.2245 Loss_G: 0.3670 MSE 103316.9062
===> Epoch[100](300/704): Loss_D: 0.2305 Loss_G: 0.3060 MSE 100853.5703
===> Epoch[100](330/704): Loss_D: 0.2911 Loss_G: 0.2869 MSE 104710.8984
===> Epoch[100](360/704): Loss_D: 0.2235 Loss_G: 0.3081 MSE 100946.6719
===> Epoch[100](390/704): Loss_D: 0.2215 Loss_G: 0.3011 MSE 100965.6484
===> Epoch[100](420/704): Loss_D: 0.2405 Loss_G: 0.2947 MSE 106855.6875
===> Epoch[100](450/704): Loss_D: 0.2419 Loss_G: 0.2859 MSE 104280.4062
===> Epoch[100](480/704): Loss_D: 0.2143 Loss_G: 0.3049 MSE 105488.3594
===> Epoch[100](510/704): Loss_D: 0.2292 Loss_G: 0.3118 MSE 105800.2422
===> Epoch[100](540/704): Loss_D: 0.2309 Loss_G: 0.2983 MSE 104736.2031
===> Epoch[100](570/704): Loss_D: 0.2137 Loss_G: 0.3587 MSE 104756.8828
===> Epoch[100](600/704): Loss_D: 0.1711 Loss_G: 0.3998 MSE 105218.7969
===> Epoch[100](630/704): Loss_D: 0.2539 Loss_G: 0.2806 MSE 100769.7656
===> Epoch[100](660/704): Loss_D: 0.2137 Loss_G: 0.3276 MSE 108633.1172
===> Epoch[100](690/704): Loss_D: 0.1837 Loss_G: 0.3554 MSE 104154.7031
learning rate = 0.0000100
learning rate = 0.0000100
Saving
Done
